{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5f85cd",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e185828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import d2l\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf1862",
   "metadata": {},
   "source": [
    "## 9.1 Working with Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1000\n",
    "tau = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = torch.arange(1, T+1, dtype=torch.float32)\n",
    "x = torch.sin(0.01 * time) + torch.randn(T) * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b77cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x[i : T-tau+i] for i in range(tau)]\n",
    "features = torch.stack(features, 1)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = x[tau:].unsqueeze(1)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f826f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "features[:3], labels[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_slice = slice(0, 600)\n",
    "test_slice = slice(600, None)\n",
    "train_features = features[train_slice]\n",
    "train_labels = labels[train_slice]\n",
    "test_features = features[test_slice]\n",
    "test_labels = labels[test_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8fff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.linear(X)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cca72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(tau)\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de4a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    result = model(train_features)\n",
    "    l = loss(result, train_labels)\n",
    "    optimizer.zero_grad()\n",
    "    l.backward()\n",
    "    optimizer.step()\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbb3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_preds = model(test_features).detach().numpy()\n",
    "plt.plot(test_labels.numpy(), label='true')\n",
    "plt.plot(one_step_preds, label='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8a658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multistep = x[train_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df83b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_len = len(test_features)\n",
    "for i in range(1, len(test_labels)+1):\n",
    "    step_features = multistep[-tau:].unsqueeze(0)\n",
    "    step_preds = model(step_features).detach().numpy()\n",
    "\n",
    "    multistep = torch.cat((multistep, torch.tensor(step_preds).flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ceb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(multistep.numpy(), label='pred')\n",
    "plt.plot(x, label='true', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d613596",
   "metadata": {},
   "source": [
    "## 9.2 Converting Raw Text into Sequence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd56c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, folder='data'):\n",
    "    import os\n",
    "    import requests\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    fname = os.path.join(folder, url.split('/')[-1])\n",
    "    if not os.path.isfile(fname):\n",
    "        r = requests.get(url)\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a10a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.gutenberg.org/cache/epub/35/pg35.txt'\n",
    "book = download_url(URL)\n",
    "with open(book, 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_book():\n",
    "    URL = 'https://www.gutenberg.org/cache/epub/35/pg35.txt'\n",
    "    book = download_url(URL)\n",
    "    with open(book, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf20a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c995630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:  #@save\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47530819",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(text)\n",
    "vocab = Vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ce9f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[tokens[:10]], tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()\n",
    "vocab = Vocab(words)\n",
    "\n",
    "vocab.token_freqs[:10], vocab.token_freqs[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e69f2f",
   "metadata": {},
   "source": [
    "## 9.4 Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f278fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, W_xh = torch.rand(3, 2), torch.rand(2, 5)\n",
    "H, W_hh = torch.rand(3, 5), torch.rand(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91763ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(X, W_xh) + torch.mm(H, W_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40153b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(torch.cat((X, H), 1), torch.cat((W_xh, W_hh), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661e8c4",
   "metadata": {},
   "source": [
    "## 9.5 Recurrent Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9598b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_machine = d2l.TimeMachine(64, 10, 10112, 5056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61b190ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_machine.X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17f5b0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(time_machine.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aeb661bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs):\n",
    "        super().__init__()\n",
    "        # Set hyperparameters\n",
    "        self.n_inputs = n_inputs # Length of input sequence\n",
    "        self.n_hidden = n_hidden # Number of hidden dimensions\n",
    "        self.n_outputs = n_outputs # Length of output dimensions\n",
    "\n",
    "        # Initialize latent layer parameters\n",
    "        self.W_xh = self._init_parameter((n_inputs, n_hidden))\n",
    "        self.W_hh = self._init_parameter((n_hidden, n_hidden))\n",
    "        self.b_h = self._init_zeros((n_hidden,))\n",
    "\n",
    "        # Initialize output layer parameters\n",
    "        self.W_hq = self._init_parameter((n_hidden, n_outputs))\n",
    "        self.b_q = self._init_zeros((n_outputs,))\n",
    "    \n",
    "    def _init_parameter(self, shape: tuple):\n",
    "        # Initialize parameters with Xavier distribution\n",
    "        return nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(shape)))\n",
    "    \n",
    "    def _init_zeros(self, shape: tuple):\n",
    "        # Initialize bias with zeros\n",
    "        return nn.Parameter(torch.zeros(shape))\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Return all parameters\n",
    "        params = [self.W_xh, self.W_hh, self.b_h, self.W_hq, self.b_q]\n",
    "        for param in params:\n",
    "            yield param\n",
    "\n",
    "    def forward(self, X: torch.Tensor, hidden_state: torch.Tensor = None):\n",
    "        if hidden_state is None:\n",
    "            hidden_state = torch.zeros((X.shape[0], self.n_hidden))\n",
    "        \n",
    "        # Compute the new hidden state\n",
    "        X_proj = torch.mm(X, self.W_xh)\n",
    "        H_proj = torch.mm(hidden_state, self.W_hh)\n",
    "        hidden_state = torch.tanh(X_proj + H_proj + self.b_h)\n",
    "        \n",
    "        # Compute output\n",
    "        output = torch.mm(hidden_state, self.W_hq) + self.b_q\n",
    "        \n",
    "        return output, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34a6d634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 20]),\n",
       " torch.Size([20, 20]),\n",
       " torch.Size([20]),\n",
       " torch.Size([20, 28]),\n",
       " torch.Size([28]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RNNScratch(n_inputs=10, n_hidden=20, n_outputs=len(time_machine.vocab))\n",
    "net.W_xh.shape, net.W_hh.shape, net.b_h.shape, net.W_hq.shape, net.b_q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d61ca981",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = time_machine.get_dataloader(False)\n",
    "test_data = time_machine.get_dataloader(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed6f3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_epoch(net, train_iterator, loss, optimizer):\n",
    "    \"\"\"Train a model for one epoch.\"\"\"\n",
    "    # Set the model to training mode\n",
    "    net.train()\n",
    "    # Initialize the total loss and number of samples\n",
    "    total_loss, num_samples = 0, 0\n",
    "\n",
    "    hidden_state = None\n",
    "    for X, y in train_iterator:\n",
    "        # Forward pass\n",
    "        y_hat, hidden_state = net(X, hidden_state)\n",
    "\n",
    "        # Detach the hidden state to prevent backpropagation through the entire sequence\n",
    "        hidden_state = hidden_state.detach()\n",
    "\n",
    "        # Compute the loss\n",
    "        l = loss(y_hat, y[:, -1].long())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        l.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the total loss and number of samples\n",
    "        total_loss += l.item() * y.shape[0]\n",
    "        num_samples += y.shape[0]\n",
    "    return total_loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "39bd2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb862e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.1420\n",
      "Epoch 2, Loss: 4.1420\n",
      "Epoch 3, Loss: 4.1420\n",
      "Epoch 4, Loss: 4.1420\n",
      "Epoch 5, Loss: 4.1420\n",
      "Epoch 6, Loss: 4.1420\n",
      "Epoch 7, Loss: 4.1420\n",
      "Epoch 8, Loss: 4.1420\n",
      "Epoch 9, Loss: 4.1420\n",
      "Epoch 10, Loss: 4.1420\n"
     ]
    }
   ],
   "source": [
    "net = RNNScratch(n_inputs=10, n_hidden=100, n_outputs=len(time_machine.vocab))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_rnn_epoch(net, train_data, loss, optimizer)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98110efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
