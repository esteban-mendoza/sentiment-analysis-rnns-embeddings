{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4c0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from utils.trainer import training_loop\n",
    "from models.rnns import RNNScratch, RNNLMScratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be00d281",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feef8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6644448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"rnnlm-scratch\"\n",
    "n_epochs = 1500\n",
    "early_stopping_params = {\n",
    "    'metric': 'f1',      # Monitor F1 score\n",
    "    'mode': 'max',       # We want to maximize F1\n",
    "    'patience': 1000,       # Wait for 5 epochs before stopping\n",
    "    'min_delta': 0   # Minimum change to qualify as improvement\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935dd2aa",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "059ce99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32, num_train=10240, num_val=5120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c6d636",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a1d0f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=512).to(device)\n",
    "model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1e-3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e4dd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0c16a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda\n",
      "Early stopping enabled: monitoring f1, mode=max, patience=1000\n",
      "TensorBoard logs will be saved to runs/rnnlm-scratch_RNNLMScratch_20250415-000130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmendoza/rnns-project/models/rnns.py:35: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n",
      "  for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-15 00:01:31.905152 Epoch 1\n",
      "  Training:   Loss: 3.1266, Accuracy: 0.2068\n",
      "  Validation: Loss: 2.8873, Accuracy: 0.1956, F1: 0.0761\n",
      "2025-04-15 00:01:34.585877 Epoch 5\n",
      "  Training:   Loss: 2.8342, Accuracy: 0.1876\n",
      "  Validation: Loss: 2.7939, Accuracy: 0.1945, F1: 0.0750\n",
      "2025-04-15 00:01:38.081152 Epoch 10\n",
      "  Training:   Loss: 2.5739, Accuracy: 0.2751\n",
      "  Validation: Loss: 2.5237, Accuracy: 0.2855, F1: 0.1940\n",
      "2025-04-15 00:01:41.403699 Epoch 15\n",
      "  Training:   Loss: 2.3523, Accuracy: 0.3228\n",
      "  Validation: Loss: 2.3194, Accuracy: 0.3354, F1: 0.2536\n",
      "2025-04-15 00:01:44.706088 Epoch 20\n",
      "  Training:   Loss: 2.2124, Accuracy: 0.3463\n",
      "  Validation: Loss: 2.2225, Accuracy: 0.3465, F1: 0.2741\n",
      "2025-04-15 00:01:47.757472 Epoch 25\n",
      "  Training:   Loss: 2.0388, Accuracy: 0.3959\n",
      "  Validation: Loss: 2.1135, Accuracy: 0.3772, F1: 0.3269\n",
      "2025-04-15 00:01:51.130064 Epoch 30\n",
      "  Training:   Loss: 1.8762, Accuracy: 0.4474\n",
      "  Validation: Loss: 2.0640, Accuracy: 0.4032, F1: 0.3710\n",
      "2025-04-15 00:01:54.378442 Epoch 35\n",
      "  Training:   Loss: 1.6884, Accuracy: 0.5040\n",
      "  Validation: Loss: 2.0560, Accuracy: 0.4087, F1: 0.3907\n",
      "2025-04-15 00:01:57.714146 Epoch 40\n",
      "  Training:   Loss: 1.5136, Accuracy: 0.5529\n",
      "  Validation: Loss: 2.1044, Accuracy: 0.4020, F1: 0.3849\n",
      "2025-04-15 00:02:01.026302 Epoch 45\n",
      "  Training:   Loss: 1.3230, Accuracy: 0.6142\n",
      "  Validation: Loss: 2.1804, Accuracy: 0.3896, F1: 0.3801\n",
      "2025-04-15 00:02:04.490556 Epoch 50\n",
      "  Training:   Loss: 1.1355, Accuracy: 0.6796\n",
      "  Validation: Loss: 2.2570, Accuracy: 0.3954, F1: 0.3814\n",
      "2025-04-15 00:02:07.760863 Epoch 55\n",
      "  Training:   Loss: 0.9883, Accuracy: 0.7304\n",
      "  Validation: Loss: 2.3494, Accuracy: 0.3872, F1: 0.3766\n",
      "2025-04-15 00:02:11.182925 Epoch 60\n",
      "  Training:   Loss: 0.8578, Accuracy: 0.7763\n",
      "  Validation: Loss: 2.4420, Accuracy: 0.3818, F1: 0.3741\n",
      "2025-04-15 00:02:14.487990 Epoch 65\n",
      "  Training:   Loss: 0.7622, Accuracy: 0.8057\n",
      "  Validation: Loss: 2.5281, Accuracy: 0.3804, F1: 0.3743\n",
      "2025-04-15 00:02:17.931906 Epoch 70\n",
      "  Training:   Loss: 0.6791, Accuracy: 0.8305\n",
      "  Validation: Loss: 2.6182, Accuracy: 0.3804, F1: 0.3720\n",
      "2025-04-15 00:02:21.415432 Epoch 75\n",
      "  Training:   Loss: 0.6145, Accuracy: 0.8455\n",
      "  Validation: Loss: 2.6928, Accuracy: 0.3776, F1: 0.3703\n",
      "2025-04-15 00:02:25.257635 Epoch 80\n",
      "  Training:   Loss: 0.5685, Accuracy: 0.8547\n",
      "  Validation: Loss: 2.7480, Accuracy: 0.3768, F1: 0.3718\n",
      "2025-04-15 00:02:28.968820 Epoch 85\n",
      "  Training:   Loss: 0.5305, Accuracy: 0.8615\n",
      "  Validation: Loss: 2.7910, Accuracy: 0.3763, F1: 0.3719\n",
      "2025-04-15 00:02:32.837167 Epoch 90\n",
      "  Training:   Loss: 0.5015, Accuracy: 0.8673\n",
      "  Validation: Loss: 2.8297, Accuracy: 0.3800, F1: 0.3751\n",
      "2025-04-15 00:02:35.796943 Epoch 95\n",
      "  Training:   Loss: 0.4770, Accuracy: 0.8718\n",
      "  Validation: Loss: 2.8627, Accuracy: 0.3839, F1: 0.3784\n",
      "2025-04-15 00:02:39.466521 Epoch 100\n",
      "  Training:   Loss: 0.4556, Accuracy: 0.8760\n",
      "  Validation: Loss: 2.8967, Accuracy: 0.3832, F1: 0.3764\n",
      "2025-04-15 00:02:43.158688 Epoch 105\n",
      "  Training:   Loss: 0.4378, Accuracy: 0.8794\n",
      "  Validation: Loss: 2.9314, Accuracy: 0.3807, F1: 0.3748\n",
      "2025-04-15 00:02:46.876887 Epoch 110\n",
      "  Training:   Loss: 0.4227, Accuracy: 0.8825\n",
      "  Validation: Loss: 2.9531, Accuracy: 0.3843, F1: 0.3795\n",
      "2025-04-15 00:02:50.588097 Epoch 115\n",
      "  Training:   Loss: 0.4087, Accuracy: 0.8850\n",
      "  Validation: Loss: 2.9926, Accuracy: 0.3863, F1: 0.3799\n",
      "2025-04-15 00:02:54.438995 Epoch 120\n",
      "  Training:   Loss: 0.3995, Accuracy: 0.8867\n",
      "  Validation: Loss: 3.0208, Accuracy: 0.3850, F1: 0.3784\n",
      "2025-04-15 00:02:58.130914 Epoch 125\n",
      "  Training:   Loss: 0.3871, Accuracy: 0.8898\n",
      "  Validation: Loss: 3.0335, Accuracy: 0.3880, F1: 0.3817\n",
      "2025-04-15 00:03:01.662149 Epoch 130\n",
      "  Training:   Loss: 0.3766, Accuracy: 0.8920\n",
      "  Validation: Loss: 3.0634, Accuracy: 0.3881, F1: 0.3828\n",
      "2025-04-15 00:03:05.384597 Epoch 135\n",
      "  Training:   Loss: 0.3699, Accuracy: 0.8933\n",
      "  Validation: Loss: 3.1036, Accuracy: 0.3852, F1: 0.3805\n",
      "2025-04-15 00:03:09.231999 Epoch 140\n",
      "  Training:   Loss: 0.3617, Accuracy: 0.8949\n",
      "  Validation: Loss: 3.1049, Accuracy: 0.3895, F1: 0.3832\n",
      "2025-04-15 00:03:12.930236 Epoch 145\n",
      "  Training:   Loss: 0.3543, Accuracy: 0.8964\n",
      "  Validation: Loss: 3.1401, Accuracy: 0.3891, F1: 0.3833\n",
      "2025-04-15 00:03:16.640230 Epoch 150\n",
      "  Training:   Loss: 0.3487, Accuracy: 0.8972\n",
      "  Validation: Loss: 3.1626, Accuracy: 0.3914, F1: 0.3854\n",
      "2025-04-15 00:03:19.943288 Epoch 155\n",
      "  Training:   Loss: 0.3415, Accuracy: 0.8987\n",
      "  Validation: Loss: 3.1948, Accuracy: 0.3916, F1: 0.3849\n",
      "2025-04-15 00:03:23.397703 Epoch 160\n",
      "  Training:   Loss: 0.3359, Accuracy: 0.8998\n",
      "  Validation: Loss: 3.2242, Accuracy: 0.3911, F1: 0.3850\n",
      "2025-04-15 00:03:26.208754 Epoch 165\n",
      "  Training:   Loss: 0.3304, Accuracy: 0.9013\n",
      "  Validation: Loss: 3.2185, Accuracy: 0.3913, F1: 0.3851\n",
      "2025-04-15 00:03:29.351919 Epoch 170\n",
      "  Training:   Loss: 0.3261, Accuracy: 0.9019\n",
      "  Validation: Loss: 3.2444, Accuracy: 0.3922, F1: 0.3868\n",
      "2025-04-15 00:03:32.311726 Epoch 175\n",
      "  Training:   Loss: 0.3218, Accuracy: 0.9028\n",
      "  Validation: Loss: 3.2675, Accuracy: 0.3918, F1: 0.3856\n",
      "2025-04-15 00:03:35.446205 Epoch 180\n",
      "  Training:   Loss: 0.3180, Accuracy: 0.9037\n",
      "  Validation: Loss: 3.2863, Accuracy: 0.3923, F1: 0.3862\n",
      "2025-04-15 00:03:38.263479 Epoch 185\n",
      "  Training:   Loss: 0.3131, Accuracy: 0.9049\n",
      "  Validation: Loss: 3.3198, Accuracy: 0.3928, F1: 0.3861\n",
      "2025-04-15 00:03:41.397322 Epoch 190\n",
      "  Training:   Loss: 0.3096, Accuracy: 0.9054\n",
      "  Validation: Loss: 3.3201, Accuracy: 0.3964, F1: 0.3896\n",
      "2025-04-15 00:03:44.379234 Epoch 195\n",
      "  Training:   Loss: 0.3065, Accuracy: 0.9062\n",
      "  Validation: Loss: 3.3442, Accuracy: 0.3934, F1: 0.3886\n",
      "2025-04-15 00:03:47.409203 Epoch 200\n",
      "  Training:   Loss: 0.3029, Accuracy: 0.9068\n",
      "  Validation: Loss: 3.3587, Accuracy: 0.3950, F1: 0.3889\n",
      "2025-04-15 00:03:50.640820 Epoch 205\n",
      "  Training:   Loss: 0.2998, Accuracy: 0.9073\n",
      "  Validation: Loss: 3.3709, Accuracy: 0.3953, F1: 0.3900\n",
      "2025-04-15 00:03:54.112332 Epoch 210\n",
      "  Training:   Loss: 0.2969, Accuracy: 0.9082\n",
      "  Validation: Loss: 3.3971, Accuracy: 0.3957, F1: 0.3914\n",
      "2025-04-15 00:03:57.472418 Epoch 215\n",
      "  Training:   Loss: 0.2937, Accuracy: 0.9086\n",
      "  Validation: Loss: 3.4079, Accuracy: 0.3921, F1: 0.3876\n",
      "2025-04-15 00:04:00.814632 Epoch 220\n",
      "  Training:   Loss: 0.2903, Accuracy: 0.9094\n",
      "  Validation: Loss: 3.4105, Accuracy: 0.4007, F1: 0.3947\n",
      "2025-04-15 00:04:04.136185 Epoch 225\n",
      "  Training:   Loss: 0.2879, Accuracy: 0.9097\n",
      "  Validation: Loss: 3.4346, Accuracy: 0.3996, F1: 0.3931\n",
      "2025-04-15 00:04:07.568543 Epoch 230\n",
      "  Training:   Loss: 0.2852, Accuracy: 0.9104\n",
      "  Validation: Loss: 3.4491, Accuracy: 0.3998, F1: 0.3946\n",
      "2025-04-15 00:04:10.575541 Epoch 235\n",
      "  Training:   Loss: 0.2826, Accuracy: 0.9107\n",
      "  Validation: Loss: 3.4573, Accuracy: 0.3993, F1: 0.3937\n",
      "2025-04-15 00:04:13.601931 Epoch 240\n",
      "  Training:   Loss: 0.2810, Accuracy: 0.9113\n",
      "  Validation: Loss: 3.4806, Accuracy: 0.4002, F1: 0.3923\n",
      "2025-04-15 00:04:16.656988 Epoch 245\n",
      "  Training:   Loss: 0.2795, Accuracy: 0.9114\n",
      "  Validation: Loss: 3.4946, Accuracy: 0.4000, F1: 0.3931\n",
      "2025-04-15 00:04:20.113402 Epoch 250\n",
      "  Training:   Loss: 0.2771, Accuracy: 0.9120\n",
      "  Validation: Loss: 3.4992, Accuracy: 0.4009, F1: 0.3951\n",
      "2025-04-15 00:04:23.003219 Epoch 255\n",
      "  Training:   Loss: 0.2747, Accuracy: 0.9124\n",
      "  Validation: Loss: 3.5048, Accuracy: 0.4035, F1: 0.3973\n",
      "2025-04-15 00:04:26.415739 Epoch 260\n",
      "  Training:   Loss: 0.2719, Accuracy: 0.9131\n",
      "  Validation: Loss: 3.5404, Accuracy: 0.4005, F1: 0.3943\n",
      "2025-04-15 00:04:29.527463 Epoch 265\n",
      "  Training:   Loss: 0.2708, Accuracy: 0.9132\n",
      "  Validation: Loss: 3.5429, Accuracy: 0.4004, F1: 0.3958\n",
      "2025-04-15 00:04:32.952302 Epoch 270\n",
      "  Training:   Loss: 0.2682, Accuracy: 0.9137\n",
      "  Validation: Loss: 3.5496, Accuracy: 0.4011, F1: 0.3954\n",
      "2025-04-15 00:04:34.756177 Epoch 275\n",
      "  Training:   Loss: 0.2674, Accuracy: 0.9139\n",
      "  Validation: Loss: 3.5575, Accuracy: 0.4008, F1: 0.3966\n",
      "2025-04-15 00:04:37.362313 Epoch 280\n",
      "  Training:   Loss: 0.2652, Accuracy: 0.9144\n",
      "  Validation: Loss: 3.5707, Accuracy: 0.4042, F1: 0.3987\n",
      "TensorBoard writer closed for runs/rnnlm-scratch_RNNLMScratch_20250415-000130\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx_to_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rnns-project/utils/trainer.py:539\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, val_loader, device, class_names, model_name, early_stopping_params, clipping)\u001b[0m\n\u001b[1;32m    534\u001b[0m train_metrics \u001b[38;5;241m=\u001b[39m train_epoch(\n\u001b[1;32m    535\u001b[0m     model, train_loader, optimizer, loss_fn, device, clipping\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Log metrics to TensorBoard\u001b[39;00m\n\u001b[1;32m    544\u001b[0m log_metrics(writer, train_metrics, val_metrics, optimizer, epoch)\n",
      "File \u001b[0;32m~/rnns-project/utils/trainer.py:132\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, val_loader, loss_fn, device, class_names)\u001b[0m\n\u001b[1;32m    129\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m    133\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    134\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/rnns-project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/rnns-project/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/rnns-project/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/rnns-project/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/rnns-project/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rnns-project/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:196\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(tensor[index] \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs=n_epochs,\n",
    "    optimizer=optimizer,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=data.get_dataloader(train=True),\n",
    "    val_loader=data.get_dataloader(train=False),\n",
    "    device=device,\n",
    "    class_names=data.vocab.idx_to_token,\n",
    "    model_name=model_name,\n",
    "    early_stopping_params=early_stopping_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65072bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
