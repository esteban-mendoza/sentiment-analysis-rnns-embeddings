<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <title>informatics-11-00024</title>
  <link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<section id="informatics-11-00024" class="level1 unnumbered">
<h1>informatics-11-00024</h1>
<ul>
</ul>
<article>
<div class="html-front">
<div class="html-art-header">
<p><em>Informatics</em> <strong>2024</strong>, <em>11</em>(2), 24; doi:<a href="https://doi.org/10.3390/informatics11020024">10.3390/informatics11020024</a></p>
</div>
<div class="html-art-type">
Article
</div>
<div id="html-article-title">
Machine Learning and Deep Learning Sentiment Analysis Models: Case Study on the SENT-COVID Corpus of Tweets in Mexican Spanish
</div>
<div class="html-author-group">
Helena Gomez-Adorno <sup><span data-rid="af1-informatics-11-00024">1</span>,</sup><span data-rid="c1-informatics-11-00024">*</span><a href="https://orcid.org/0000-0002-6966-9912"><img src="https://www.mdpi.com/img/design/orcid.png" class="html-orcid-img" /></a>, Gemma Bel-Enguix <sup><span data-rid="af2-informatics-11-00024">2</span></sup><a href="https://orcid.org/0000-0002-1411-5736"><img src="https://www.mdpi.com/img/design/orcid.png" class="html-orcid-img" /></a>, Gerardo Sierra <sup><span data-rid="af2-informatics-11-00024">2</span></sup><a href="https://orcid.org/0000-0002-6724-1090"><img src="https://www.mdpi.com/img/design/orcid.png" class="html-orcid-img" /></a>, Juan-Carlos Barajas <sup><span data-rid="af3-informatics-11-00024">3</span></sup><a href="https://orcid.org/0009-0008-8349-7142"><img src="https://www.mdpi.com/img/design/orcid.png" class="html-orcid-img" /></a> and William Álvarez <sup><span data-rid="af1-informatics-11-00024">1</span></sup><a href="https://orcid.org/0000-0002-3768-7817"><img src="https://www.mdpi.com/img/design/orcid.png" class="html-orcid-img" /></a>
</div>
<div class="html-aff-group">
<div id="af1-informatics-11-00024" class="html-aff">
<div class="html-label">
<sup>1</sup>
</div>
<div class="html-content">
Instituto de Investigaciones en Matemáticas Aplicadas y en Sistemas, Universidad Nacional Autónoma de México, Mexico City 04510, Mexico
</div>
</div>
<div id="af2-informatics-11-00024" class="html-aff">
<div class="html-label">
<sup>2</sup>
</div>
<div class="html-content">
Instituto de Ingeniería, Universidad Nacional Autónoma de México, Mexico City 04510, Mexico
</div>
</div>
<div id="af3-informatics-11-00024" class="html-aff">
<div class="html-label">
<sup>3</sup>
</div>
<div class="html-content">
Facultad de Ciencias, Universidad Nacional Autónoma de México, Mexico City 04510, Mexico
</div>
</div>
</div>
<div class="html-notes">
<div id="c1-informatics-11-00024" class="html-note">
<div class="html-label">
*
</div>
<div class="html-content">
Correspondence: <a href="mailto:helena.gomez@iimas.unam.mx">helena.gomez@iimas.unam.mx</a>
</div>
</div>
</div>
<div class="html-content">
<strong>Citation:</strong> Gomez-Adorno, H.; Bel-Enguix, G.; Sierra, G.; Barajas, J.-C.; Álvarez, W. Machine Learning and Deep Learning Sentiment Analysis Models: Case Study on the SENT-COVID Corpus of Tweets in Mexican Spanish. <em>Informatics</em> <strong>2024</strong>, <em>11</em>, 24. https://doi.org/10.3390/informatics11020024
</div>
<div class="html-external_editor">
Academic Editor: Olga Kurasova
</div>
<div class="html-history">
Received: 22 February 2024 / Revised: 23 March 2024 / Accepted: 16 April 2024 / Published: 23 April 2024
</div>
<section id="html-abstract" class="html-abstract">
<h2 id="html-abstract-title">Abstract</h2>
<div class="html-p">
This article presents a comprehensive evaluation of traditional machine learning and deep learning models in analyzing sentiment trends within the SENT-COVID Twitter corpus, curated during the COVID-19 pandemic. The corpus, filtered by COVID-19 related keywords and manually annotated for polarity, is a pivotal resource for conducting sentiment analysis experiments. Our study investigates various approaches, including classic vector-based systems such as word2vec, doc2vec, and diverse phrase modeling techniques, alongside Spanish pre-trained BERT models. We assess the performance of readily available sentiment analysis libraries for Python users, including TextBlob, VADER, and Pysentimiento. Additionally, we implement and evaluate traditional classification algorithms such as Logistic Regression, Naive Bayes, Support Vector Machines, and simple neural networks like Multilayer Perceptron. Throughout the research, we explore different dimensionality reduction techniques. This methodology enables a precise comparison among classification methods, with BETO-uncased achieving the highest accuracy of 0.73 on the test set. Our findings underscore the efficacy and applicability of traditional machine learning and deep learning models in analyzing sentiment trends within the context of low-resource Spanish language scenarios and emerging topics like COVID-19.
</div>
</section>
<div id="html-keywords">
<div class="html-gwd-group">
<div id="html-keywords-title">
Keywords:
</div>
sentiment analysis; COVID-19; machine learning; social media; Spanish
</div>
<div>

</div>
</div>
</div>
<div class="html-body">
<section id="sec1-informatics-11-00024" type="intro">
<h2 id="introduction" data-nested="1">1. Introduction</h2>
<div class="html-p">
Social media communication is crucial in all sectors of the population’s life. Companies use social media to massively promote products and services, while people use them to transmit experiences and opinions. Natural Language Processing (NLP) and Text Mining have been of great interest in exploring this source of textual communication to generate information about mass behavior, thoughts, and emotions on a wide variety of topics, such as product reviews [<a href="ch001.xhtml#B1-informatics-11-00024" class="html-bibr">1</a>], political trends [<a href="ch001.xhtml#B2-informatics-11-00024" class="html-bibr">2</a>], and stock market sentiment [<a href="ch001.xhtml#B3-informatics-11-00024" class="html-bibr">3</a>]. During the Coronavirus pandemic, people expressed how they experienced the consequences of quarantine, the way it altered the daily rhythm of life, and how they changed their day-to-day activities.
</div>
<div class="html-p">
Among the most used social media during the pandemic was Twitter, which at the time functioned as a freely accessible universal microexpression tool. This made it an ideal platform to capture the population’s feelings during this historic moment. Many studies have been presented that analyze various aspects of the epidemic, some of them on Twitter and mainly in English.
</div>
<div class="html-p">
This article presents the work carried out to study the emotional impact of COVID-19 on the Mexican population. The MIOPERS platform responded to UNAM’s initiative to develop models for the analysis and visualization of information that support strategic decision-making, especially during lockdown. During the pandemic, there were two main motivations for starting such work: (a) to evaluate people’s behavior, moods, and popularity of the measures given by the government and (b) to monitor users with possible symptoms.
</div>
<div class="html-p">
This initiative, which covers two years (2020–2022), the duration of the pandemic, allowed a compilation of many tweets related to COVID-19. This facilitated the study of topic-related lexicon, mentions, and hashtags, which in turn served as a basis for studying other important NLP topics, such as sentiment analysis.
</div>
<div class="html-p">
This article focuses on developing a specific corpus for polarity analysis of COVID-19, the SENT-COVID corpus, taking a subset of the tweets collected by the Miopers system during the pandemic. Furthermore, polarity classification experiments are performed, applying both traditional ML and DL methods. To do this, the article follows the structure explained below. Related work is discussed in <a href="ch001.xhtml#sec2-informatics-11-00024" class="html-sec">Section 2</a>, especially on sentiment analysis in social networks or specifically oriented to the topic of COVID-19. <a href="ch001.xhtml#sec3-informatics-11-00024" class="html-sec">Section 3</a> explains the compilation of the corpus, the annotation protocol, and the agreement results. The methodology that has been followed to carry out the analysis is described in <a href="ch001.xhtml#sec4-informatics-11-00024" class="html-sec">Section 4</a>, including pre-processing, forms of text representation, and algorithms used. The results are presented and discussed in <a href="ch001.xhtml#sec5-informatics-11-00024" class="html-sec">Section 5</a>. The article concludes with the conclusions in <a href="ch001.xhtml#sec6-informatics-11-00024" class="html-sec">Section 6</a>.
</div>
</section>
<section id="sec2-informatics-11-00024" type="">
<h2 id="related-work" data-nested="1">2. Related Work</h2>
<div class="html-p">
Numerous toolkits are available to process textual data, which makes complex NLP tasks more accessible with user-friendly interfaces. In the context of sentiment analysis, several researchers have used libraries such as TextBlob, VADER, and Pysentimiento, among others. TextBlob and VADER have the advantage of not requiring training data, as it is a lexicon-based approach. Therefore, they have been popular tools for analyzing comments on social networks, such as tweets [<a href="ch001.xhtml#B4-informatics-11-00024" class="html-bibr">4</a>,<a href="ch001.xhtml#B5-informatics-11-00024" class="html-bibr">5</a>,<a href="ch001.xhtml#B6-informatics-11-00024" class="html-bibr">6</a>,<a href="ch001.xhtml#B7-informatics-11-00024" class="html-bibr">7</a>,<a href="ch001.xhtml#B8-informatics-11-00024" class="html-bibr">8</a>], youtube [<a href="ch001.xhtml#B9-informatics-11-00024" class="html-bibr">9</a>,<a href="ch001.xhtml#B10-informatics-11-00024" class="html-bibr">10</a>,<a href="ch001.xhtml#B11-informatics-11-00024" class="html-bibr">11</a>,<a href="ch001.xhtml#B12-informatics-11-00024" class="html-bibr">12</a>,<a href="ch001.xhtml#B13-informatics-11-00024" class="html-bibr">13</a>] or Reddit [<a href="ch001.xhtml#B14-informatics-11-00024" class="html-bibr">14</a>,<a href="ch001.xhtml#B15-informatics-11-00024" class="html-bibr">15</a>,<a href="ch001.xhtml#B16-informatics-11-00024" class="html-bibr">16</a>,<a href="ch001.xhtml#B17-informatics-11-00024" class="html-bibr">17</a>] comments. Although the lexicon-based approach is suitable for general use, its main limitation lies in its difficulty adapting to changing contexts and linguistic uses [<a href="ch001.xhtml#B18-informatics-11-00024" class="html-bibr">18</a>]. Examples are texts such as tweets that have a lively and casual tone [<a href="ch001.xhtml#B19-informatics-11-00024" class="html-bibr">19</a>]. In addition, if we look at those related to COVID-19, we find new terms associated with the phenomenon. Additionally, since TextBlob and VADER were designed mainly for English-language texts, they may not be as effective when used in texts in other languages. Therefore, a toolkit for analyzing text sentiments and emotions in a wide range of languages is the Pysentimiento library, which offers support for multiple languages [<a href="ch001.xhtml#B20-informatics-11-00024" class="html-bibr">20</a>,<a href="ch001.xhtml#B21-informatics-11-00024" class="html-bibr">21</a>,<a href="ch001.xhtml#B22-informatics-11-00024" class="html-bibr">22</a>], including Spanish [<a href="ch001.xhtml#B23-informatics-11-00024" class="html-bibr">23</a>,<a href="ch001.xhtml#B24-informatics-11-00024" class="html-bibr">24</a>]. Furthermore, Pysentimiento uses state-of-the-art machine learning models, such as BERT (Bidirectional Encoder Representations from Transformers) models, for sentiment analysis. However, this requires more computing resources than TextBlob or VADER.
</div>
<div class="html-p">
From the beginning of the quarantine period, several researchers studied social media information to measure people’s feelings about their situation during the COVID-19 pandemic [<a href="ch001.xhtml#B25-informatics-11-00024" class="html-bibr">25</a>]. This has been done considering the language and domain of the comments posted on the different social platforms [<a href="ch001.xhtml#B26-informatics-11-00024" class="html-bibr">26</a>]. Many studies have used TextBlob, VADER, and Pysentimiento tools for sentiment analysis on social networks [<a href="ch001.xhtml#B6-informatics-11-00024" class="html-bibr">6</a>,<a href="ch001.xhtml#B23-informatics-11-00024" class="html-bibr">23</a>,<a href="ch001.xhtml#B27-informatics-11-00024" class="html-bibr">27</a>,<a href="ch001.xhtml#B28-informatics-11-00024" class="html-bibr">28</a>,<a href="ch001.xhtml#B29-informatics-11-00024" class="html-bibr">29</a>,<a href="ch001.xhtml#B30-informatics-11-00024" class="html-bibr">30</a>,<a href="ch001.xhtml#B31-informatics-11-00024" class="html-bibr">31</a>]. Moreover, machine learning approaches have been widely adopted to categorize sentiments into two (negative and positive) or three classes (positive, negative, and neutral). For example, Long Short Term Memory (LSTM) recurrent neural network has been used in Reddit comments, which allows for 81.15% accuracy [<a href="ch001.xhtml#B32-informatics-11-00024" class="html-bibr">32</a>].
</div>
<div class="html-p">
Chunduri and Perera [<a href="ch001.xhtml#B33-informatics-11-00024" class="html-bibr">33</a>] have used advanced deep learning models, such as Spiking Neural Networks (SNN), for polarity-based classification. SNNs encompass what is known as brain-based computing, and attempt to mimic the distinctive functionalities of the human brain in terms of energy efficiency, computational power, and robust learning. Although they report 100% accuracy with their model, their main claim is that SNNs have lower energy consumption than ANNs.
</div>
<div class="html-p">
For public tweets related to COVID-19, the TClustVID model [<a href="ch001.xhtml#B34-informatics-11-00024" class="html-bibr">34</a>] was developed, achieving a high accuracy of 98.3%.
</div>
<div class="html-p">
Researchers have also analyzed the performance of language models for sentiment analysis in Spanish. Specifically, for the COVID-19 tweet polarity, Contreras et al. [<a href="ch001.xhtml#B35-informatics-11-00024" class="html-bibr">35</a>] found that pre-trained BERT models in Spanish (BETO), with domain-adjusted, have achieved a high accuracy of 97% in training and 81% in testing. Such performance was the best compared to multilingual BERT models and other classification methods such as Decision Trees, Support Vector Machines, Naive Bayes, and Logistic Regression.
</div>
<div class="html-p">
Research has focused not only on creating computational models for text classification but also on annotated datasets, which help to train and evaluate models in supervised learning approaches. An example is COVIDSENTI [<a href="ch001.xhtml#B36-informatics-11-00024" class="html-bibr">36</a>], which consists of 90,000 COVID-19-related English-language tweets collected in the early stage of the pandemic from February to March 2020. Each tweet has been labeled as positive, negative, or neutral. Furthermore, state-of-the-art BERT models have been applied to the data to obtain a high precision of 98.3%.
</div>
<div class="html-p">
For sentiment analysis, several corpora of annotated tweets related to COVID-19, mainly in English, have been released [<a href="ch001.xhtml#B36-informatics-11-00024" class="html-bibr">36</a>,<a href="ch001.xhtml#B37-informatics-11-00024" class="html-bibr">37</a>,<a href="ch001.xhtml#B38-informatics-11-00024" class="html-bibr">38</a>,<a href="ch001.xhtml#B39-informatics-11-00024" class="html-bibr">39</a>,<a href="ch001.xhtml#B40-informatics-11-00024" class="html-bibr">40</a>]. However, since the behavior of social media users also varies with language [<a href="ch001.xhtml#B41-informatics-11-00024" class="html-bibr">41</a>], having datasets in various languages besides English is crucial. Therefore, efforts have been made to compile multilingual corpora [<a href="ch001.xhtml#B42-informatics-11-00024" class="html-bibr">42</a>,<a href="ch001.xhtml#B43-informatics-11-00024" class="html-bibr">43</a>] as well as language-specific datasets such as Portuguese [<a href="ch001.xhtml#B44-informatics-11-00024" class="html-bibr">44</a>,<a href="ch001.xhtml#B45-informatics-11-00024" class="html-bibr">45</a>], Arabic [<a href="ch001.xhtml#B46-informatics-11-00024" class="html-bibr">46</a>,<a href="ch001.xhtml#B47-informatics-11-00024" class="html-bibr">47</a>], French [<a href="ch001.xhtml#B48-informatics-11-00024" class="html-bibr">48</a>], among others [<a href="ch001.xhtml#B49-informatics-11-00024" class="html-bibr">49</a>,<a href="ch001.xhtml#B50-informatics-11-00024" class="html-bibr">50</a>,<a href="ch001.xhtml#B51-informatics-11-00024" class="html-bibr">51</a>]. For the Spanish language, there are annotated tweet datasets for tasks such as hate speech detection [<a href="ch001.xhtml#B52-informatics-11-00024" class="html-bibr">52</a>], aggression detection [<a href="ch001.xhtml#B53-informatics-11-00024" class="html-bibr">53</a>], LGBT-phobia detection [<a href="ch001.xhtml#B54-informatics-11-00024" class="html-bibr">54</a>], and automatic stance detection [<a href="ch001.xhtml#B55-informatics-11-00024" class="html-bibr">55</a>], among others. However, to our knowledge, there is no manually annotated public corpus for the sentiment polarity of COVID-19-related tweets in Spanish. Given that research tends to use an automatic labeling process. Like the work by Contreras mentioned above [<a href="ch001.xhtml#B35-informatics-11-00024" class="html-bibr">35</a>]. Therefore, we present a corpus with a manual labeling process and an annotation guideline. Furthermore, we provided an extensive analysis of the agreement between the annotators.
</div>
</section>
<section id="sec3-informatics-11-00024" type="">
<h2 id="data-collection-and-annotation-the-sent-covid-corpus" data-nested="1">3. Data Collection and Annotation: The SENT-COVID Corpus</h2>
<div class="html-p">
We collected COVID-19 tweets by implementing the Twitter API in Python. The messages are from 1 April 2020, to the end of 2022. About 4,000,000 tweets were collected, including only messages labeled as written in Mexican Spanish. We also included tweets that were responses or retweets, i.e., the type and form of the tweet did not matter to the extraction and annotation process.
</div>
<div class="html-p">
Once the data was obtained, we filtered the messages with a dictionary of appropriate terms, hashtags, and mentions depending on the development of the pandemic. In the first lexicon, the terms focused on different variants of the word COVID-19 (<span class="html-italic">coronavirus, el virus, covid, lo del contagio</span>) and symptoms. (<span class="html-italic">dolor de cabeza agudo, cuerpo cortado, diarrea, fiebre [leve], tos [seca], dolor de garganta, altas temperaturas</span>). Regarding hastaghs, many of them were government messages or slogans, used to support their policies, such as <span class="html-italic">#QuedateEnCasa, #TecuidasTúNosCuidamosTodos, #SusanaDistancia</span>. This is shown in <a href="ch001.xhtml#informatics-11-00024-t001" class="html-table">Table 1</a>.
</div>
<div class="html-p">
After applying the initial filter, our corpus remained at only 4986 tweets. However, we removed 120 tweets that were not in Spanish and those that contained less than three words because they did not provide enough information to assign them any label. Therefore, our final corpus consists of 4799 tweets.
</div>
<section id="sec3dot1-informatics-11-00024" type="">
<h3 id="annotation-protocol" class="html-italic" data-nested="2">3.1. Annotation Protocol</h3>
<div class="html-p">
We created an annotation guideline, summarized in this section, based on the polarity of sentiments. This describes how we labeled tweets and the criteria we used to categorize sentiments into three classes: positive, negative, and neutral. Each tweet in the corpus was manually assigned to one of the three categories.
</div>
<div class="html-p">
We used as reference the Robert Plutchik’s description of the eight primary emotions [<a href="ch001.xhtml#B56-informatics-11-00024" class="html-bibr">56</a>]-anger, fear, sadness, disgust, surprise, anticipation, trust, and joy. This allowed us to describe the polarity categories as follows.
</div>
<ul>
<li><div class="html-p">
<span class="html-small-caps">Positive Tags</span>. Positive tags are used to identify tweets that communicate joy/trust. Positive tweets are characterized by:
</div>
<ul>
<li><div class="html-p">
Predominance of pleasure or well-being:
</div>
<ul>
<li><div class="html-p">
‘No se ustedes pero yo he sido muy feliz durante esta cuarentena’.
</div>
<div class="html-p">
[I don’t know about you but I have been very happy during this quarantine].
</div></li>
<li><div class="html-p">
‘jaja Ana me acaba de alegrar la cuarentena’.
</div>
<div class="html-p">
[haha Ana just made my quarantine happy].
</div></li>
</ul></li>
<li><div class="html-p">
Cultivation of personal strengths and virtues that lead to happiness.
</div>
<ul>
<li><div class="html-p">
‘Desde que inició la cuarentena le ando dando duro al ejercicio y a la dieta’.
</div>
<div class="html-p">
[Since the quarantine began, I have been doing exercise and diet.]
</div></li>
<li><div class="html-p">
‘Algunos están estresados, preocupado yo digo #GraciasCuarentena porq me ha hecho valorar tanto, porque la salud es primero y solo se valora cuando se pierde’.
</div>
<div class="html-p">
[Some are stressed or worried but I say #ThanksQuarantine cause it has made me appreciate a lot of things, since health comes first and health is only appreciated when lost.]
</div></li>
</ul></li>
<li><div class="html-p">
Optimization of health, psychological resilience and promotion of efficient, flexible and creative reasoning.
</div>
<ul>
<li><div class="html-p">
‘Si, en una época de crisis económica, saqué adelante mi economía con una olla de tamales y no me avergüenzo. Fui muy feliz; trabaje duro; hice muchos amigos y sobre todo aprendí a ser humilde’.
</div>
<div class="html-p">
[Yes, in a time of economic crisis, I managed my economy with a pot of tamales and I am not ashamed. I was very happy; I worked hard; I made many friends and above all I learned to be humble.]
</div></li>
<li><div class="html-p">
‘Con esta cuarentena aprendí apreciar el hoy, reírse de uno mismo, valorar a los que están, animarse, dar amor porque si confiar porque si, y perdonar porque si. Estos días me motivaron a …’.
</div>
<div class="html-p">
[With this quarantine I learned to appreciate the present, to laugh at myself, to value those who are with me, to cheer up, to give love just because, trust just because and forgive just because. These days motivated me to …]
</div></li>
</ul></li>
<li><div class="html-p">
Motivation to achieve the life goals people set for themselves.
</div>
<ul>
<li><div class="html-p">
‘Compré un vestido para una boda en la playa, sin invitación a ninguna boda en la playa y en plena cuarentena. Me gusta soñar’.
</div>
<div class="html-p">
[I bought a dress for a beach wedding, with no invitation to any beach wedding and in the middle of quarantine. I like to dream.]
</div></li>
</ul></li>
</ul></li>
<li><div class="html-p">
<span class="html-small-caps">Negative Tags.</span> Negative tags are used to identify tweets that communicate anger, fear, or sadness. Negative tweets are characterized by:
</div>
<ul>
<li><div class="html-p">
Expressing situations where there is something unpleasant or violent
</div>
<ul>
<li><div class="html-p">
‘Oigan, me duele el pecho medio raro ¿Es síntoma de covid o ya me volví loca?’
</div>
<div class="html-p">
[Hey, my chest hurts kind of weird. Is it a symptom of covid or have I gone crazy? ]
</div></li>
</ul></li>
<li><div class="html-p">
Representing a barrier to achieving a goal or requires the mobilization of resources for the creation and elaboration of plans to resolve a situation
</div>
<ul>
<li><div class="html-p">
‘El número de contagios de COVID-19 en México puede ser hasta 50 veces más que los reportados: Julio Frenk’.
</div>
<div class="html-p">
[The number of COVID-19 infections in Mexico might be even 50 times more than the number reported: Julio Frenk]
</div></li>
<li><div class="html-p">
‘La contingencia continua y las necesidades son cada vez en más familias’. [The contingency continues and the needs are increasing in more families]
</div></li>
</ul></li>
</ul></li>
<li><div class="html-p">
<span class="html-small-caps">Neutral Tags.</span> Neutral cues are used to identify tweets that do not communicate any emotion/sentiment. Neutral tweets are characterized by:
</div>
<ul>
<li><div class="html-p">
Not communicating a specific message.
</div>
<ul>
<li><div class="html-p">
‘@ExpansionMx El virus no tiene nada que ver. Lo que sí, es el Gobierno!’
</div>
<div class="html-p">
[The virus has nothing to do. But the government has something to do with it!]
</div></li>
</ul></li>
<li><div class="html-p">
Expressing some kind of doubt, without attacking something or someone.
</div>
<ul>
<li><div class="html-p">
‘No se supone que suspendieron parquímetros por contingencia? @ecoParq @Claudiashein’.
</div>
<div class="html-p">
[Aren’t parking meters supposed to be suspended due to the contingency?]
</div></li>
<li><div class="html-p">
‘Mateo cuando escucho la pregunta de la vacuna del jabón dijo—pero como de jabón? Que no sabe que en las vacunas van la mitad de los virus??’
</div>
<div class="html-p">
[When Mateo heard the question about the soap vaccine, said—but how about soap? Who doesn’t know that half of the viruses are in vaccines??]
</div></li>
</ul></li>
</ul></li>
</ul>
<div class="html-p">
To annotate the corpus, we had a previous step. We designed an experiment to check which is the best way to proceed to categorize the messages in type of corpus. Two students were asked to label a sample of 100 tweets with the tags positive, negative, or neutral, without any guidance but based solely on their own opinions.
</div>
<div class="html-p">
At the same time, two more students were in charge of labeling the same messages following the guide that had been developed. The analysis of the results showed that the guide favors the agreement between the annotators. Thus, we moved on to a second phase with new students, with the help of the guide. Therefore, the final process of tagging involved three annotators who labeled the tweets according to our created guide.
</div>
</section>
<section id="sec3dot2-informatics-11-00024" type="">
<h3 id="data-statementannotators-data" class="html-italic" data-nested="2">3.2. Data Statement/Annotators Data</h3>
<div class="html-p">
We followed the guidelines specified by [<a href="ch001.xhtml#B57-informatics-11-00024" class="html-bibr">57</a>] to create this data statement.
</div>
<dl>
<dt>A.</dt>
<dd><div class="html-p">
Curation Rationale: We collected tweets from the widely used social media platform, Twitter, due to its convenience in acquiring concise statements from the general user population on diverse topics within a digital context. We used specific key terms and hashtags commonly used to refer to the pandemic.
</div>
</dd>
<dt>B.</dt>
<dd><div class="html-p">
Language variety: We systematically extract a set of tweets by filtering for specific keywords and ensuring that they are in Spanish and geographically associated with the designated region (Mexico).
</div>
</dd>
<dt>C.</dt>
<dd><div class="html-p">
Tweet author demographic: The data is likely to come from a wide range of users with different characteristics such as age, gender, nationality, race, socioeconomic status and educational backgrounds. This is because we collected the data using Twitter’s data collection API, which is expected to have a diverse user base in Mexico.
</div>
</dd>
<dt>D.</dt>
<dd><div class="html-p">
Annotator demographic: We selected three annotators from the UNAM Language Engineering Group to label the tweets. All of them were undergraduate students from this university, between 20 and 25 years old, Spanish native speakers with Mexican nationality and residence.
</div>
</dd>
<dt>E.</dt>
<dd><div class="html-p">
Speech Situation: All tweets are about the pandemic. The years of extraction are 2020 to 2022.
</div>
</dd>
<dt>F.</dt>
<dd><div class="html-p">
Text characteristics: The tweets collected come from a pandemic context, so they followed a specific global trend. They could be a unique tweet or a response to another tweet. The limited length of tweets is an important factor to consider, as is the social media policy. All the data are public.
</div>
</dd>
<dt>G.</dt>
<dd><div class="html-p">
Recording Quality: We extracted the tweets from the Twitter API.
</div>
</dd>
<dt>H.</dt>
<dd><div class="html-p">
Ethical Statements: We collected all tweets for academic use according to Twitter’s privacy policy.
</div>
</dd>
</dl>
</section>
<section id="sec3dot3-informatics-11-00024" type="results">
<h3 id="results-of-the-annotation-process" class="html-italic" data-nested="2">3.3. Results of the Annotation Process</h3>
<div class="html-p">
The “interrater reliability” [<a href="ch001.xhtml#B58-informatics-11-00024" class="html-bibr">58</a>] is a measurement of the extent to which data annotators (raters) assign the same score or label to the same variable. Frequently, this quantity is calculated by Cohen’s kappa coefficient (<math display="inline"><semantics> <mi>κ</mi> </semantics></math>). We assessed the agreement of the corpus annotators by using the Inter-Annotator Agreement (IAA) score, where Cohen’s Kappa statistical measure is used in its definition. So, the annotators who did not use the guide presented a Cohen’s <math display="inline"><semantics> <mrow> <mo>(</mo> <mi>κ</mi> <mo>)</mo> </mrow> </semantics></math> score of 0.178, a very slight agreement. In contrast, the annotators that used the guide presented a Cohen’s <math display="inline"><semantics> <mrow> <mo>(</mo> <mi>κ</mi> <mo>)</mo> </mrow> </semantics></math> score of 0.4369, a moderate agreement indeed. <a href="ch001.xhtml#informatics-11-00024-t002" class="html-table">Table 2</a> below shows the percent of agreement and Cohen’s (<math display="inline"><semantics> <mi>κ</mi> </semantics></math>) score for each pair of annotators who did not use the guide. Compared to this, <a href="ch001.xhtml#informatics-11-00024-t003" class="html-table">Table 3</a> below shows the percent of agreement and Cohen’s <math display="inline"><semantics> <mrow> <mo>(</mo> <mi>κ</mi> <mo>)</mo> </mrow> </semantics></math> score for each pair of annotators who used the guide.
</div>
<div class="html-p">
Cohen’s Kappa suits very well for estimating the agreement between not more than two annotators. So, given the characteristics of our annotation process, where we have at least three annotators for each tweet, we used Fleiss’ kappa to measure the agreement between the three annotators that used a guide. This analysis resulted in an overall agreement of 0.4369, reflecting a moderate inter-annotator agreement.
</div>
<div class="html-p">
Having three annotators for each tweet allowed us to identify the labels with the majority vote. By having three independent labels where there was disagreement, we could seek agreement on two out of three to set the repeated label as the definitive label in the final corpus. At the end of the annotation process, some tweets did not have an assigned label since the three annotators disagreed. For this reason, it was necessary that all the annotators together decide on the final label for the tweets without agreement.
</div>
<div class="html-p">
Our final corpus consists of 4799 tweets, of which 1834 (38.21%) contain negative sentiments, 1126 (23.46%) contain positive sentiments, and 1839 (38.33%) contain neutral sentiments. Finally, <a href="ch001.xhtml#informatics-11-00024-t004" class="html-table">Table 4</a> presented below shows the general statistics computed from word counts on each tweet of our corpus. The minimum number of words across all categories is three. We can ascertain the range of words in each category by computing the maximum. Tweets with a negative sentiment have the highest maximum number of words, while those with a positive sentiment have a lower range. The maximum count of words varies significantly between categories. However, the average number of words is quite similar. Additionally, on average, tweets contain a low number of words, which could explain why the standard deviation of the count is so high.
</div>
</section>
</section>
<section id="sec4-informatics-11-00024" type="">
<h2 id="sentiment-analysis-methods" data-nested="1">4. Sentiment Analysis Methods</h2>
<div class="html-p">
Once the corpus has been annotated, the next step is to process the raw tweets into data that we can use for classification. This section outlines the sentiment analysis methods we evaluate to build a classification model on COVID-19-related tweets.
</div>
<div class="html-p">
<a href="ch001.xhtml#informatics-11-00024-f001" class="html-fig">Figure 1</a> shows the workflow of our experimentation, starting with the preprocessing of the text data before feature extraction, which includes removing digits, separating words based on patterns, normalizing words, wrapping special tokens, transcribing emojis, lemmatizing, and removing stop words. The results of processing the tweets using these methods are presented, demonstrating the transformation from raw tweets to processed text. The feature extraction techniques we evaluated are the Bag of Words (BoW) model, Term Frequency-Inverse Document Frequency (Tf-Idf), word embedding, and phrase modeling. Once the features were extracted using the BOW or <span class="html-italic">n</span>-grams models, we used feature selection techniques (explained in <a href="ch001.xhtml#sec5dot1-informatics-11-00024" class="html-sec">Section 5.1</a> and <a href="ch001.xhtml#sec5dot2-informatics-11-00024" class="html-sec">Section 5.2</a>) to reduce the vector dimensions while keeping the highest amount of information as possible. The models and algorithms for text classification we evaluated include ready-to-use libraries such as TextBlob, VADER, and the Pysentimiento Toolkit. With respect to the supervised learning models, we evaluated Logistic Regression, Naive Bayes Classification, Support Vector Machines, and Multilayer Perceptron (MLP). Moreover, transformer networks are also explored. Finally, for evaluating the sentiment classification models, we used cross-validation for the traditional machine learning algorithms that required training; for the ready-to-use libraries, we used the entire datasets because no training is required; for the transformers’ models, we only did one training test split due to the computational cost of the experiments.
</div>
<section id="sec4dot1-informatics-11-00024" type="">
<h3 id="text-processing" class="html-italic" data-nested="2">4.1. Text Processing</h3>
<div class="html-p">
To build a Bag-of-Words (BoW) representation (we consider bi-grams and other structures in <a href="ch001.xhtml#sec4dot2-informatics-11-00024" class="html-sec">Section 4.2</a>) we compose a vocabulary of all unique words in the corpus following the process outlined below:
</div>
<div class="html-p">
<ul>
<li><div class="html-p">
Remove digits, double blanks, and line breaks: Extracting information from digits presents a challenge since they can represent different things such as magnitudes, time-dates, directions, etc. Consequently, digits are intentionally omitted.
</div></li>
<li><div class="html-p">
Separate words with patterns: Tweets contain misspellings or camel case writing in hashtags. So, we identified common patterns (such as dots before a capital letter or symbols like ‘#’ or ‘¿’) and separated them using regular expressions.
</div></li>
<li><div class="html-p">
Normalize words: We transformed text to lowercase and removed punctuation marks and other special characters. It would not have split properly if we had done this before the previous step. In addition, consecutive repeated characters (usually used for laughs) were minimized to two repetitions to prevent the formation of new tokens for words already present in the vocabulary.
</div></li>
<li><div class="html-p">
Wrap special tokens: Tweets often contain mentions, web links, and pictures. Thus, we identified common objects with general labels. For example, mentions to users were wrapped with the token ‘usuario’ and web links by the token ‘url’.
</div></li>
<li><div class="html-p">
Transcribe emojis to words: We convert emojis into words, positioning them properly within the tweet using the emoji (<a href="https://pypi.org/project/emoji/" class="uri">https://pypi.org/project/emoji/</a> accessed on 20 February 2024) Python module. All of these are normalized to lowercase.
</div></li>
<li><div class="html-p">
Lemmatize: We find a word’s dictionary form (or lemma). This process was done using the Spacy library. This allowed us to reduce vocabulary size by avoiding multiple tokens for different inflections of the same word.
</div></li>
<li><div class="html-p">
Remove Stop words: Common words that do not carry semantic information, called ‘stop words’, are removed to reduce the vocabulary size.
</div></li>
</ul>
</div>
<div class="html-p">
In the following, we show the results of the comments processing using the described method. <a href="ch001.xhtml#informatics-11-00024-t005" class="html-table">Table 5</a> shows the original raw tweet in the first column and the processed text in the second column. After the process, the entire text is in lowercase. The links are changed to the token ‘url’, while the users mentioned are replaced with ‘usuarios’. Furthermore, emojis are converted into words and any repetition is avoided.
</div>
</section>
<section id="sec4dot2-informatics-11-00024" type="">
<h3 id="feature-extraction" class="html-italic" data-nested="2">4.2. Feature Extraction</h3>
<div class="html-p">
After processing the text, we investigated and experimented with different numerical data representations in the training stage. We compared some simple feature extraction techniques and algorithms to build a vocabulary from all words in the corpus. Also, we tried out some pre-trained embedding models, which provide a more complex yet more effective way of extracting text features.
</div>
<section id="sec4dot2dot1-informatics-11-00024" type="">
<h4 id="bag-of-words-model" data-nested="3">4.2.1. Bag of Words Model</h4>
<div class="html-p">
In the BoW model, each token in the text corresponds to a given dimension (feature) in a vector representation. Each token in a text will have a weight that can be given by the number of occurrences of a word in the text (term frequency) or by multiplying the number of occurrences of a word in the entire corpus with the occurrences of a word in the text (Tf-Idf).
</div>
<ul>
<li><div class="html-p">
Term Frequency: To transform tweets to vector representations, we used the Scikit-learn library (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer" class="uri">https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html?highlight=countvectorizer#sklearn.feature_extraction.text.CountVectorizer</a> (accessed on 20 February 2024)). The size of the vector of each text is equal to the size of the vocabulary of the corpus, and, as mentioned before, the value of each dimension is the number of times that the words appear in the tweet. We extracted different feature sets by modifying the following parameters:
</div>
<ul>
<li><div class="html-p">
n_gram_range: It allows us to know if each token is formed by singular words or n_grams (<span class="html-italic">n</span>-gram is a continuous sequence of n items from a given sequence of text) of words (this tries to preserve local ordering of words but at the cost of highly increasing the number of features).
</div></li>
<li><div class="html-p">
min_df: It is the minimum number of times that a word must appear in all documents to be considered part of the vocabulary. Many words are present in the corpus with few appearances, so small variations of this parameter highly increase vocabulary size.
</div></li>
<li><div class="html-p">
stop_words: We used the stopword list provided by the Nltk library, which consists of 313 words.
</div></li>
</ul>
<div class="html-p">
<a href="ch001.xhtml#informatics-11-00024-t006" class="html-table">Table 6</a> shows in the last column the vocabulary size extracted by modifying the parameters mentioned above. The first column indicates whether the text is lemmatized or not, the second column indicates whether stopwords are removed, the third column indicates the n_gram range, and the fourth column shows the value of the minimum document frequency.
<div id="informatics-11-00024-t006" class="html-table-wrap" data-position="anchor">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t006">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t006" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 6.</strong> Vocabulary size by different parameter settings.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t006" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 6.</strong> Vocabulary size by different parameter settings.
</div>
<table>
<thead>
<tr>
<th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-center">
Text
</th>
<th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-center">
Stopwords
</th>
<th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-center">
<span class="html-italic">n</span>-Gram Range
</th>
<th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-center">
Mindf
</th>
<th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-center">
Vocabulary Size
</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Normalized
</td>
<td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Yes
</td>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
1_gram
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
4343
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2797
</td>
</tr>
<tr>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2_gram
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
10,674
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
5919
</td>
</tr>
<tr>
<td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
No
</td>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
1_gram
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
4198
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2664
</td>
</tr>
<tr>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2_gram
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
6456
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3614
</td>
</tr>
<tr>
<td rowspan="8" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Lemmatized
</td>
<td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Yes
</td>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
1_gram
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3663
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2422
</td>
</tr>
<tr>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2_gram
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
10,213
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
5812
</td>
</tr>
<tr>
<td rowspan="4" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
No
</td>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
1_gram
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3605
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2369
</td>
</tr>
<tr>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2_gram
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
2
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
6533
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
3629
</td>
</tr>
</tbody>
</table>
</div>
</div></li>
<li><div class="html-p">
Tf-Idf: The Term frequency weighting treats all words as having the same importance. This can be improved by considering the significance of each term in the corpus. To give each term a weight. A high Tf-Idf score indicates that a word is present in the tweet but not in many other tweets in the corpus. However, a low Tf-Idf value implies that most tweets frequently use the word. This process emphasizes words that are significant to the tweet. <a href="ch001.xhtml#informatics-11-00024-t007" class="html-table">Table 7</a> shows terms with the smallest and largest Tf-Idf values. This, using bigram normalized tokenization with stop words included and mindf = 2:
</div></li>
</ul>
</section>
<section id="sec4dot2dot2-informatics-11-00024" type="">
<h4 id="word-embedding-and-phrase-modeling" data-nested="3">4.2.2. Word Embedding and Phrase Modeling</h4>
<div class="html-p">
Word embedding is a technique for representing words as vectors. The purpose is to reduce the high-dimensional word features to low-dimensional feature vectors while preserving the context similarity.
</div>
<ul>
<li><div class="html-p">
Word2Vec: is a popular natural language processing technique that represents words as dense vectors in a continuous vector space. It relies on the assumption that words with similar meanings often appear in similar contexts. Word2Vec uses a neural network to learn these word embeddings, capturing semantic relationships and similarities between words [<a href="ch001.xhtml#B59-informatics-11-00024" class="html-bibr">59</a>]. This allows for word representations that can be used in various NLP tasks like sentiment analysis, machine translation, and information retrieval. CBOW (Continuous Bag of Words) and Skip-gram are the two fundamental architectures in Word2Vec. CBOW aims to predict a target word based on its surrounding context words. Skip-gram predicts the context words given a target word. Both architectures contribute to creating meaningful word embeddings. In the Gensim library, we can specify whether to use CBOW or Skip-gram. We decided to use the Skip-gram technique since it performed better in our experiments.
</div></li>
<li><div class="html-p">
Doc2Vec: extends the principles of Word2Vec to generate fixed-length vector representations for entire documents, such as sentences, paragraphs, or even entire documents. Doc2Vec employs neural networks to learn these document embeddings while considering the context of words within the document. It assigns a unique vector to each document, capturing its semantic content, and allowing for similarity comparisons between documents [<a href="ch001.xhtml#B60-informatics-11-00024" class="html-bibr">60</a>].
</div></li>
<li><div class="html-p">
Phrase Modeling: The Gensim library offers phrase detection, similar to the <span class="html-italic">n</span>-gram representation. However, instead of getting all <span class="html-italic">n</span>-grams by sliding the window, it detects frequently used phrases and sticks them together. Hence, we integrated the vector representation of sentences to capture the collective meaning of a group of words, rather than merely aggregating the meanings of individual words. This process allows us to extract many reasonable phrases while keeping the vocabulary size in a manageable size [<a href="ch001.xhtml#B59-informatics-11-00024" class="html-bibr">59</a>]. We built 2-gram and 3-gram models to detect and combine frequently used two and three-word phrases within the corpus. After we obtained the corpus with the phrases, we did the same Doc2Vec process previously applied to unigram tokens. Thus, we present the results for each, using both the Distributed Bag of Words (DBOW) and Distributed Memory (DM), as well as their combination. DBOW and DM are two distinct training algorithms used to generate vector representations of documents.
</div></li>
</ul>
<div class="html-p">
<a href="ch001.xhtml#informatics-11-00024-t008" class="html-table">Table 8</a> shows the phrases detected by Gensim’s phrase detection algorithm in a given tweet. We can see that unigram yields 13 tokens, while 2-gram and 3-gram yields 10 and 9 tokens, respectively. As we can see, phrases like ‘<span class="html-italic">no es justo</span>’ and ‘<span class="html-italic">quedate en casa</span>’ become a trigram but the rest of the tokens remain as unigrams. This is because the algorithm only extracts the most significant <span class="html-italic">n</span>-grams.
</div>
</section>
<section id="sec4dot2dot3-informatics-11-00024" type="">
<h4 id="spanish-pre-trained-bert-models" data-nested="3">4.2.3. Spanish Pre-Trained BERT Models</h4>
<div class="html-p">
BERT models are considered state-of-the-art models for various NLP tasks that involve text representation. BERT possesses the significant benefit of supporting transfer learning. These language models have undergone extensive training over several days on robust machines over a large amount of text from platforms like Wikipedia and news websites. A pre-trained model can then be fine-tuned to align with our specific classification task.
</div>
<div class="html-p">
We used some variants of BERT models in Spanish. These are particularly useful since they have been built for NLP tasks with high-dimensional analysis. Spanish models are hard to come by, and when available, they are frequently developed using substantial proprietary datasets and resources. As a result, the relevant algorithms and techniques are restricted to large technology corporations. However, a fundamental objective of these models is to promote openness by making them available as open-source resources. Examples of these models are
</div>
<ul>
<li><div class="html-p">
<span class="html-small-caps">BERTIN:</span> Series of BERT-based models in Spanish, the current model hub points to the best of RoBERTa-base models trained from scratch on the Spanish portion of mC4 using Flax—a neural network library ecosystem for JAX designed for flexibility [<a href="ch001.xhtml#B61-informatics-11-00024" class="html-bibr">61</a>].
</div></li>
<li><div class="html-p">
<span class="html-small-caps">RoBERTuito:</span> A language model for user-generated content in Spanish, trained following the RoBERTa guidelines on 500 million tweets. RoBERTuito comes cased and uncased [<a href="ch001.xhtml#B62-informatics-11-00024" class="html-bibr">62</a>].
</div></li>
<li><div class="html-p">
BETO: Another model trained on a large Spanish corpus that is size similar to a BERT-Base and was trained with the whole word masking technique, which outperforms some other models [<a href="ch001.xhtml#B63-informatics-11-00024" class="html-bibr">63</a>].
</div></li>
</ul>
<div class="html-p">
In recent years, there have been more advances in pre-trained BERT models [<a href="ch001.xhtml#B64-informatics-11-00024" class="html-bibr">64</a>]. As a result of their growing popularity, several versions of lighter and faster versions of BERT (e.g., DistilBERT) have been made available to accelerate training and inference processes. However, there is a lack of these for languages other than English.
</div>
</section>
</section>
<section id="sec4dot3-informatics-11-00024" type="">
<h3 id="models-and-algorithms" class="html-italic" data-nested="2">4.3. Models and Algorithms</h3>
<section id="sec4dot3dot1-informatics-11-00024" type="">
<h4 id="ready-to-use-libraries" data-nested="3">4.3.1. Ready-to-Use Libraries</h4>
<div class="html-p">
We evaluated NLP libraries that do not need to train machine learning-specific models:
</div>
<div class="html-p">
<ul>
<li><div class="html-p">
TextBlob: A Python library that allows users to perform various textual data processing tasks. For sentiment analysis, this tool uses a lexicon-based approach. It makes use of a vocabulary consisting of around 3000 words in English along with their corresponding scores. Thus, for a given text, the TextBlob sentiment analyzer returns two outputs. The polarity value belongs to [−1, 1], where −1 indicates a negative sentiment text, and +1 a positive one. On the other hand, the subjectivity value ranges from 0 to 1, with 0 indicating an objective text, while 1 representing a subjective text. <a href="ch001.xhtml#informatics-11-00024-t009" class="html-table">Table 9</a> shows examples of how TextBlob works with different sentences in Spanish. Prior analysis, these sentences were previously translated into English.
</div></li>
<li><div class="html-p">
VADER (Valence Aware Dictionary and sEntiment Reasoner): Similar to TextBlob, this tool uses a sentiment analyzer that is based on a lexicon. But, this tool is specifically tuned to the sentiments expressed in social media since its lexicon (of approximately 9000 token features) includes slangs and emoticons [<a href="ch001.xhtml#B65-informatics-11-00024" class="html-bibr">65</a>]. The words in the lexicon have a valence score that ranges from extremely positive [4] to extremely negative [−4], with [0] representing neutral sentiment. These scores are determined based on the semantic orientation of the lexical features. The <a href="ch001.xhtml#informatics-11-00024-t010" class="html-table">Table 10</a> illustrates the functioning of VADER. The first column displays the input text. The ‘compound’ column shows the normalized sum of the valence scores of each word in the text. A value of [−1] indicates a negative sentiment, while [+1] indicates a positive polarity in the text. The columns ‘neg’, ‘neu’, and ‘pos’ indicate the percentage likelihood of the text belonging to the negative, neutral, or positive class, respectively. Vader has achieved good results in English texts due to the quality of its lexicon [<a href="ch001.xhtml#B66-informatics-11-00024" class="html-bibr">66</a>]. However, our texts are in Spanish, so we need a Spanish lexicon (and this resource is not easy to obtain specifically tailored to Latin Spanish and social media) or to translate the tweet into English. We opted for the SentiSense Lexicon [<a href="ch001.xhtml#B67-informatics-11-00024" class="html-bibr">67</a>] which contains a list of Spanish words classified according to their emotional connotation and information about the intensity of the emotion transmitted by each word.
</div></li>
<li><div class="html-p">
Pysentimiento Multilingual Toolkit: A very useful transformer-based library for Text Mining and Social NLP tasks such as sentiment analysis and hate speech detection [<a href="ch001.xhtml#B68-informatics-11-00024" class="html-bibr">68</a>] for text classification in Spanish this library uses BETO (<a href="https://github.com/dccuchile/beto" class="uri">https://github.com/dccuchile/beto</a> (accessed on 20 February 2024)) and RoBERTuito (<a href="https://github.com/pysentimiento/robertuito" class="uri">https://github.com/pysentimiento/robertuito</a> (accessed on 20 February 2024)) language models. Pysentimiento is trained with ‘<span class="html-italic">pos</span>’, ‘<span class="html-italic">neg</span>’, ‘<span class="html-italic">neu</span>’ labels using the ‘<span class="html-italic">TASS-2020 task-1</span>’ corpus (<a href="http://tass.sepln.org/2020/" class="uri">http://tass.sepln.org/2020/</a> (accessed on 20 February 2024)) merged with the Spanish subsets for each dialect, summing up to 6000 tweets.
</div></li>
</ul>
</div>
</section>
<section id="sec4dot3dot2-informatics-11-00024" type="">
<h4 id="machine-learning-algorithms" data-nested="3">4.3.2. Machine Learning Algorithms</h4>
<div class="html-p">
We also trained and evaluated supervised classification models with the SENT-COVID corpus. This was done under the hypothesis that the models trained with the corpus outperform the ready-to-use libraries. The algorithms we evaluated were:
</div>
<div class="html-p">
<ul>
<li><div class="html-p">
Logistic Regression: A generalized linear model widely employed in machine learning applications for classification purposes. It is especially useful for text mining tasks because of its ability to handle large, sparse data sets with robust performance [<a href="ch001.xhtml#B69-informatics-11-00024" class="html-bibr">69</a>]. Logistic regression is used to compress the output of a given set of data into discrete values to a categorical response value. As <math display="inline"><semantics> <mover accent="true"> <mi>y</mi> <mo>^</mo> </mover> </semantics></math> output is the probability that the input instance belongs to a certain class, we use a binary ‘one vs. the-rest’ model for each class. This is interpreted as the probability of being or not being within the class. Hence, three binary classifiers [‘neg’, ‘neu’, ‘pos’] are created, which we trained with the following parameters using the scikit-learn library:
</div>
<ul>
<li><div class="html-p">
Regularization: We use ‘<span class="html-italic">L2</span>’ penalty (this is by using the usual Euclidian distance when calculating the norm) on estimated coefficients (as Ridge regression), which can be controlled using the ‘<span class="html-italic">C</span>’ parameter. Higher values of ‘<span class="html-italic">C</span>’ correspond to reduced regularization, allowing the model to prioritize fitting the training data optimally. In contrast, for lower values, the model gives priority to finding coefficients close to zero, even if this means a slightly reduced fit to the training data.
</div></li>
<li><div class="html-p">
Multi_class: The training algorithm uses the one-vs-rest scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’.
</div></li>
<li><div class="html-p">
Solver: Algorithm to use in the optimization problem. Only ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support L2 regularization with primal formulation as we selected for penalty.
</div></li>
<li><div class="html-p">
Formulation: (Dual is only implemented for L2 penalty with ‘liblinear’ solver) We prefer Primal when training data instances are greater than the number of features and Dual for other cases.
</div></li>
</ul></li>
<li><div class="html-p">
Naive Bayes: Naive Bayes (NB) classifier is a probabilistic classifier based on Bayes’s theorem for prior distributions. It is applied in problems such as spam filtering, text classification, and hybrid recommender systems [<a href="ch001.xhtml#B69-informatics-11-00024" class="html-bibr">69</a>]. The NB classifier has been shown to be optimal and efficient in many machine learning text classification tasks (especially with independence between document labels assumptions) [<a href="ch001.xhtml#B70-informatics-11-00024" class="html-bibr">70</a>,<a href="ch001.xhtml#B71-informatics-11-00024" class="html-bibr">71</a>]. According to our needs, we decided to use the version of Multinomial Naive Bayes which is used specifically for discrete cases (such as word counts in documents) and incorporates the assumption that characteristics follow a multinomial distribution. In sentiment analysis this model can work better since it allows text data modeled as word frequencies to be handled more efficiently and this makes it particularly useful. The BoW model is used as a feature model for implementation because it has been found to produce results comparable to those obtained by Support Vector Machines and logistic regression algorithms. The predicted label <math display="inline"><semantics> <mover accent="true"> <mi>y</mi> <mo>^</mo> </mover> </semantics></math> is the <span class="html-italic">y</span> that maximizes the probability of Y given X. We considered the following parameters for NB:
</div>
<ul>
<li><div class="html-p">
alpha: We conducted tests using the smoothing parameter, which has a default value 1.
</div></li>
<li><div class="html-p">
force_alpha: If ‘False’ is set and alpha is less than <math display="inline"><semantics> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>10</mn> </mrow> </msup> </semantics></math>, it sets alpha to <math display="inline"><semantics> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>10</mn> </mrow> </msup> </semantics></math>. If ‘True’, alpha remains unchanged. In this case, if alpha is too close to zero, it may cause numerical errors. Besides, when alpha = 0 and force_alpha = true means no smoothing.
</div></li>
<li><div class="html-p">
fit_prior: Whether to learn class prior probabilities or not. If false, a uniform prior is used.
</div></li>
<li><div class="html-p">
class_prior: The prior class probabilities of the model. If not specified, these priors are adjusted according to the data frequency. This is useful for an imbalanced class distribution.
</div></li>
</ul></li>
<li><div class="html-p">
Support Vector Machines (SVM): Various studies show that SVM outperforms other classification algorithms [<a href="ch001.xhtml#B72-informatics-11-00024" class="html-bibr">72</a>] for text classification problems. The SVM objective (primal) is to find the decision surface that maximizes the margin between the data points from different classes. In the linear case, this classifier rewards the amount of separation between classes by applying a sign function to produce a categorical output. Consequently, we handle multi-class classification by creating single linear binary classifiers for each class. Once this criterion has been defined as a decision rule, we define the decision boundaries and corresponding classification margins for each classifier. The most proficient classifier is the ‘linear support vector machine’, which is characterized by the maximum margin of separation between points.
</div>
<div class="html-p">
The advantage of this method is that slight modifications to the data for a particular document will not alter the label that the classifier assigns. So, the approach is more resistant to noise or perturbations. Also, it is still effective even when the number of features is greater than the number of data instances, and it only requires a limited number of training data to learn the decision function, making it memory efficient.
</div>
<div class="html-p">
We chose LinearSVC in scikit-learn rather than SVC implemented in terms of liblinear rather than libsvm. So, the choice of penalties and loss functions has more flexibility and scales better to large numbers of samples. Thus, we tested the following parameters:
</div>
<ul>
<li><div class="html-p">
Regularization: We applied <span class="html-italic">L2</span> penalty with ‘<span class="html-italic">C</span>’ = 1 same as in logistic regression. In order to determine the significance of correctly labeling individual documents, smaller values of ‘C’ (more regularization) indicate a greater tolerance for errors on individual documents.
</div></li>
<li><div class="html-p">
Kernel: A linear kernel usually works better when using text data, but we also tested polynomial kernels.
</div></li>
<li><div class="html-p">
Multi_class: We preferred learning fewer number of classifiers so we used one-versus-rest over one-versus-one.
</div></li>
</ul></li>
<li><div class="html-p">
Multilayer Perceptron (MLP): It is one of the simplest neural network models. These networks have achieved remarkable results on various classification problems, from object classification in images to fast, accurate machine translation [<a href="ch001.xhtml#B73-informatics-11-00024" class="html-bibr">73</a>]. Their approach is similar to logistic regression but takes a step beyond by adding ‘hidden layers’ contained by ‘hidden units’. Each layer performs a non-linear transformation (called activation functions) in the input features. These functions adopt ‘S-shaped’ curves, and various forms of them were considered during data training. Introducing this extra hidden layer renders the prediction model more complex compared to logistic regression. However, the computational cost is greater, as predicting the response necessitates computing a distinct initial weighted sum of feature values for each hidden unit.
</div>
<ul>
<li><div class="html-p">
Hidden layer sizes: A list with one element for each hidden layer that gives the number of hidden units for that layer. We passed two values of 100 (two hidden layers, 100 units per layer)
</div></li>
<li><div class="html-p">
Activation function: Activation function for the hidden layer. Options include the logistic sigmoid, hyperbolic tan, or rectified linear unit functions. In this work, we used the rectified linear unit function.
</div></li>
<li><div class="html-p">
Solver: The solver for weight optimization. ‘lbfgs’ is an optimizer in the family of quasi-Newton methods, ‘sgd’ refers to stochastic gradient descent, ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik [<a href="ch001.xhtml#B74-informatics-11-00024" class="html-bibr">74</a>]. The default solver ‘adam’ works pretty well on relatively large datasets in terms of both training time and validation score.
</div></li>
</ul></li>
</ul>
</div>
</section>
<section id="sec4dot3dot3-informatics-11-00024" type="">
<h4 id="transformers" data-nested="3">4.3.3. Transformers</h4>
<div class="html-p">
With their self-attention mechanisms, transformer-based models have brought a paradigm shift in natural language processing (NLP) [<a href="ch001.xhtml#B75-informatics-11-00024" class="html-bibr">75</a>]. Unlike traditional recurrent or convolutional neural networks, transformers can process entire sequences of input tokens in parallel. This parallel processing enables efficient computation of contextual representations. The multi-head self-attention mechanisms empower the model to assign importance to each token in the input sequence based on its contextual relevance to other tokens. This feature allows transformers to effectively capture long-range dependencies and contextual information, making them an ideal choice for tasks that demand an understanding of complex linguistic patterns, such as sentiment analysis. To use transformer-based models for sentiment analysis in Spanish, one typically fine-tunes a pre-trained transformer model on a labeled dataset of Spanish text. The pre-training process involves initializing the model’s parameters with weights learned from a large corpus of Spanish text. During fine-tuning, the model learns to adjust its parameters better to capture the nuances of sentiments, leveraging the contextual information encoded in the transformer’s self-attention mechanisms. Once the model is fine-tuned, it can be used to predict the sentiment of new text inputs by feeding them through the model and interpreting the output probabilities or scores assigned to each sentiment class.
</div>
<div class="html-p">
Using transformer pre-trained models is computationally expensive in training, limiting our ability to execute comprehensive cross-validation tests for score metrics or conduct extensive hyperparameter searches. The experiments were carried out with a single train-test split of the data, usinng 75% for training and 25% for testing. We created the neural network with a single hidden layer and a single output unit. Essential parameters such as input size, hidden units, output size, batch size, dropout, and learning rate were considered. Then, we randomly initialized the dummy input and the output target data (or tensor), and using built-in functions, we created a simple sequential model with an output sigmoid layer and defined the corresponding loss function. This computed the mean-squared error between the input, target, and optimizer. For the gradient descent, the ‘torch.optim’ package provides various optimization algorithms, so we used a stochastic gradient descent (SGD) optimizer. Finally, we defined the training loop with the following steps:
</div>
<div class="html-p">
<ul>
<li><div class="html-p">
Forward propagation: This computed the predicted <math display="inline"><semantics> <mover accent="true"> <mi>y</mi> <mo>^</mo> </mover> </semantics></math> label and calculated the current loss. This helped us see how the model trains over each epoch (we considered 5 epochs).
</div></li>
<li><div class="html-p">
Backward propagation: After each epoch, we set the gradients to zero before starting.
</div></li>
<li><div class="html-p">
Gradient descent: Finally, we updated model parameters by calling the optimizer function.
</div></li>
</ul>
</div>
</section>
</section>
</section>
<section id="sec5-informatics-11-00024" type="results">
<h2 id="results-of-sentiment-analysis-methods" data-nested="1">5. Results of Sentiment Analysis Methods</h2>
<div class="html-p">
We split the SENT-COVID corpus in a single train-test division randomly separated into approximately 3600 for training and 1200 for testing (75–25%) and used the same seed to preserve the same partitions in different experiments. <a href="ch001.xhtml#informatics-11-00024-t011" class="html-table">Table 11</a> shows the distribution of labels in each partition. It can be observed that both partitions maintain the same class distribution.
</div>
<div class="html-p">
We established a fundamental baseline for benchmarking performance among the different classification methods. For this, we used the Zero Rule (ZeroR), which predicts the most frequent class in the training dataset. If a model’s performance is worse than ZeroR under the same parameters, it suggests the model is useless. As shown in <a href="ch001.xhtml#informatics-11-00024-t011" class="html-table">Table 11</a> the majority class is neutral with 44%. This implies that a ZeroR classifier that predicts the neutral class consistently for each test data point would achieve an accuracy rate of 44%.
</div>
<div class="html-p">
The initial experiments were conducted using the BoW model to determine the best vocabulary size for the classification models. Subsequently, a second set of experiments was carried out using word embeddings, Doc2Vec models, and phrase modeling instead of the classical BoW model. The goal was to compare different text representations and their usefulness in machine learning algorithms.
</div>
<section id="sec5dot1-informatics-11-00024" type="">
<h3 id="vocabulary-size-evaluation" class="html-italic" data-nested="2">5.1. Vocabulary Size Evaluation</h3>
<div class="html-p">
Initially, We wanted to figure out how many features are suitable for the model and seek insights that could guide the establishment of a simplistic criterion for feature selection. However, since the selection of text features is a broad topic, we do not address it in this paper. An attempt to preserve some of the contexts lost when using BoW model was done by considering the use of <span class="html-italic">n</span>-grams and removing stop words (or just some of them using the frequency of use as a criterion). Thus, we tested different numbers of <span class="html-italic">n</span>-grams and stopwords comparing them using simple logistic regression and computing the accuracy on the test set for the different vocabulary sizes.
</div>
<div class="html-p">
In addition to <span class="html-italic">n</span>-grams and stopwords, we compared the results produced by the TF and Tf-Idf weighting schemes. Then, these findings were combined with the previous experiments, aligning with various text processing approaches such as lemmatization, stemming, and normalization applied to tweets.
</div>
<div class="html-p">
As we see in <a href="ch001.xhtml#informatics-11-00024-f002" class="html-fig">Figure 2</a>a the prediction accuracy is improved when more features are included in the model. Interestingly, removing the ‘stopwords’ is not useful for increasing the accuracy of the model even though these words do not carry semantic information. <a href="ch001.xhtml#informatics-11-00024-f002" class="html-fig">Figure 2</a>b shows that using ‘unigrams’ (i.e., bag of words) works better as features increase than using ‘bigrams’ or ‘trigrams’. This means that the <span class="html-italic">n</span>-grams were unable to capture the desired context, so tokens of just one word seem to do the work better for sentiment classification.
</div>
</section>
<section id="sec5dot2-informatics-11-00024" type="">
<h3 id="dimension-reduction-evaluation" class="html-italic" data-nested="2">5.2. Dimension Reduction Evaluation</h3>
<div class="html-p">
When constructing a BoW model, we encounter a high number of features, necessitating a reduction in feature dimensions prior to their integration into learning models. We explored several feature selection methods for comparative analysis in our experiments.
</div>
<section id="sec5dot2dot1-informatics-11-00024" type="">
<h4 id="χ-2-feature-selection" data-nested="3">5.2.1. <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> Feature Selection</h4>
<div class="html-p">
The chi-squared (<math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math>) statistic measures the degree of dependence between a feature (here, a term within a tweet) and a class (the sentiment of the tweet, whether positive or negative). Through a contingency table, which shows frequency distribution, we see the relationship between a term within a tweet and the class that the tweet belongs to.
</div>
<div class="html-p">
Initially, we assessed this method using the BoW model. This involved transforming the training data into TF vectors, followed by the calculation of the <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> statistics between each feature and class. This score helps to select the number of features with the highest values relative to the classes. Subsequently, we used the <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> statistic to determine which features were useful and then presented our findings graphically to show which word features were important for prediction. For better visualization, only the top 20 features are shown in <a href="ch001.xhtml#informatics-11-00024-f003" class="html-fig">Figure 3</a>. We then decreased the dimensions to different amounts of features and assessed the precision based on the test set.
</div>
<div class="html-p">
<a href="ch001.xhtml#informatics-11-00024-f003" class="html-fig">Figure 3</a>a shows the 20 most significant words identified, some of which are surprisingly considered ‘stopwords’. The plot in <a href="ch001.xhtml#informatics-11-00024-f003" class="html-fig">Figure 3</a>b shows that enhanced accuracy was achieved by selecting around 8000 features based on the <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> criterion rather than the unconstrained BoW model. Despite the slight increase in accuracy, the main objective of dimension reduction has not been completely achieved. Fewer features do not necessarily obtain better results. But we can see that using 3000 <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> selected features (0.56 of accuracy) yields better results than employing the most frequent 9000 features (0.55 of accuracy).
</div>
</section>
<section id="sec5dot2dot2-informatics-11-00024" type="">
<h4 id="truncated-singular-value-decomposition" data-nested="3">5.2.2. Truncated Singular Value Decomposition</h4>
<div class="html-p">
Another method to reduce the dimension is Singular Value Decomposition (SVD). Contrary to the Principal Component Analysis (PCA) method, this estimator does not center the data before computing the singular value decomposition. In SVD the term-document matrix is decomposed into three matrices—U, <math display="inline"><semantics> <mi>σ</mi> </semantics></math> (sigma), and V—and retaining the top-k singular values and their corresponding columns from U and V [<a href="ch001.xhtml#B76-informatics-11-00024" class="html-bibr">76</a>]. These retained singular vectors effectively represent the most important features in the text data. Subsequently, these vectors can be utilized as a reduced feature set of <span class="html-italic">n</span> components.
</div>
<div class="html-p">
In <a href="ch001.xhtml#informatics-11-00024-f004" class="html-fig">Figure 4</a>, we observe that with a minimum of 1000 components, more than 90% of the variance is already taken into account, which is a considerable reduction. To determine whether these new features yield good predictions, we tested the accuracy as we have done in previous experiments. <a href="ch001.xhtml#informatics-11-00024-t012" class="html-table">Table 12</a> shows that the best results are obtained when using around 2000 components.
</div>
</section>
</section>
<section id="sec5dot3-informatics-11-00024" type="">
<h3 id="document-embeddings-evaluation" class="html-italic" data-nested="2">5.3. Document Embeddings Evaluation</h3>
<div class="html-p">
To test this vector representation model, we obtained the ‘embeddings’ using pre-trained vectors, specifically utilizing the Word2Vec embeddings from Spanish Billion Word Corpora (<a href="https://crscardellino.github.io/SBWCE/" class="uri">https://crscardellino.github.io/SBWCE/</a> (accessed on 20 February 2024)). Employing these 300-size vectors we were able to represent a tweet as a vector in a more precise way through the Doc2Vec models.
</div>
<div class="html-p">
Next, regarding the identification of phrases, we initially constructed the ‘1-gram’, ‘2-gram’, and ‘3-gram’ representations of the tokens from all documents. With the Gensim library, we implemented Doc2Vec to learn the paragraph and document embeddings via distributed memory (DM) and distributed Bag of Words (DBOW). Specifically, for DM we used the Distributed Memory Concatenation (DMC) and Distributed Memory Mean (DMM) alternative training algorithms for document vector generation. DMC enhances the DM model by concatenating the document vector with the average of context word vectors, aiming to capture both overall document semantics and specific word context. On the other hand, DMM simplifies this process by directly averaging the context word vectors without concatenation. Once this was done, we compared them evaluating the accuracy on the test set. These methods were tested both separately and in combination.
</div>
<div class="html-p">
In <a href="ch001.xhtml#informatics-11-00024-t013" class="html-table">Table 13</a> we can see that the best results are given when combining the DBOW and DMM models. Although these results are not better than what we have obtained so far, it is remarkable that the representation using ‘2-gram’ and ‘3-gram’ was increasingly effective. This can potentially be a direction to explore further.
</div>
</section>
<section id="sec5dot4-informatics-11-00024" type="">
<h3 id="hyperparameter-tuning" class="html-italic" data-nested="2">5.4. Hyperparameter Tuning</h3>
<div class="html-p">
Having obtained representations of the corpus suitable for learning models, we shifted our focus to model selection and hyperparameter optimization to get the best performance and accuracy in our prediction. Our decisions were driven by optimizing accuracy. Therefore, we performed a 10-fold grid search cross-validation method using a repeated stratified k-fold to identify the optimal values within the range we examined for each classification algorithm. For logistic regression, we explored a range of regularization strength (C) that spans from <math display="inline"><semantics> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>5</mn> </mrow> </msup> </semantics></math> to <math display="inline"><semantics> <msup> <mn>10</mn> <mn>2</mn> </msup> </semantics></math> (or from 0.00001 to 100) on a logarithmic scale. For the solvers, we consider {‘newton-cg’, ‘lbfgs’, ‘liblinear’}, and for specifying the penalty we experimented with the values {‘none’, ‘l1’, ‘l2’, ‘elasticnet’}. The Naïve Bayes model was tested with the smoothing parameter alpha varying twenty values from 0 to 1, with both true and false fit_prior. For the implementation of SVM, we used the same set of regularization strength values as those used in logistic regression. Furthermore, we conducted trials using {‘poly’, ‘rbf’, ‘sigmoid’ } kernels with their default coefficients. Lastly, when creating our neural network architecture, we explored 1 to 5 hidden layer sizes. We also tested different activation functions including {‘relu’, ‘tanh’, ‘logistic’}, and considered multiple solvers like {‘lbfgs’, ‘sgd’, ‘adam’}. <a href="ch001.xhtml#informatics-11-00024-t014" class="html-table">Table 14</a> summarizes the values tested for the different models.
</div>
<div class="html-p">
In <a href="ch001.xhtml#informatics-11-00024-t014" class="html-table">Table 14</a>, the chosen parameters for each supervised learning algorithm are presented as the optimal ones, utilizing the BoW unconstrained model as the feature set. Regrettably, there is little variation in performance when employing various penalty values or types, showing only slight enhancements. This observation also holds for the alpha smoothing parameter within the Naive Bayes model.
</div>
</section>
<section id="sec5dot5-informatics-11-00024" type="">
<h3 id="sentiment-analysis-final-models" class="html-italic" data-nested="2">5.5. Sentiment Analysis Final Models</h3>
<div class="html-p">
Finally, we fitted the supervised learning algorithms using the hyperparameters found with the grid search using as features the unconstrained and reduced BoW representations, as well as the Doc2Vec with DBOW+DMM using ‘3-gram’. <a href="ch001.xhtml#informatics-11-00024-t015" class="html-table">Table 15</a> reports the results of the obtained classification models in terms of the accuracy, precision, recall and the micro F1-score using 10-fold cross-validation. Additionally, the results of fitting an additional KNN model with k = 5 (which does not estimate coefficients) were only included as a reference point for the performance of the models we analyzed. This non-parametric model was included for empirical analysis purposes. Although we cannot affirm that the parametric models are more effective, initial observations appear to indicate that they are more successful in this task.
</div>
<div class="html-p">
For the black-box libraries, there is no need for matrix transformations or data splitting, the evaluation is done over the complete set because there is no need to train an algorithm. We only need to pre-process the corpus. This is especially important for libraries such as Vader, which needs to match the largest number of words possible to obtain better predictions. The same applies to pysentimiento since it is not recommended to use lemmatized words (in fact, low processing is more recommended for this library). Vader has a low computational cost, while pysentimiento requires a bit of time to compute the results, but the advantage is that no prior knowledge is needed. <a href="ch001.xhtml#informatics-11-00024-t016" class="html-table">Table 16</a> shows the results of sentiment analysis using the black-box libraries. We can see substantial difference in all performance metrics among TextBlob, VADER and Pysentimiento. Given that TextBlob relies on a pattern-based and lexicon-driven approach and VADER, which is tailored for social media texts, utilizes a rule-based system with a sentiment lexicon to capture both polarity and intensity; both fail to capture the nuances of the Spanish language. PySentimiento is specifically designed for Spanish texts. It employs machine learning models trained on large corpora to classify sentiment and detect emotions, offering a more nuanced and context-aware approach compared to the rule-based methods of TextBlob and VADER, particularly in the Spanish language domain.
</div>
<div class="html-p">
<a href="ch001.xhtml#informatics-11-00024-t017" class="html-table">Table 17</a> shows the evaluation of the Spanish BERT models, BETO, BerTin-base, and roBERTa. All of them had been pre-trained in the Spanish language, with the latter specifically for social networks (robertuito). We employed their pre-trained weights to tokenize the text using the corresponding tokenizers for each model. Additionally, we used the PyTorch library with a batch data size of 16 and tested with the unconstrained BoW model since pre-trained models have shown to perform better with all the words within the tweet present. We have defined a loss function based on Categorical Cross-Entropy which measures the discrepancy between the probability distribution predicted and the real probability distribution of the labels, in this way we can calculate the loss in each iteration of the loop, then the calculated loss is used to perform gradient backpropagation to adjust the model parameters during training. A dropout was also defined to regularize the model and prevent overfitting and prevent neurons from becoming too dependent on each other during training, in this way, a dropout layer is created that specifies the probability that a neuron is deactivated during training. the training. In this case, it is set to 0.3, which means that each neuron has a 30% probability of being deactivated during each training step.
</div>
<div class="html-p">
Finetunning on the pre-trained models yielded the best results. However, the more epochs we use for training, the more overfitting we observe, as it can be seen in <a href="ch001.xhtml#informatics-11-00024-t018" class="html-table">Table 18</a>, where we show the accuracy on the train and test sets when training on 3, 5 and 10 epochs. This increase in variance is not as easy to interpret as the training error from previous models, where we can take a better look at error rates.
</div>
<div class="html-p">
Finally, <a href="ch001.xhtml#informatics-11-00024-t019" class="html-table">Table 19</a> compares the best accuracy scores of various sentiment models evaluated in our study. In particular, the table shows that BERT-based models perform better than ruled-based or machine learning models. These levels of accuracy are even achieved by the ready-to-use Pysentimiento library. Among these, BETO-uncased is the most accurate model, with an accuracy score of 73.26%. This performance of the BERT-base models may be due to the fact that the model achieves a deep understanding of the context and nuances of the Mexican Spanish discourse, acquired through an extensive pre-training on a variety of text corpora. This allows them to capture the expressions of sentiment and complex linguistic structures of COVID-19 related tweets. In contrast, rule-based models such as Vader and TextBlob exhibit the lowest accuracy scores in our analysis, reflecting their limitation to adapting to the specific linguistic structure and domain.
</div>
</section>
</section>
<section id="sec6-informatics-11-00024" type="conclusions">
<h2 id="conclusions" data-nested="1">6. Conclusions</h2>
<div class="html-p">
This paper presents SENT-COVID, a Twitter corpus of COVID-19 in Mexican Spanish manually annotated with polarity. We have designed several classification experiments with this resource using ready-to-use libraries, classical machine learning methods, and deep learning approaches based on transformers.
</div>
<div class="html-p">
In light of the temporal context surrounding the compilation and presentation of our corpus, it is crucial to emphasize the importance of its value in hindsight. While we acknowledge that the corpus’s arrival may seem overdue, We firmly assert that it remains relevant to our understanding of linguistic patterns and public discourse. As a historical archive of Mexican Spanish tweets during the pandemic, our corpus offers unique insights into the evolution of societal responses, linguistic shifts, and sentiment fluctuations over time. Despite the availability of other resources, the retrospective nature of our corpus provides researchers with an invaluable opportunity to conduct comparative analyses, trace the trajectory of linguistic trends, and evaluate the enduring impact of COVID-19 discourse on societal norms and behaviors. Furthermore, we emphasize the corpus’s potential to complement existing datasets and tools, enriching interdisciplinary research endeavors in fields such as linguistics, public health communication, and computational social science.
</div>
<div class="html-p">
Given the experiments, we observe that, among the black-box libraries, neither TextBlob nor Vader demonstrated satisfactory performance, probably due to the difficulty of obtaining a suitable lexicon in Spanish. In contrast, Pysentimiento exhibits better performance because it employs machine learning models trained on large Spanish corpora to classify text into sentiment categories such as positive, negative, or neutral, and to detect emotions such as joy, anger, sadness, and fear with higher accuracy and contextual understanding. By leveraging machine learning techniques, PySentimiento can capture the nuances of sentiment expressed in Spanish text more effectively, overcoming the limitations faced by lexicon-based approaches like TextBlob and Vader.
</div>
<div class="html-p">
The supervised models have revealed that contrary to our initial expectations, removing common words is not as effective as we had thought. However, the models showed that including a broader range of features and observations improved performance without requiring too much computing power. The dimension reduction models managed to improve the prediction results with fewer features, so we can conclude that it is a viable alternative to tackle this problem. However, there is still much to explore. Furthermore, the penalty parameter selection did not make a major difference as expected, neither Ridge nor Lasso regularization, and it performed almost the same as with the default parameters.
</div>
<div class="html-p">
The results of the Doc2Vec models did not meet the expectations, as they could not outperform basic BoW models. Additionally, training these models is associated with a higher computational cost.
</div>
<div class="html-p">
Finally, pre-trained BERT models yielded the best results. However, they are the most expensive in terms of computational cost. Additionally, it is difficult to perform different tests since cross-validation is difficult. Therefore, the parameters and configuration settings must be chosen based on another criterion. Despite these challenges, for datasets that are not too large, pre-trained BERT models are the most suitable choice.
</div>
</section>
</div>
<div class="html-back">
<section class="html-notes">
<h2 id="author-contributions">Author Contributions</h2>
<div class="html-p">
Conceptualization, H.G.-A., G.B.-E. and G.S.; methodology, H.G.-A., G.B.-E. and G.S.; software, H.G.-A. and J.-C.B.; validation, G.B.-E., G.S. and W.Á.; formal analysis, H.G.-A. and J.-C.B.; investigation, H.G.-A. and J.-C.B.; resources, H.G.-A., G.B.-E. and G.S.; data curation, H.G.-A., J.-C.B. and W.Á.; writing—original draft preparation, H.G.-A. and J.-C.B.; writing—review and editing, G.B.-E., G.S. and W.Á.; visualization, J.-C.B. and W.Á.; supervision, H.G.-A.; project administration, H.G.-A.; funding acquisition, H.G.-A., G.B.-E. and G.S. All authors have read and agreed to the published version of the manuscript.
</div>
</section>
<section class="html-notes">
<h2 id="funding">Funding</h2>
<div class="html-p">
This research was funded by CONAHCYT project number CF-2023-G-64, and by PAPIIT projects TA101722 and IN104424.
</div>
</section>
<section class="html-notes">
<h2 id="institutional-review-board-statement">Institutional Review Board Statement</h2>
<div class="html-p">
Not applicable.
</div>
</section>
<section class="html-notes">
<h2 id="informed-consent-statement">Informed Consent Statement</h2>
<div class="html-p">
Not applicable.
</div>
</section>
<section class="html-notes">
<h2 id="data-availability-statement">Data Availability Statement</h2>
<div class="html-p">
SENT-COVID corpus can be found here: <a href="https://github.com/GIL-UNAM/SENT-COVID" class="uri">https://github.com/GIL-UNAM/SENT-COVID</a> (accessed on 20 February 2024). The dataset is licensed under CC0, so it is open data. If the data are used, we would appreciate citing this article as the corpus descriptor.
</div>
</section>
<section id="html-ack" class="html-ack">
<h2 id="acknowledgments">Acknowledgments</h2>
<div class="html-p">
Authors thank CONAHCYT for the computing resources provided through the Deep Learning Platform for Language Technologies of the INAOE Supercomputing Laboratory, as well as Gabriel Castillo for the computing services.
</div>
</section>
<section class="html-notes">
<h2 id="conflicts-of-interest">Conflicts of Interest</h2>
<div class="html-p">
The authors declare no conflict of interest.
</div>
</section>
<section id="html-glossary">
<h2 id="abbreviations">Abbreviations</h2>
<div class="html-p">
The following abbreviations are used in this manuscript:
</div>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">NLP</td>
<td style="text-align: left;">Natural Language Processing</td>
</tr>
<tr class="even">
<td style="text-align: left;">ML</td>
<td style="text-align: left;">Machine Learning</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DL</td>
<td style="text-align: left;">Deep Learning</td>
</tr>
<tr class="even">
<td style="text-align: left;">LSTM</td>
<td style="text-align: left;">Long Short Term Memory</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">Bidirectional Encoder Representations from Transformers</td>
</tr>
<tr class="even">
<td style="text-align: left;">IAA</td>
<td style="text-align: left;">Inter-Annotator Agreement</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BoW</td>
<td style="text-align: left;">Bag of Words</td>
</tr>
<tr class="even">
<td style="text-align: left;">Tf-Idf</td>
<td style="text-align: left;">Term Frequency-Inverse Document Frequency</td>
</tr>
<tr class="odd">
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">Multilayer Perceptron</td>
</tr>
<tr class="even">
<td style="text-align: left;">CBOW</td>
<td style="text-align: left;">Continuous Bag of Words</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DBOW</td>
<td style="text-align: left;">Distributed Bag of Words</td>
</tr>
<tr class="even">
<td style="text-align: left;">DM</td>
<td style="text-align: left;">Distributed Memory</td>
</tr>
<tr class="odd">
<td style="text-align: left;">NB</td>
<td style="text-align: left;">Naïve Bayes</td>
</tr>
<tr class="even">
<td style="text-align: left;">SVM</td>
<td style="text-align: left;">Support Vector Machines</td>
</tr>
</tbody>
</table>
</section>
<section id="html-references_list">
<h2 id="references">References</h2>
<ol>
<li><div id="B1-informatics-11-00024">

</div>
Shivaprasad, T.; Shetty, J. Sentiment analysis of product reviews: A review. In Proceedings of the 2017 International Conference on Inventive Communication and Computational Technologies (ICICCT), Coimbatore, India, 10–11 March 2017; pp. 298–301. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+analysis+of+product+reviews:+A+review&amp;conference=Proceedings+of+the+2017+International+Conference+on+Inventive+Communication+and+Computational+Technologies+(ICICCT)&amp;author=Shivaprasad,+T.&amp;author=Shetty,+J.&amp;publication_year=2017&amp;pages=298%E2%80%93301" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B2-informatics-11-00024">

</div>
Das, A.; Gunturi, K.S.; Chandrasekhar, A.; Padhi, A.; Liu, Q. Automated pipeline for sentiment analysis of political tweets. In Proceedings of the 2021 International Conference on Data Mining Workshops (ICDMW), Auckland, New Zealand, 7–10 December 2021; pp. 128–135. [<a href="https://scholar.google.com/scholar_lookup?title=Automated+pipeline+for+sentiment+analysis+of+political+tweets&amp;conference=Proceedings+of+the+2021+International+Conference+on+Data+Mining+Workshops+(ICDMW)&amp;author=Das,+A.&amp;author=Gunturi,+K.S.&amp;author=Chandrasekhar,+A.&amp;author=Padhi,+A.&amp;author=Liu,+Q.&amp;publication_year=2021&amp;pages=128%E2%80%93135" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B3-informatics-11-00024">

</div>
Man, X.; Luo, T.; Lin, J. Financial sentiment analysis (fsa): A survey. In Proceedings of the 2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS), Taipei, Taiwan, 6–9 May 2019; pp. 617–622. [<a href="https://scholar.google.com/scholar_lookup?title=Financial+sentiment+analysis+(fsa):+A+survey&amp;conference=Proceedings+of+the+2019+IEEE+International+Conference+on+Industrial+Cyber+Physical+Systems+(ICPS)&amp;author=Man,+X.&amp;author=Luo,+T.&amp;author=Lin,+J.&amp;publication_year=2019&amp;pages=617%E2%80%93622" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B4-informatics-11-00024">

</div>
Shelar, A.; Huang, C.Y. Sentiment Analysis of Twitter Data. In Proceedings of the 2018 International Conference on Computational Science and Computational Intelligence (CSCI), Las Vegas, NV, USA, 12–14 December 2018; pp. 1301–1302. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+Analysis+of+Twitter+Data&amp;conference=Proceedings+of+the+2018+International+Conference+on+Computational+Science+and+Computational+Intelligence+(CSCI)&amp;author=Shelar,+A.&amp;author=Huang,+C.Y.&amp;publication_year=2018&amp;pages=1301%E2%80%931302&amp;doi=10.1109/CSCI46756.2018.00252" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/CSCI46756.2018.00252" class="cross-ref">CrossRef</a>]</li>
<li><div id="B5-informatics-11-00024">

</div>
Zahoor, S.; Rohilla, R. Twitter Sentiment Analysis Using Lexical or Rule Based Approach: A Case Study. In Proceedings of the 2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO), Noida, India, 4–5 June 2020; pp. 537–542. [<a href="https://scholar.google.com/scholar_lookup?title=Twitter+Sentiment+Analysis+Using+Lexical+or+Rule+Based+Approach:+A+Case+Study&amp;conference=Proceedings+of+the+2020+8th+International+Conference+on+Reliability,+Infocom+Technologies+and+Optimization+(Trends+and+Future+Directions)+(ICRITO)&amp;author=Zahoor,+S.&amp;author=Rohilla,+R.&amp;publication_year=2020&amp;pages=537%E2%80%93542&amp;doi=10.1109/ICRITO48877.2020.9197910" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ICRITO48877.2020.9197910" class="cross-ref">CrossRef</a>]</li>
<li><div id="B6-informatics-11-00024">

</div>
Nair, A.J.; G, V.; Vinayak, A. Comparative study of Twitter Sentiment On COVID-19 Tweets. In Proceedings of the 2021 5th International Conference on Computing Methodologies and Communication (ICCMC), Erode, India, 8–10 April 2021; pp. 1773–1778. [<a href="https://scholar.google.com/scholar_lookup?title=Comparative+study+of+Twitter+Sentiment+On+COVID-19+Tweets&amp;conference=Proceedings+of+the+2021+5th+International+Conference+on+Computing+Methodologies+and+Communication+(ICCMC)&amp;author=Nair,+A.J.&amp;author=G,+V.&amp;author=Vinayak,+A.&amp;publication_year=2021&amp;pages=1773%E2%80%931778&amp;doi=10.1109/ICCMC51019.2021.9418320" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ICCMC51019.2021.9418320" class="cross-ref">CrossRef</a>]</li>
<li><div id="B7-informatics-11-00024">

</div>
Diyasa, I.G.S.M.; Mandenni, N.M.I.M.; Fachrurrozi, M.I.; Pradika, S.I.; Manab, K.R.N.; Sasmita, N.R. Twitter Sentiment Analysis as an Evaluation and Service Base On Python Textblob. <span class="html-italic">IOP Conf. Ser. Mater. Sci. Eng.</span> <strong>2021</strong>, <span class="html-italic">1125</span>, 012034. [<a href="https://scholar.google.com/scholar_lookup?title=Twitter+Sentiment+Analysis+as+an+Evaluation+and+Service+Base+On+Python+Textblob&amp;author=Diyasa,+I.G.S.M.&amp;author=Mandenni,+N.M.I.M.&amp;author=Fachrurrozi,+M.I.&amp;author=Pradika,+S.I.&amp;author=Manab,+K.R.N.&amp;author=Sasmita,+N.R.&amp;publication_year=2021&amp;journal=IOP+Conf.+Ser.+Mater.+Sci.+Eng.&amp;volume=1125&amp;pages=012034&amp;doi=10.1088/1757-899X/1125/1/012034" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1088/1757-899X/1125/1/012034" class="cross-ref">CrossRef</a>]</li>
<li><div id="B8-informatics-11-00024">

</div>
Aljedaani, W.; Rustam, F.; Mkaouer, M.W.; Ghallab, A.; Rupapara, V.; Washington, P.B.; Lee, E.; Ashraf, I. Sentiment analysis on Twitter data integrating TextBlob and deep learning models: The case of US airline industry. <span class="html-italic">Knowl.-Based Syst.</span> <strong>2022</strong>, <span class="html-italic">255</span>, 109780. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+analysis+on+Twitter+data+integrating+TextBlob+and+deep+learning+models:+The+case+of+US+airline+industry&amp;author=Aljedaani,+W.&amp;author=Rustam,+F.&amp;author=Mkaouer,+M.W.&amp;author=Ghallab,+A.&amp;author=Rupapara,+V.&amp;author=Washington,+P.B.&amp;author=Lee,+E.&amp;author=Ashraf,+I.&amp;publication_year=2022&amp;journal=Knowl.-Based+Syst.&amp;volume=255&amp;pages=109780&amp;doi=10.1016/j.knosys.2022.109780" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.knosys.2022.109780" class="cross-ref">CrossRef</a>]</li>
<li><div id="B9-informatics-11-00024">

</div>
Pradhan, R. Extracting Sentiments from YouTube Comments. In Proceedings of the 2021 Sixth International Conference on Image Information Processing (ICIIP), Shimla, India, 26–28 November 2021; Volume 6, pp. 1–4. [<a href="https://scholar.google.com/scholar_lookup?title=Extracting+Sentiments+from+YouTube+Comments&amp;conference=Proceedings+of+the+2021+Sixth+International+Conference+on+Image+Information+Processing+(ICIIP)&amp;author=Pradhan,+R.&amp;publication_year=2021&amp;pages=1%E2%80%934&amp;doi=10.1109/ICIIP53038.2021.9702561" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ICIIP53038.2021.9702561" class="cross-ref">CrossRef</a>]</li>
<li><div id="B10-informatics-11-00024">

</div>
Sahu, S.; Kumar, R.; MohdShafi, P.; Shafi, J.; Kim, S.; Ijaz, M.F. A Hybrid Recommendation System of Upcoming Movies Using Sentiment Analysis of YouTube Trailer Reviews. <span class="html-italic">Mathematics</span> <strong>2022</strong>, <span class="html-italic">10</span>, 1568. [<a href="https://scholar.google.com/scholar_lookup?title=A+Hybrid+Recommendation+System+of+Upcoming+Movies+Using+Sentiment+Analysis+of+YouTube+Trailer+Reviews&amp;author=Sahu,+S.&amp;author=Kumar,+R.&amp;author=MohdShafi,+P.&amp;author=Shafi,+J.&amp;author=Kim,+S.&amp;author=Ijaz,+M.F.&amp;publication_year=2022&amp;journal=Mathematics&amp;volume=10&amp;pages=1568&amp;doi=10.3390/math10091568" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/math10091568" class="cross-ref">CrossRef</a>]</li>
<li><div id="B11-informatics-11-00024">

</div>
Alawadh, H.M.; Alabrah, A.; Meraj, T.; Rauf, H.T. English Language Learning via YouTube: An NLP-Based Analysis of Users’ Comments. <span class="html-italic">Computers</span> <strong>2023</strong>, <span class="html-italic">12</span>, 24. [<a href="https://scholar.google.com/scholar_lookup?title=English+Language+Learning+via+YouTube:+An+NLP-Based+Analysis+of+Users%E2%80%99+Comments&amp;author=Alawadh,+H.M.&amp;author=Alabrah,+A.&amp;author=Meraj,+T.&amp;author=Rauf,+H.T.&amp;publication_year=2023&amp;journal=Computers&amp;volume=12&amp;pages=24&amp;doi=10.3390/computers12020024" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/computers12020024" class="cross-ref">CrossRef</a>]</li>
<li><div id="B12-informatics-11-00024">

</div>
Anastasiou, P.; Tzafilkou, K.; Karapiperis, D.; Tjortjis, C. YouTube Sentiment Analysis on Healthcare Product Campaigns: Combining Lexicons and Machine Learning Models. In Proceedings of the 2023 14th International Conference on Information, Intelligence, Systems &amp; Applications (IISA), Volos, Greece, 10–12 July 2023; pp. 1–8. [<a href="https://scholar.google.com/scholar_lookup?title=YouTube+Sentiment+Analysis+on+Healthcare+Product+Campaigns:+Combining+Lexicons+and+Machine+Learning+Models&amp;conference=Proceedings+of+the+2023+14th+International+Conference+on+Information,+Intelligence,+Systems+&amp;+Applications+(IISA)&amp;author=Anastasiou,+P.&amp;author=Tzafilkou,+K.&amp;author=Karapiperis,+D.&amp;author=Tjortjis,+C.&amp;publication_year=2023&amp;pages=1%E2%80%938&amp;doi=10.1109/IISA59645.2023.10345900" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/IISA59645.2023.10345900" class="cross-ref">CrossRef</a>]</li>
<li><div id="B13-informatics-11-00024">

</div>
Gupta, S.; Kirthica, S. Sentiment Analysis of Youtube Comment Section in Indian News Channels. In Proceedings of the ICT for Intelligent Systems, Ahmedabad, India, 27–28 April 2023; Springer Nature: Singapore, 2023; pp. 191–200. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+Analysis+of+Youtube+Comment+Section+in+Indian+News+Channels&amp;conference=Proceedings+of+the+ICT+for+Intelligent+Systems&amp;author=Gupta,+S.&amp;author=Kirthica,+S.&amp;publication_year=2023&amp;pages=191%E2%80%93200" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B14-informatics-11-00024">

</div>
Melton, C.A.; Olusanya, O.A.; Ammar, N.; Shaban-Nejad, A. Public sentiment analysis and topic modeling regarding COVID-19 vaccines on the Reddit social media platform: A call to action for strengthening vaccine confidence. <span class="html-italic">J. Infect. Public Health</span> <strong>2021</strong>, <span class="html-italic">14</span>, 1505–1512. [<a href="https://scholar.google.com/scholar_lookup?title=Public+sentiment+analysis+and+topic+modeling+regarding+COVID-19+vaccines+on+the+Reddit+social+media+platform:+A+call+to+action+for+strengthening+vaccine+confidence&amp;author=Melton,+C.A.&amp;author=Olusanya,+O.A.&amp;author=Ammar,+N.&amp;author=Shaban-Nejad,+A.&amp;publication_year=2021&amp;journal=J.+Infect.+Public+Health&amp;volume=14&amp;pages=1505%E2%80%931512&amp;doi=10.1016/j.jiph.2021.08.010" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.jiph.2021.08.010" class="cross-ref">CrossRef</a>]</li>
<li><div id="B15-informatics-11-00024">

</div>
Botzer, N.; Gu, S.; Weninger, T. Analysis of Moral Judgment on Reddit. <span class="html-italic">IEEE Trans. Comput. Soc. Syst.</span> <strong>2023</strong>, <span class="html-italic">10</span>, 947–957. [<a href="https://scholar.google.com/scholar_lookup?title=Analysis+of+Moral+Judgment+on+Reddit&amp;author=Botzer,+N.&amp;author=Gu,+S.&amp;author=Weninger,+T.&amp;publication_year=2023&amp;journal=IEEE+Trans.+Comput.+Soc.+Syst.&amp;volume=10&amp;pages=947%E2%80%93957&amp;doi=10.1109/TCSS.2022.3160677" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/TCSS.2022.3160677" class="cross-ref">CrossRef</a>]</li>
<li><div id="B16-informatics-11-00024">

</div>
Ruan, T.; Lv, Q. Public perception of electric vehicles on Reddit and Twitter: A cross-platform analysis. <span class="html-italic">Transp. Res. Interdiscip. Perspect.</span> <strong>2023</strong>, <span class="html-italic">21</span>, 100872. [<a href="https://scholar.google.com/scholar_lookup?title=Public+perception+of+electric+vehicles+on+Reddit+and+Twitter:+A+cross-platform+analysis&amp;author=Ruan,+T.&amp;author=Lv,+Q.&amp;publication_year=2023&amp;journal=Transp.+Res.+Interdiscip.+Perspect.&amp;volume=21&amp;pages=100872&amp;doi=10.1016/j.trip.2023.100872" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.trip.2023.100872" class="cross-ref">CrossRef</a>]</li>
<li><div id="B17-informatics-11-00024">

</div>
Sekar, V.R.; Kannan, T.K.R.; N, S.; Vijay, P. Hybrid Perception Analysis of World Leaders in Reddit using Sentiment Analysis. In Proceedings of the 2023 International Conference on Advances in Intelligent Computing and Applications (AICAPS), Kochi, India, 1–3 February 2023; pp. 1–5. [<a href="https://scholar.google.com/scholar_lookup?title=Hybrid+Perception+Analysis+of+World+Leaders+in+Reddit+using+Sentiment+Analysis&amp;conference=Proceedings+of+the+2023+International+Conference+on+Advances+in+Intelligent+Computing+and+Applications+(AICAPS)&amp;author=Sekar,+V.R.&amp;author=Kannan,+T.K.R.&amp;author=N,+S.&amp;author=Vijay,+P.&amp;publication_year=2023&amp;pages=1%E2%80%935&amp;doi=10.1109/AICAPS57044.2023.10074005" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/AICAPS57044.2023.10074005" class="cross-ref">CrossRef</a>]</li>
<li><div id="B18-informatics-11-00024">

</div>
Ligthart, A.; Catal, C.; Tekinerdogan, B. Systematic reviews in sentiment analysis: A tertiary study. <span class="html-italic">Artif. Intell. Rev.</span> <strong>2021</strong>, <span class="html-italic">54</span>, 4997–5053. [<a href="https://scholar.google.com/scholar_lookup?title=Systematic+reviews+in+sentiment+analysis:+A+tertiary+study&amp;author=Ligthart,+A.&amp;author=Catal,+C.&amp;author=Tekinerdogan,+B.&amp;publication_year=2021&amp;journal=Artif.+Intell.+Rev.&amp;volume=54&amp;pages=4997%E2%80%935053&amp;doi=10.1007/s10462-021-09973-3" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1007/s10462-021-09973-3" class="cross-ref">CrossRef</a>]</li>
<li><div id="B19-informatics-11-00024">

</div>
Shayaa, S.; Jaafar, N.I.; Bahri, S.; Sulaiman, A.; Seuk Wai, P.; Wai Chung, Y.; Piprani, A.Z.; Al-Garadi, M.A. Sentiment Analysis of Big Data: Methods, Applications, and Open Challenges. <span class="html-italic">IEEE Access</span> <strong>2018</strong>, <span class="html-italic">6</span>, 37807–37827. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+Analysis+of+Big+Data:+Methods,+Applications,+and+Open+Challenges&amp;author=Shayaa,+S.&amp;author=Jaafar,+N.I.&amp;author=Bahri,+S.&amp;author=Sulaiman,+A.&amp;author=Seuk+Wai,+P.&amp;author=Wai+Chung,+Y.&amp;author=Piprani,+A.Z.&amp;author=Al-Garadi,+M.A.&amp;publication_year=2018&amp;journal=IEEE+Access&amp;volume=6&amp;pages=37807%E2%80%9337827&amp;doi=10.1109/ACCESS.2018.2851311" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ACCESS.2018.2851311" class="cross-ref">CrossRef</a>]</li>
<li><div id="B20-informatics-11-00024">

</div>
Nia, Z.M.; Bragazzi, N.L.; Ahamadi, A.; Asgary, A.; Mellado, B.; Orbinski, J.; Seyyed-Kalantari, L.; Woldegerima, W.A.; Wu, J.; Kong, J.D. Off-label drug use during the COVID-19 pandemic in Africa: Topic modelling and sentiment analysis of ivermectin in South Africa and Nigeria as a case study. <span class="html-italic">J. R. Soc. Interface</span> <strong>2023</strong>, <span class="html-italic">20</span>, 20230200. [<a href="https://scholar.google.com/scholar_lookup?title=Off-label+drug+use+during+the+COVID-19+pandemic+in+Africa:+Topic+modelling+and+sentiment+analysis+of+ivermectin+in+South+Africa+and+Nigeria+as+a+case+study&amp;author=Nia,+Z.M.&amp;author=Bragazzi,+N.L.&amp;author=Ahamadi,+A.&amp;author=Asgary,+A.&amp;author=Mellado,+B.&amp;author=Orbinski,+J.&amp;author=Seyyed-Kalantari,+L.&amp;author=Woldegerima,+W.A.&amp;author=Wu,+J.&amp;author=Kong,+J.D.&amp;publication_year=2023&amp;journal=J.+R.+Soc.+Interface&amp;volume=20&amp;pages=20230200&amp;doi=10.1098/rsif.2023.0200" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1098/rsif.2023.0200" class="cross-ref">CrossRef</a>]</li>
<li><div id="B21-informatics-11-00024">

</div>
Movahedi Nia, Z.; Bragazzi, N.; Asgary, A.; Orbinski, J.; Wu, J.; Kong, J. Mpox Panic, Infodemic, and Stigmatization of the Two-Spirit, Lesbian, Gay, Bisexual, Transgender, Queer or Questioning, Intersex, Asexual Community: Geospatial Analysis, Topic Modeling, and Sentiment Analysis of a Large, Multilingual Social Media Database. <span class="html-italic">J. Med. Internet Res.</span> <strong>2023</strong>, <span class="html-italic">25</span>, e45108. [<a href="https://scholar.google.com/scholar_lookup?title=Mpox+Panic,+Infodemic,+and+Stigmatization+of+the+Two-Spirit,+Lesbian,+Gay,+Bisexual,+Transgender,+Queer+or+Questioning,+Intersex,+Asexual+Community:+Geospatial+Analysis,+Topic+Modeling,+and+Sentiment+Analysis+of+a+Large,+Multilingual+Social+Media+Database&amp;author=Movahedi+Nia,+Z.&amp;author=Bragazzi,+N.&amp;author=Asgary,+A.&amp;author=Orbinski,+J.&amp;author=Wu,+J.&amp;author=Kong,+J.&amp;publication_year=2023&amp;journal=J.+Med.+Internet+Res.&amp;volume=25&amp;pages=e45108&amp;doi=10.2196/45108&amp;pmid=37126377" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.2196/45108" class="cross-ref">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/pubmed/37126377" class="cross-ref pub_med">PubMed</a>]</li>
<li><div id="B22-informatics-11-00024">

</div>
Kappaun, A.; Oliveira, J. Análise sobre Viés de Gênero no Youtube: Um Estudo sobre as Eleições Presidenciais de 2018 e 2022. In Proceedings of the Anais do XII Brazilian Workshop on Social Network Analysis and Mining, João Pessoa, PB, Brazil, 6–11 August 2023; SBC: Porto Alegre, RS, Brazil, 2023; pp. 127–138. [<a href="https://scholar.google.com/scholar_lookup?title=An%C3%A1lise+sobre+Vi%C3%A9s+de+G%C3%AAnero+no+Youtube:+Um+Estudo+sobre+as+Elei%C3%A7%C3%B5es+Presidenciais+de+2018+e+2022&amp;conference=Proceedings+of+the+Anais+do+XII+Brazilian+Workshop+on+Social+Network+Analysis+and+Mining&amp;author=Kappaun,+A.&amp;author=Oliveira,+J.&amp;publication_year=2023&amp;pages=127%E2%80%93138" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B23-informatics-11-00024">

</div>
Aleksandric, A.; Anderson, H.I.; Melcher, S.; Nilizadeh, S.; Wilson, G.M. Spanish Facebook Posts as an Indicator of COVID-19 Vaccine Hesitancy in Texas. <span class="html-italic">Vaccines</span> <strong>2022</strong>, <span class="html-italic">10</span>, 1713. [<a href="https://scholar.google.com/scholar_lookup?title=Spanish+Facebook+Posts+as+an+Indicator+of+COVID-19+Vaccine+Hesitancy+in+Texas&amp;author=Aleksandric,+A.&amp;author=Anderson,+H.I.&amp;author=Melcher,+S.&amp;author=Nilizadeh,+S.&amp;author=Wilson,+G.M.&amp;publication_year=2022&amp;journal=Vaccines&amp;volume=10&amp;pages=1713&amp;doi=10.3390/vaccines10101713" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/vaccines10101713" class="cross-ref">CrossRef</a>]</li>
<li><div id="B24-informatics-11-00024">

</div>
Balbontín, C.; Contreras, S.; Browne, R. Using Sentiment Analysis in Understanding the Information and Political Pluralism under the Chilean New Constitution Discussion. <span class="html-italic">Soc. Sci.</span> <strong>2023</strong>, <span class="html-italic">12</span>, 140. [<a href="https://scholar.google.com/scholar_lookup?title=Using+Sentiment+Analysis+in+Understanding+the+Information+and+Political+Pluralism+under+the+Chilean+New+Constitution+Discussion&amp;author=Balbont%C3%ADn,+C.&amp;author=Contreras,+S.&amp;author=Browne,+R.&amp;publication_year=2023&amp;journal=Soc.+Sci.&amp;volume=12&amp;pages=140&amp;doi=10.3390/socsci12030140" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/socsci12030140" class="cross-ref">CrossRef</a>]</li>
<li><div id="B25-informatics-11-00024">

</div>
Agustiningsih, K.K.; Utami, E.; Al Fatta, H. Sentiment Analysis of COVID-19 Vaccine on Twitter Social Media: Systematic Literature Review. In Proceedings of the 2021 IEEE 5th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE), Purwokerto, Indonesia, 24–25 November 2021; pp. 121–126. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+Analysis+of+COVID-19+Vaccine+on+Twitter+Social+Media:+Systematic+Literature+Review&amp;conference=Proceedings+of+the+2021+IEEE+5th+International+Conference+on+Information+Technology,+Information+Systems+and+Electrical+Engineering+(ICITISEE)&amp;author=Agustiningsih,+K.K.&amp;author=Utami,+E.&amp;author=Al+Fatta,+H.&amp;publication_year=2021&amp;pages=121%E2%80%93126&amp;doi=10.1109/ICITISEE53823.2021.9655960" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ICITISEE53823.2021.9655960" class="cross-ref">CrossRef</a>]</li>
<li><div id="B26-informatics-11-00024">

</div>
Alamoodi, A.; Zaidan, B.; Zaidan, A.; Albahri, O.; Mohammed, K.; Malik, R.; Almahdi, E.; Chyad, M.; Tareq, Z.; Albahri, A.; et al. Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic review. <span class="html-italic">Expert Syst. Appl.</span> <strong>2021</strong>, <span class="html-italic">167</span>, 114155. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+analysis+and+its+applications+in+fighting+COVID-19+and+infectious+diseases:+A+systematic+review&amp;author=Alamoodi,+A.&amp;author=Zaidan,+B.&amp;author=Zaidan,+A.&amp;author=Albahri,+O.&amp;author=Mohammed,+K.&amp;author=Malik,+R.&amp;author=Almahdi,+E.&amp;author=Chyad,+M.&amp;author=Tareq,+Z.&amp;author=Albahri,+A.&amp;author=et+al.&amp;publication_year=2021&amp;journal=Expert+Syst.+Appl.&amp;volume=167&amp;pages=114155&amp;doi=10.1016/j.eswa.2020.114155" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.eswa.2020.114155" class="cross-ref">CrossRef</a>]</li>
<li><div id="B27-informatics-11-00024">

</div>
Hussain, A.; Tahir, A.; Hussain, Z.; Sheikh, Z.; Gogate, M.; Dashtipour, K.; Ali, A.; Sheikh, A. Artificial Intelligence–Enabled Analysis of Public Attitudes on Facebook and Twitter Toward COVID-19 Vaccines in the United Kingdom and the United States: Observational Study. <span class="html-italic">J. Med. Internet Res.</span> <strong>2021</strong>, <span class="html-italic">23</span>, e26627. [<a href="https://scholar.google.com/scholar_lookup?title=Artificial+Intelligence%E2%80%93Enabled+Analysis+of+Public+Attitudes+on+Facebook+and+Twitter+Toward+COVID-19+Vaccines+in+the+United+Kingdom+and+the+United+States:+Observational+Study&amp;author=Hussain,+A.&amp;author=Tahir,+A.&amp;author=Hussain,+Z.&amp;author=Sheikh,+Z.&amp;author=Gogate,+M.&amp;author=Dashtipour,+K.&amp;author=Ali,+A.&amp;author=Sheikh,+A.&amp;publication_year=2021&amp;journal=J.+Med.+Internet+Res.&amp;volume=23&amp;pages=e26627&amp;doi=10.2196/26627&amp;pmid=33724919" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.2196/26627" class="cross-ref">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/pubmed/33724919" class="cross-ref pub_med">PubMed</a>]</li>
<li><div id="B28-informatics-11-00024">

</div>
Khan, R.; Rustam, F.; Kanwal, K.; Mehmood, A.; Choi, G.S. US Based COVID-19 Tweets Sentiment Analysis Using TextBlob and Supervised Machine Learning Algorithms. In Proceedings of the 2021 International Conference on Artificial Intelligence (ICAI), Islamabad, Pakistan, 5–7 April 2021; pp. 1–8. [<a href="https://scholar.google.com/scholar_lookup?title=US+Based+COVID-19+Tweets+Sentiment+Analysis+Using+TextBlob+and+Supervised+Machine+Learning+Algorithms&amp;conference=Proceedings+of+the+2021+International+Conference+on+Artificial+Intelligence+(ICAI)&amp;author=Khan,+R.&amp;author=Rustam,+F.&amp;author=Kanwal,+K.&amp;author=Mehmood,+A.&amp;author=Choi,+G.S.&amp;publication_year=2021&amp;pages=1%E2%80%938&amp;doi=10.1109/ICAI52203.2021.9445207" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ICAI52203.2021.9445207" class="cross-ref">CrossRef</a>]</li>
<li><div id="B29-informatics-11-00024">

</div>
Mudassir, M.A.; Mor, Y.; Munot, R.; Shankarmani, R. Sentiment Analysis of COVID-19 Vaccine Perception Using NLP. In Proceedings of the 2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA), Coimbatore, India, 2–4 September 2021; pp. 516–521. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+Analysis+of+COVID-19+Vaccine+Perception+Using+NLP&amp;conference=Proceedings+of+the+2021+Third+International+Conference+on+Inventive+Research+in+Computing+Applications+(ICIRCA)&amp;author=Mudassir,+M.A.&amp;author=Mor,+Y.&amp;author=Munot,+R.&amp;author=Shankarmani,+R.&amp;publication_year=2021&amp;pages=516%E2%80%93521&amp;doi=10.1109/ICIRCA51532.2021.9544512" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ICIRCA51532.2021.9544512" class="cross-ref">CrossRef</a>]</li>
<li><div id="B30-informatics-11-00024">

</div>
Rahul, K.; Jindal, B.R.; Singh, K.; Meel, P. Analysing Public Sentiments Regarding COVID-19 Vaccine on Twitter. In Proceedings of the 2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS), Coimbatore, India, 19–20 March 2021; Volume 1, pp. 488–493. [<a href="https://scholar.google.com/scholar_lookup?title=Analysing+Public+Sentiments+Regarding+COVID-19+Vaccine+on+Twitter&amp;conference=Proceedings+of+the+2021+7th+International+Conference+on+Advanced+Computing+and+Communication+Systems+(ICACCS)&amp;author=Rahul,+K.&amp;author=Jindal,+B.R.&amp;author=Singh,+K.&amp;author=Meel,+P.&amp;publication_year=2021&amp;pages=488%E2%80%93493&amp;doi=10.1109/ICACCS51430.2021.9441693" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ICACCS51430.2021.9441693" class="cross-ref">CrossRef</a>]</li>
<li><div id="B31-informatics-11-00024">

</div>
Abiola, O.; Abayomi-Alli, A.; Tale, O.A.; Misra, S.; Abayomi-Alli, O. Sentiment analysis of COVID-19 tweets from selected hashtags in Nigeria using VADER and Text Blob analyser. <span class="html-italic">J. Electr. Syst. Inf. Technol.</span> <strong>2023</strong>, <span class="html-italic">10</span>, 5. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+analysis+of+COVID-19+tweets+from+selected+hashtags+in+Nigeria+using+VADER+and+Text+Blob+analyser&amp;author=Abiola,+O.&amp;author=Abayomi-Alli,+A.&amp;author=Tale,+O.A.&amp;author=Misra,+S.&amp;author=Abayomi-Alli,+O.&amp;publication_year=2023&amp;journal=J.+Electr.+Syst.+Inf.+Technol.&amp;volume=10&amp;pages=5&amp;doi=10.1186/s43067-023-00070-9" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1186/s43067-023-00070-9" class="cross-ref">CrossRef</a>]</li>
<li><div id="B32-informatics-11-00024">

</div>
Jelodar, H.; Wang, Y.; Orji, R.; Huang, H. Deep Sentiment Classification and Topic Discovery on Novel Coronavirus or COVID-19 Online Discussions: NLP Using LSTM Recurrent Neural Network Approach. <span class="html-italic">IEEE J. Biomed. Health Inform.</span> <strong>2020</strong>, <span class="html-italic">24</span>, 2733–2742. [<a href="https://scholar.google.com/scholar_lookup?title=Deep+Sentiment+Classification+and+Topic+Discovery+on+Novel+Coronavirus+or+COVID-19+Online+Discussions:+NLP+Using+LSTM+Recurrent+Neural+Network+Approach&amp;author=Jelodar,+H.&amp;author=Wang,+Y.&amp;author=Orji,+R.&amp;author=Huang,+H.&amp;publication_year=2020&amp;journal=IEEE+J.+Biomed.+Health+Inform.&amp;volume=24&amp;pages=2733%E2%80%932742&amp;doi=10.1109/JBHI.2020.3001216" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/JBHI.2020.3001216" class="cross-ref">CrossRef</a>]</li>
<li><div id="B33-informatics-11-00024">

</div>
Chunduri, R.K.; Perera, D.G. Neuromorphic Sentiment Analysis Using Spiking Neural Networks. <span class="html-italic">Sensors</span> <strong>2023</strong>, <span class="html-italic">23</span>, 7701. [<a href="https://scholar.google.com/scholar_lookup?title=Neuromorphic+Sentiment+Analysis+Using+Spiking+Neural+Networks&amp;author=Chunduri,+R.K.&amp;author=Perera,+D.G.&amp;publication_year=2023&amp;journal=Sensors&amp;volume=23&amp;pages=7701&amp;doi=10.3390/s23187701" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/s23187701" class="cross-ref">CrossRef</a>]</li>
<li><div id="B34-informatics-11-00024">

</div>
Satu, M.S.; Khan, M.I.; Mahmud, M.; Uddin, S.; Summers, M.A.; Quinn, J.M.; Moni, M.A. TClustVID: A novel machine learning classification model to investigate topics and sentiment in COVID-19 tweets. <span class="html-italic">Knowl.-Based Syst.</span> <strong>2021</strong>, <span class="html-italic">226</span>, 107126. [<a href="https://scholar.google.com/scholar_lookup?title=TClustVID:+A+novel+machine+learning+classification+model+to+investigate+topics+and+sentiment+in+COVID-19+tweets&amp;author=Satu,+M.S.&amp;author=Khan,+M.I.&amp;author=Mahmud,+M.&amp;author=Uddin,+S.&amp;author=Summers,+M.A.&amp;author=Quinn,+J.M.&amp;author=Moni,+M.A.&amp;publication_year=2021&amp;journal=Knowl.-Based+Syst.&amp;volume=226&amp;pages=107126&amp;doi=10.1016/j.knosys.2021.107126" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.knosys.2021.107126" class="cross-ref">CrossRef</a>]</li>
<li><div id="B35-informatics-11-00024">

</div>
Contreras Hernández, S.; Tzili Cruz, M.P.; Espínola Sánchez, J.M.; Pérez Tzili, A. Deep Learning Model for COVID-19 Sentiment Analysis on Twitter. <span class="html-italic">New Gener. Comput.</span> <strong>2023</strong>, <span class="html-italic">41</span>, 189–212. [<a href="https://scholar.google.com/scholar_lookup?title=Deep+Learning+Model+for+COVID-19+Sentiment+Analysis+on+Twitter&amp;author=Contreras+Hern%C3%A1ndez,+S.&amp;author=Tzili+Cruz,+M.P.&amp;author=Esp%C3%ADnola+S%C3%A1nchez,+J.M.&amp;author=P%C3%A9rez+Tzili,+A.&amp;publication_year=2023&amp;journal=New+Gener.+Comput.&amp;volume=41&amp;pages=189%E2%80%93212&amp;doi=10.1007/s00354-023-00209-2" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1007/s00354-023-00209-2" class="cross-ref">CrossRef</a>]</li>
<li><div id="B36-informatics-11-00024">

</div>
Naseem, U.; Razzak, I.; Khushi, M.; Eklund, P.W.; Kim, J. COVIDSenti: A Large-Scale Benchmark Twitter Data Set for COVID-19 Sentiment Analysis. <span class="html-italic">IEEE Trans. Comput. Soc. Syst.</span> <strong>2021</strong>, <span class="html-italic">8</span>, 1003–1015. [<a href="https://scholar.google.com/scholar_lookup?title=COVIDSenti:+A+Large-Scale+Benchmark+Twitter+Data+Set+for+COVID-19+Sentiment+Analysis&amp;author=Naseem,+U.&amp;author=Razzak,+I.&amp;author=Khushi,+M.&amp;author=Eklund,+P.W.&amp;author=Kim,+J.&amp;publication_year=2021&amp;journal=IEEE+Trans.+Comput.+Soc.+Syst.&amp;volume=8&amp;pages=1003%E2%80%931015&amp;doi=10.1109/TCSS.2021.3051189&amp;pmid=35783149" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/TCSS.2021.3051189" class="cross-ref">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/pubmed/35783149" class="cross-ref pub_med">PubMed</a>]</li>
<li><div id="B37-informatics-11-00024">

</div>
Dimitrov, D.; Baran, E.; Fafalios, P.; Yu, R.; Zhu, X.; Zloch, M.; Dietze, S. TweetsCOV19—A Knowledge Base of Semantically Annotated Tweets about the COVID-19 Pandemic. In Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, Virtual Event, 19–23 October 2020; Association for Computing Machinery: New York, NY, USA; pp. 2991–2998. [<a href="https://scholar.google.com/scholar_lookup?title=TweetsCOV19%E2%80%94A+Knowledge+Base+of+Semantically+Annotated+Tweets+about+the+COVID-19+Pandemic&amp;conference=Proceedings+of+the+29th+ACM+International+Conference+on+Information+&amp;+Knowledge+Management&amp;author=Dimitrov,+D.&amp;author=Baran,+E.&amp;author=Fafalios,+P.&amp;author=Yu,+R.&amp;author=Zhu,+X.&amp;author=Zloch,+M.&amp;author=Dietze,+S.&amp;publication_year=2020&amp;pages=2991%E2%80%932998" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B38-informatics-11-00024">

</div>
Kabir, M.Y.; Madria, S. EMOCOV: Machine learning for emotion detection, analysis and visualization using COVID-19 tweets. <span class="html-italic">Online Soc. Netw. Media</span> <strong>2021</strong>, <span class="html-italic">23</span>, 100135. [<a href="https://scholar.google.com/scholar_lookup?title=EMOCOV:+Machine+learning+for+emotion+detection,+analysis+and+visualization+using+COVID-19+tweets&amp;author=Kabir,+M.Y.&amp;author=Madria,+S.&amp;publication_year=2021&amp;journal=Online+Soc.+Netw.+Media&amp;volume=23&amp;pages=100135&amp;doi=10.1016/j.osnem.2021.100135&amp;pmid=34722957" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.osnem.2021.100135" class="cross-ref">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/pubmed/34722957" class="cross-ref pub_med">PubMed</a>]</li>
<li><div id="B39-informatics-11-00024">

</div>
Lamsal, R. Design and analysis of a large-scale COVID-19 tweets dataset. <span class="html-italic">Appl. Intell.</span> <strong>2021</strong>, <span class="html-italic">51</span>, 2790–2804. [<a href="https://scholar.google.com/scholar_lookup?title=Design+and+analysis+of+a+large-scale+COVID-19+tweets+dataset&amp;author=Lamsal,+R.&amp;publication_year=2021&amp;journal=Appl.+Intell.&amp;volume=51&amp;pages=2790%E2%80%932804&amp;doi=10.1007/s10489-020-02029-z&amp;pmid=34764561" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1007/s10489-020-02029-z" class="cross-ref">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/pubmed/34764561" class="cross-ref pub_med">PubMed</a>]</li>
<li><div id="B40-informatics-11-00024">

</div>
Guo, R.; Xu, K. A Large-Scale Analysis of COVID-19 Twitter Dataset in a New Phase of the Pandemic. In Proceedings of the 2022 IEEE 12th International Conference on Electronics Information and Emergency Communication (ICEIEC), Beijing, China, 15–17 July 2022; pp. 276–281. [<a href="https://scholar.google.com/scholar_lookup?title=A+Large-Scale+Analysis+of+COVID-19+Twitter+Dataset+in+a+New+Phase+of+the+Pandemic&amp;conference=Proceedings+of+the+2022+IEEE+12th+International+Conference+on+Electronics+Information+and+Emergency+Communication+(ICEIEC)&amp;author=Guo,+R.&amp;author=Xu,+K.&amp;publication_year=2022&amp;pages=276%E2%80%93281&amp;doi=10.1109/ICEIEC54567.2022.9835047" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ICEIEC54567.2022.9835047" class="cross-ref">CrossRef</a>]</li>
<li><div id="B41-informatics-11-00024">

</div>
Hong, L.; Convertino, G.; Chi, E. Language Matters In Twitter: A Large Scale Study. In Proceedings of the International AAAI Conference on Web and Social Media, Virtually, 7–10 June 2021; Volume 5, pp. 518–521. [<a href="https://scholar.google.com/scholar_lookup?title=Language+Matters+In+Twitter:+A+Large+Scale+Study&amp;conference=Proceedings+of+the+International+AAAI+Conference+on+Web+and+Social+Media&amp;author=Hong,+L.&amp;author=Convertino,+G.&amp;author=Chi,+E.&amp;publication_year=2021&amp;pages=518%E2%80%93521" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B42-informatics-11-00024">

</div>
Lopez, C.E.; Gallemore, C. An augmented multilingual Twitter dataset for studying the COVID-19 infodemic. <span class="html-italic">Soc. Netw. Anal. Min.</span> <strong>2021</strong>, <span class="html-italic">11</span>, 102. [<a href="https://scholar.google.com/scholar_lookup?title=An+augmented+multilingual+Twitter+dataset+for+studying+the+COVID-19+infodemic&amp;author=Lopez,+C.E.&amp;author=Gallemore,+C.&amp;publication_year=2021&amp;journal=Soc.+Netw.+Anal.+Min.&amp;volume=11&amp;pages=102&amp;doi=10.1007/s13278-021-00825-0&amp;pmid=34697560" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1007/s13278-021-00825-0" class="cross-ref">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/pubmed/34697560" class="cross-ref pub_med">PubMed</a>]</li>
<li><div id="B43-informatics-11-00024">

</div>
Imran, M.; Qazi, U.; Ofli, F. TBCOV: Two Billion Multilingual COVID-19 Tweets with Sentiment, Entity, Geo, and Gender Labels. <span class="html-italic">Data</span> <strong>2022</strong>, <span class="html-italic">7</span>, 8. [<a href="https://scholar.google.com/scholar_lookup?title=TBCOV:+Two+Billion+Multilingual+COVID-19+Tweets+with+Sentiment,+Entity,+Geo,+and+Gender+Labels&amp;author=Imran,+M.&amp;author=Qazi,+U.&amp;author=Ofli,+F.&amp;publication_year=2022&amp;journal=Data&amp;volume=7&amp;pages=8&amp;doi=10.3390/data7010008" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/data7010008" class="cross-ref">CrossRef</a>]</li>
<li><div id="B44-informatics-11-00024">

</div>
Garcia, K.; Berton, L. Topic detection and sentiment analysis in Twitter content related to COVID-19 from Brazil and the USA. <span class="html-italic">Appl. Soft Comput.</span> <strong>2021</strong>, <span class="html-italic">101</span>, 107057. [<a href="https://scholar.google.com/scholar_lookup?title=Topic+detection+and+sentiment+analysis+in+Twitter+content+related+to+COVID-19+from+Brazil+and+the+USA&amp;author=Garcia,+K.&amp;author=Berton,+L.&amp;publication_year=2021&amp;journal=Appl.+Soft+Comput.&amp;volume=101&amp;pages=107057&amp;doi=10.1016/j.asoc.2020.107057&amp;pmid=33519326" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.asoc.2020.107057" class="cross-ref">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/pubmed/33519326" class="cross-ref pub_med">PubMed</a>]</li>
<li><div id="B45-informatics-11-00024">

</div>
Jonker, R.A.A.; Poudel, R.; Fajarda, O.; Matos, S.; Oliveira, J.L.; Lopes, R.P. Portuguese Twitter Dataset on COVID-19. In Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), Istanbul, Turkey, 10–13 November 2022; pp. 332–338. [<a href="https://scholar.google.com/scholar_lookup?title=Portuguese+Twitter+Dataset+on+COVID-19&amp;conference=Proceedings+of+the+2022+IEEE/ACM+International+Conference+on+Advances+in+Social+Networks+Analysis+and+Mining+(ASONAM)&amp;author=Jonker,+R.A.A.&amp;author=Poudel,+R.&amp;author=Fajarda,+O.&amp;author=Matos,+S.&amp;author=Oliveira,+J.L.&amp;author=Lopes,+R.P.&amp;publication_year=2022&amp;pages=332%E2%80%93338&amp;doi=10.1109/ASONAM55673.2022.10068592" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/ASONAM55673.2022.10068592" class="cross-ref">CrossRef</a>]</li>
<li><div id="B46-informatics-11-00024">

</div>
Yang, Q.; Alamro, H.; Albaradei, S.; Salhi, A.; Lv, X.; Ma, C.; Alshehri, M.; Jaber, I.; Tifratene, F.; Wang, W.; et al. SenWave: Monitoring the Global Sentiments under the COVID-19 Pandemic. <span class="html-italic">arXiv</span> <strong>2020</strong>, arXiv:2006.10842. [<a href="https://scholar.google.com/scholar_lookup?title=SenWave:+Monitoring+the+Global+Sentiments+under+the+COVID-19+Pandemic&amp;author=Yang,+Q.&amp;author=Alamro,+H.&amp;author=Albaradei,+S.&amp;author=Salhi,+A.&amp;author=Lv,+X.&amp;author=Ma,+C.&amp;author=Alshehri,+M.&amp;author=Jaber,+I.&amp;author=Tifratene,+F.&amp;author=Wang,+W.&amp;author=et+al.&amp;publication_year=2020&amp;journal=arXiv" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B47-informatics-11-00024">

</div>
Al-Laith, A.; Alenezi, M. Monitoring People’s Emotions and Symptoms from Arabic Tweets during the COVID-19 Pandemic. <span class="html-italic">Information</span> <strong>2021</strong>, <span class="html-italic">12</span>, 86. [<a href="https://scholar.google.com/scholar_lookup?title=Monitoring+People%E2%80%99s+Emotions+and+Symptoms+from+Arabic+Tweets+during+the+COVID-19+Pandemic&amp;author=Al-Laith,+A.&amp;author=Alenezi,+M.&amp;publication_year=2021&amp;journal=Information&amp;volume=12&amp;pages=86&amp;doi=10.3390/info12020086" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/info12020086" class="cross-ref">CrossRef</a>]</li>
<li><div id="B48-informatics-11-00024">

</div>
Balech, S.; Benavent, C.; Calciu, M. The First French COVID19 Lockdown Twitter Dataset. <span class="html-italic">arXiv</span> <strong>2020</strong>, arXiv:2005.05075. [<a href="https://scholar.google.com/scholar_lookup?title=The+First+French+COVID19+Lockdown+Twitter+Dataset&amp;author=Balech,+S.&amp;author=Benavent,+C.&amp;author=Calciu,+M.&amp;publication_year=2020&amp;journal=arXiv" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B49-informatics-11-00024">

</div>
Babić, K.; Petrović, M.; Beliga, S.; Martinčić-Ipšić, S.; Matešić, M.; Meštrović, A. Characterisation of COVID-19-Related Tweets in the Croatian Language: Framework Based on the Cro-CoV-cseBERT Model. <span class="html-italic">Appl. Sci.</span> <strong>2021</strong>, <span class="html-italic">11</span>, 442. [<a href="https://scholar.google.com/scholar_lookup?title=Characterisation+of+COVID-19-Related+Tweets+in+the+Croatian+Language:+Framework+Based+on+the+Cro-CoV-cseBERT+Model&amp;author=Babi%C4%87,+K.&amp;author=Petrovi%C4%87,+M.&amp;author=Beliga,+S.&amp;author=Martin%C4%8Di%C4%87-Ip%C5%A1i%C4%87,+S.&amp;author=Mate%C5%A1i%C4%87,+M.&amp;author=Me%C5%A1trovi%C4%87,+A.&amp;publication_year=2021&amp;journal=Appl.+Sci.&amp;volume=11&amp;pages=442&amp;doi=10.3390/app112110442" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/app112110442" class="cross-ref">CrossRef</a>]</li>
<li><div id="B50-informatics-11-00024">

</div>
Nurdeni, D.A.; Budi, I.; Santoso, A.B. Sentiment Analysis on Covid19 Vaccines in Indonesia: From The Perspective of Sinovac and Pfizer. In Proceedings of the 2021 3rd East Indonesia Conference on Computer and Information Technology (EIConCIT), Surabaya, Indonesia, 9–11 April 2021; pp. 122–127. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+Analysis+on+Covid19+Vaccines+in+Indonesia:+From+The+Perspective+of+Sinovac+and+Pfizer&amp;conference=Proceedings+of+the+2021+3rd+East+Indonesia+Conference+on+Computer+and+Information+Technology+(EIConCIT)&amp;author=Nurdeni,+D.A.&amp;author=Budi,+I.&amp;author=Santoso,+A.B.&amp;publication_year=2021&amp;pages=122%E2%80%93127&amp;doi=10.1109/EIConCIT50028.2021.9431852" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1109/EIConCIT50028.2021.9431852" class="cross-ref">CrossRef</a>]</li>
<li><div id="B51-informatics-11-00024">

</div>
Samaras, L.; García-Barriocanal, E.; Sicilia, M.A. Sentiment analysis of COVID-19 cases in Greece using Twitter data. <span class="html-italic">Expert Syst. Appl.</span> <strong>2023</strong>, <span class="html-italic">230</span>, 120577. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+analysis+of+COVID-19+cases+in+Greece+using+Twitter+data&amp;author=Samaras,+L.&amp;author=Garc%C3%ADa-Barriocanal,+E.&amp;author=Sicilia,+M.A.&amp;publication_year=2023&amp;journal=Expert+Syst.+Appl.&amp;volume=230&amp;pages=120577&amp;doi=10.1016/j.eswa.2023.120577" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.eswa.2023.120577" class="cross-ref">CrossRef</a>]</li>
<li><div id="B52-informatics-11-00024">

</div>
Cotik, V.; Debandi, N.; Luque, F.M.; Miguel, P.; Moro, A.; Pérez, J.M.; Serrati, P.; Zajac, J.; Zayat, D. A Study of Hate Speech in Social Media during the COVID-19 Outbreak. 2020. Available online: <a href="https://openreview.net/forum?id=01eOESDhbSW" class="uri">https://openreview.net/forum?id=01eOESDhbSW</a> (accessed on 15 April 2024).</li>
<li><div id="B53-informatics-11-00024">

</div>
Aragón, M.E.; Jarquín-Vásquez, H.J.; Montes-y Gómez, M.; Escalante, H.J.; Pineda, L.V.; Gómez-Adorno, H.; Posadas-Durán, J.P.; Bel-Enguix, G. Overview of MEX-A3T at IberLEF 2020: Fake News and Aggressiveness Analysis in Mexican Spanish. In Proceedings of the IberLEF@ SEPLN, Virtually, 22 September 2020; pp. 222–235. [<a href="https://scholar.google.com/scholar_lookup?title=Overview+of+MEX-A3T+at+IberLEF+2020:+Fake+News+and+Aggressiveness+Analysis+in+Mexican+Spanish&amp;conference=Proceedings+of+the+IberLEF@+SEPLN&amp;author=Arag%C3%B3n,+M.E.&amp;author=Jarqu%C3%ADn-V%C3%A1squez,+H.J.&amp;author=Montes-y+G%C3%B3mez,+M.&amp;author=Escalante,+H.J.&amp;author=Pineda,+L.V.&amp;author=G%C3%B3mez-Adorno,+H.&amp;author=Posadas-Dur%C3%A1n,+J.P.&amp;author=Bel-Enguix,+G.&amp;publication_year=2020&amp;pages=222%E2%80%93235" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B54-informatics-11-00024">

</div>
Vásquez, J.; Andersen, S.; Bel-Enguix, G.; Gómez-Adorno, H.; Ojeda-Trueba, S.L. Homo-mex: A mexican spanish annotated corpus for lgbt+ phobia detection on twitter. In Proceedings of the 7th Workshop on Online Abuse and Harms (WOAH), Toronto, ON, Canada, 13 July 2023; pp. 202–214. [<a href="https://scholar.google.com/scholar_lookup?title=Homo-mex:+A+mexican+spanish+annotated+corpus+for+lgbt++phobia+detection+on+twitter&amp;conference=Proceedings+of+the+7th+Workshop+on+Online+Abuse+and+Harms+(WOAH)&amp;author=V%C3%A1squez,+J.&amp;author=Andersen,+S.&amp;author=Bel-Enguix,+G.&amp;author=G%C3%B3mez-Adorno,+H.&amp;author=Ojeda-Trueba,+S.L.&amp;publication_year=2023&amp;pages=202%E2%80%93214" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B55-informatics-11-00024">

</div>
Martínez, R.Y.; Blanco, G.; Lourenço, A. Spanish Corpora of tweets about COVID-19 vaccination for automatic stance detection. <span class="html-italic">Inf. Process. Manag.</span> <strong>2023</strong>, <span class="html-italic">60</span>, 103294. [<a href="https://scholar.google.com/scholar_lookup?title=Spanish+Corpora+of+tweets+about+COVID-19+vaccination+for+automatic+stance+detection&amp;author=Mart%C3%ADnez,+R.Y.&amp;author=Blanco,+G.&amp;author=Louren%C3%A7o,+A.&amp;publication_year=2023&amp;journal=Inf.+Process.+Manag.&amp;volume=60&amp;pages=103294&amp;doi=10.1016/j.ipm.2023.103294" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.ipm.2023.103294" class="cross-ref">CrossRef</a>]</li>
<li><div id="B56-informatics-11-00024">

</div>
Plutchik, R. <span class="html-italic">The Emotions</span>; University Press of America: Lanham, MD, USA, 1991. [<a href="https://scholar.google.com/scholar_lookup?title=The+Emotions&amp;author=Plutchik,+R.&amp;publication_year=1991" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B57-informatics-11-00024">

</div>
Bender, E.M.; Friedman, B. Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. <span class="html-italic">Trans. Assoc. Comput. Linguist.</span> <strong>2018</strong>, <span class="html-italic">6</span>, 587–604. [<a href="https://scholar.google.com/scholar_lookup?title=Data+Statements+for+Natural+Language+Processing:+Toward+Mitigating+System+Bias+and+Enabling+Better+Science&amp;author=Bender,+E.M.&amp;author=Friedman,+B.&amp;publication_year=2018&amp;journal=Trans.+Assoc.+Comput.+Linguist.&amp;volume=6&amp;pages=587%E2%80%93604&amp;doi=10.1162/tacl_a_00041" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1162/tacl_a_00041" class="cross-ref">CrossRef</a>]</li>
<li><div id="B58-informatics-11-00024">

</div>
McHugh, M.L. Interrater reliability: The kappa statistic. <span class="html-italic">Biochem. Medica</span> <strong>2012</strong>, <span class="html-italic">22</span>, 276–282. [<a href="https://scholar.google.com/scholar_lookup?title=Interrater+reliability:+The+kappa+statistic&amp;author=McHugh,+M.L.&amp;publication_year=2012&amp;journal=Biochem.+Medica&amp;volume=22&amp;pages=276%E2%80%93282&amp;doi=10.11613/BM.2012.031" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.11613/BM.2012.031" class="cross-ref">CrossRef</a>]</li>
<li><div id="B59-informatics-11-00024">

</div>
Mikolov, T.; Chen, K.; Corrado, G.; Dean, J. Efficient estimation of word representations in vector space. <span class="html-italic">arXiv</span> <strong>2013</strong>, arXiv:1301.3781. [<a href="https://scholar.google.com/scholar_lookup?title=Efficient+estimation+of+word+representations+in+vector+space&amp;author=Mikolov,+T.&amp;author=Chen,+K.&amp;author=Corrado,+G.&amp;author=Dean,+J.&amp;publication_year=2013&amp;journal=arXiv" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B60-informatics-11-00024">

</div>
Le, Q.; Mikolov, T. Distributed representations of sentences and documents. In Proceedings of the International Conference on Machine Learning (PMLR), Beijing, China, 22–24 June 2014; pp. 1188–1196. [<a href="https://scholar.google.com/scholar_lookup?title=Distributed+representations+of+sentences+and+documents&amp;conference=Proceedings+of+the+International+Conference+on+Machine+Learning+(PMLR)&amp;author=Le,+Q.&amp;author=Mikolov,+T.&amp;publication_year=2014&amp;pages=1188%E2%80%931196" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B61-informatics-11-00024">

</div>
la Rosa y Eduardo, G. Ponferrada y Manu Romero y Paulo Villegas y Pablo González de Prado Salas y María Grandury, J.D. BERTIN: Efficient Pre-Training of a Spanish Language Model using Perplexity Sampling. <span class="html-italic">Proces. Leng. Nat.</span> <strong>2022</strong>, <span class="html-italic">68</span>, 13–23. [<a href="https://scholar.google.com/scholar_lookup?title=Ponferrada+y+Manu+Romero+y+Paulo+Villegas+y+Pablo+Gonz%C3%A1lez+de+Prado+Salas+y+Mar%C3%ADa+Grandury,+J.D.+BERTIN:+Efficient+Pre-Training+of+a+Spanish+Language+Model+using+Perplexity+Sampling&amp;author=la+Rosa+y+Eduardo,+G.&amp;publication_year=2022&amp;journal=Proces.+Leng.+Nat.&amp;volume=68&amp;pages=13%E2%80%9323" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B62-informatics-11-00024">

</div>
Pérez, J.M.; Furman, D.A.; Alemany, L.A.; Luque, F. RoBERTuito: A pre-trained language model for social media text in Spanish. <span class="html-italic">arXiv</span> <strong>2021</strong>, arXiv:2111.09453. [<a href="https://scholar.google.com/scholar_lookup?title=RoBERTuito:+A+pre-trained+language+model+for+social+media+text+in+Spanish&amp;author=P%C3%A9rez,+J.M.&amp;author=Furman,+D.A.&amp;author=Alemany,+L.A.&amp;author=Luque,+F.&amp;publication_year=2021&amp;journal=arXiv" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B63-informatics-11-00024">

</div>
Cañete, J.; Chaperon, G.; Fuentes, R.; Ho, J.H.; Kang, H.; Pérez, J. BETO, Spanish Pre-Trained BERT Model and Evaluation Data. In Proceedings of the PML4DC at ICLR 2020, Virtually, 26 April 2020. [<a href="https://scholar.google.com/scholar_lookup?title=BETO,+Spanish+Pre-Trained+BERT+Model+and+Evaluation+Data&amp;conference=Proceedings+of+the+PML4DC+at+ICLR+2020&amp;author=Ca%C3%B1ete,+J.&amp;author=Chaperon,+G.&amp;author=Fuentes,+R.&amp;author=Ho,+J.H.&amp;author=Kang,+H.&amp;author=P%C3%A9rez,+J.&amp;publication_year=2020" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B64-informatics-11-00024">

</div>
Tenney, I.; Das, D.; Pavlick, E. BERT rediscovers the classical NLP pipeline. <span class="html-italic">arXiv</span> <strong>2019</strong>, arXiv:1905.05950. [<a href="https://scholar.google.com/scholar_lookup?title=BERT+rediscovers+the+classical+NLP+pipeline&amp;author=Tenney,+I.&amp;author=Das,+D.&amp;author=Pavlick,+E.&amp;publication_year=2019&amp;journal=arXiv" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B65-informatics-11-00024">

</div>
Hutto, C.; Gilbert, E. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Proceedings of the International AAAI Conference on Web and Social Media, Ann Arbor, MI, USA, 1–4 June 2014; Volume 8, pp. 216–225. [<a href="https://scholar.google.com/scholar_lookup?title=Vader:+A+parsimonious+rule-based+model+for+sentiment+analysis+of+social+media+text&amp;conference=Proceedings+of+the+International+AAAI+Conference+on+Web+and+Social+Media&amp;author=Hutto,+C.&amp;author=Gilbert,+E.&amp;publication_year=2014&amp;pages=216%E2%80%93225" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B66-informatics-11-00024">

</div>
Pano, T.; Kashef, R. A Complete VADER-Based Sentiment Analysis of Bitcoin (BTC) Tweets during the Era of COVID-19. <span class="html-italic">Big Data Cogn. Comput.</span> <strong>2020</strong>, <span class="html-italic">4</span>, 33. [<a href="https://scholar.google.com/scholar_lookup?title=A+Complete+VADER-Based+Sentiment+Analysis+of+Bitcoin+(BTC)+Tweets+during+the+Era+of+COVID-19&amp;author=Pano,+T.&amp;author=Kashef,+R.&amp;publication_year=2020&amp;journal=Big+Data+Cogn.+Comput.&amp;volume=4&amp;pages=33&amp;doi=10.3390/bdcc4040033" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.3390/bdcc4040033" class="cross-ref">CrossRef</a>]</li>
<li><div id="B67-informatics-11-00024">

</div>
de Albornoz, J.C.; Plaza, L.; Gervás, P. SentiSense: An easily scalable concept-based affective lexicon for sentiment analysis. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, 21–27 May 2012; Calzolari, N., Choukri, K., Declerck, T., Doğan, M.U., Maegaard, B., Mariani, J., Moreno, A., Odijk, J., Piperidis, S., Eds.; European Language Resources Association (ELRA): Luxemburg, 2012; pp. 3562–3567. [<a href="https://scholar.google.com/scholar_lookup?title=SentiSense:+An+easily+scalable+concept-based+affective+lexicon+for+sentiment+analysis&amp;conference=Proceedings+of+the+Eighth+International+Conference+on+Language+Resources+and+Evaluation+(LREC%E2%80%9912)&amp;author=de+Albornoz,+J.C.&amp;author=Plaza,+L.&amp;author=Gerv%C3%A1s,+P.&amp;publication_year=2012&amp;pages=3562%E2%80%933567" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B68-informatics-11-00024">

</div>
Pérez, J.M.; Giudici, J.C.; Luque, F. pysentimiento: A Python Toolkit for Sentiment Analysis and SocialNLP tasks. <span class="html-italic">arXiv</span> <strong>2021</strong>, arXiv:2106.09462. [<a href="https://scholar.google.com/scholar_lookup?title=pysentimiento:+A+Python+Toolkit+for+Sentiment+Analysis+and+SocialNLP+tasks&amp;author=P%C3%A9rez,+J.M.&amp;author=Giudici,+J.C.&amp;author=Luque,+F.&amp;publication_year=2021&amp;journal=arXiv" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B69-informatics-11-00024">

</div>
Prabhat, A.; Khullar, V. Sentiment classification on big data using Naive Bayes and logistic regression. In Proceedings of the 2017 International Conference on Computer Communication and Informatics (ICCCI), Coimbatore, India, 5–7 January 2017; pp. 1–5. [<a href="https://scholar.google.com/scholar_lookup?title=Sentiment+classification+on+big+data+using+Naive+Bayes+and+logistic+regression&amp;conference=Proceedings+of+the+2017+International+Conference+on+Computer+Communication+and+Informatics+(ICCCI)&amp;author=Prabhat,+A.&amp;author=Khullar,+V.&amp;publication_year=2017&amp;pages=1%E2%80%935" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B70-informatics-11-00024">

</div>
Lewis, D.D. Naive (Bayes) at forty: The independence assumption in information retrieval. In Proceedings of the European Conference on Machine Learning, Chemnitz, Germany, 21–23 April 1998; Springer: Berlin/Heidelberg, Germany, 1998; pp. 4–15. [<a href="https://scholar.google.com/scholar_lookup?title=Naive+(Bayes)+at+forty:+The+independence+assumption+in+information+retrieval&amp;conference=Proceedings+of+the+European+Conference+on+Machine+Learning&amp;author=Lewis,+D.D.&amp;publication_year=1998&amp;pages=4%E2%80%9315" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B71-informatics-11-00024">

</div>
Domingos, P.; Pazzani, M. On the Optimality of the Simple Bayesian Classifier under Zero-One Loss. <span class="html-italic">Mach. Learn.</span> <strong>1997</strong>, <span class="html-italic">29</span>, 103–130. [<a href="https://scholar.google.com/scholar_lookup?title=On+the+Optimality+of+the+Simple+Bayesian+Classifier+under+Zero-One+Loss&amp;author=Domingos,+P.&amp;author=Pazzani,+M.&amp;publication_year=1997&amp;journal=Mach.+Learn.&amp;volume=29&amp;pages=103%E2%80%93130&amp;doi=10.1023/A:1007413511361" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1023/A:1007413511361" class="cross-ref">CrossRef</a>]</li>
<li><div id="B72-informatics-11-00024">

</div>
Colas, F.; Brazdil, P. Comparison of SVM and some older classification algorithms in text classification tasks. In Proceedings of the Artificial Intelligence in Theory and Practice: IFIP 19th World Computer Congress, TC 12: IFIP AI 2006 Stream, Santiago, Chile, 21–24 August 2006; Springer: Berlin/Heidelberg, Germany, 2006; pp. 169–178. [<a href="https://scholar.google.com/scholar_lookup?title=Comparison+of+SVM+and+some+older+classification+algorithms+in+text+classification+tasks&amp;conference=Proceedings+of+the+Artificial+Intelligence+in+Theory+and+Practice:+IFIP+19th+World+Computer+Congress,+TC+12:+IFIP+AI+2006+Stream&amp;author=Colas,+F.&amp;author=Brazdil,+P.&amp;publication_year=2006&amp;pages=169%E2%80%93178" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B73-informatics-11-00024">

</div>
Abiodun, O.I.; Jantan, A.; Omolara, A.E.; Dada, K.V.; Mohamed, N.A.; Arshad, H. State-of-the-art in artificial neural network applications: A survey. <span class="html-italic">Heliyon</span> <strong>2018</strong>, <span class="html-italic">4</span>, e00938. [<a href="https://scholar.google.com/scholar_lookup?title=State-of-the-art+in+artificial+neural+network+applications:+A+survey&amp;author=Abiodun,+O.I.&amp;author=Jantan,+A.&amp;author=Omolara,+A.E.&amp;author=Dada,+K.V.&amp;author=Mohamed,+N.A.&amp;author=Arshad,+H.&amp;publication_year=2018&amp;journal=Heliyon&amp;volume=4&amp;pages=e00938&amp;doi=10.1016/j.heliyon.2018.e00938&amp;pmid=30519653" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1016/j.heliyon.2018.e00938" class="cross-ref">CrossRef</a>] [<a href="https://www.ncbi.nlm.nih.gov/pubmed/30519653" class="cross-ref pub_med">PubMed</a>]</li>
<li><div id="B74-informatics-11-00024">

</div>
Guo, M.H.; Xu, T.X.; Liu, J.J.; Liu, Z.N.; Jiang, P.T.; Mu, T.J.; Zhang, S.H.; Martin, R.R.; Cheng, M.M.; Hu, S.M. Attention mechanisms in computer vision: A survey. <span class="html-italic">Comput. Vis. Media</span> <strong>2022</strong>, <span class="html-italic">8</span>, 331–368. [<a href="https://scholar.google.com/scholar_lookup?title=Attention+mechanisms+in+computer+vision:+A+survey&amp;author=Guo,+M.H.&amp;author=Xu,+T.X.&amp;author=Liu,+J.J.&amp;author=Liu,+Z.N.&amp;author=Jiang,+P.T.&amp;author=Mu,+T.J.&amp;author=Zhang,+S.H.&amp;author=Martin,+R.R.&amp;author=Cheng,+M.M.&amp;author=Hu,+S.M.&amp;publication_year=2022&amp;journal=Comput.+Vis.+Media&amp;volume=8&amp;pages=331%E2%80%93368&amp;doi=10.1007/s41095-022-0271-y" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1007/s41095-022-0271-y" class="cross-ref">CrossRef</a>]</li>
<li><div id="B75-informatics-11-00024">

</div>
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin, I. Attention is all you need. In Proceedings of the Advances in Neural Information Processing Systems, Long Beach, CA, USA, 4–9 December 2017; pp. 5998–6008. [<a href="https://scholar.google.com/scholar_lookup?title=Attention+is+all+you+need&amp;conference=Proceedings+of+the+Advances+in+Neural+Information+Processing+Systems&amp;author=Vaswani,+A.&amp;author=Shazeer,+N.&amp;author=Parmar,+N.&amp;author=Uszkoreit,+J.&amp;author=Jones,+L.&amp;author=Gomez,+A.N.&amp;author=Kaiser,+%C5%81.&amp;author=Polosukhin,+I.&amp;publication_year=2017&amp;pages=5998%E2%80%936008" class="google-scholar">Google Scholar</a>]</li>
<li><div id="B76-informatics-11-00024">

</div>
Stewart, G.W. On the early history of the singular value decomposition. <span class="html-italic">SIAM Rev.</span> <strong>1993</strong>, <span class="html-italic">35</span>, 551–566. [<a href="https://scholar.google.com/scholar_lookup?title=On+the+early+history+of+the+singular+value+decomposition&amp;author=Stewart,+G.W.&amp;publication_year=1993&amp;journal=SIAM+Rev.&amp;volume=35&amp;pages=551%E2%80%93566&amp;doi=10.1137/1035134" class="google-scholar">Google Scholar</a>] [<a href="https://doi.org/10.1137/1035134" class="cross-ref">CrossRef</a>]</li>
</ol>
</section>
<section id="FiguresandTables" type="display-objects">
<div id="informatics-11-00024-f001" class="html-fig-wrap" data-position="float">
<div class="html-fig_img">
<div class="html-figpopup html-figpopup-link" href="#fig_body_display_informatics-11-00024-f001">
<img src="media/file2.jpg" alt="Informatics 11 00024 g001" /> <a href="ch001.xhtml#fig_body_display_informatics-11-00024-f001" class="html-expand html-figpopup"></a>
</div>
</div>
<div class="html-fig_description">
<strong>Figure 1.</strong> Sentiment analysis experimentation workflow. <!--     <p><a class="html-figpopup" href="#fig_body_display_informatics-11-00024-f001">
      Click here to enlarge figure
    </a></p> -->
</div>
</div>
<div id="fig_body_display_informatics-11-00024-f001" class="html-fig_show mfp-hide">
<div class="html-caption">
<strong>Figure 1.</strong> Sentiment analysis experimentation workflow.
</div>
<div class="html-img">
<img src="media/file3.png" alt="Informatics 11 00024 g001" />
</div>
</div>
<div id="informatics-11-00024-f002" class="html-fig-wrap" data-position="float">
<div class="html-fig_img">
<div class="html-figpopup html-figpopup-link" href="#fig_body_display_informatics-11-00024-f002">
<img src="media/file4.jpg" alt="Informatics 11 00024 g002" /> <a href="ch001.xhtml#fig_body_display_informatics-11-00024-f002" class="html-expand html-figpopup"></a>
</div>
</div>
<div class="html-fig_description">
<strong>Figure 2.</strong> Test Accuracy for different number of features. (<strong>a</strong>) Without vs with stopwords using unigrams. (<strong>b</strong>) <span class="html-italic">n</span>-gram test results. We tested <math display="inline"><semantics> <mrow> <mi>N</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>3</mn> </mrow> </semantics></math>. <!--     <p><a class="html-figpopup" href="#fig_body_display_informatics-11-00024-f002">
      Click here to enlarge figure
    </a></p> -->
</div>
</div>
<div id="fig_body_display_informatics-11-00024-f002" class="html-fig_show mfp-hide">
<div class="html-caption">
<strong>Figure 2.</strong> Test Accuracy for different number of features. (<strong>a</strong>) Without vs with stopwords using unigrams. (<strong>b</strong>) <span class="html-italic">n</span>-gram test results. We tested <math display="inline"><semantics> <mrow> <mi>N</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mn>3</mn> </mrow> </semantics></math>.
</div>
<div class="html-img">
<img src="media/file5.png" alt="Informatics 11 00024 g002" />
</div>
</div>
<div id="informatics-11-00024-f003" class="html-fig-wrap" data-position="float">
<div class="html-fig_img">
<div class="html-figpopup html-figpopup-link" href="#fig_body_display_informatics-11-00024-f003">
<img src="media/file6.jpg" alt="Informatics 11 00024 g003" /> <a href="ch001.xhtml#fig_body_display_informatics-11-00024-f003" class="html-expand html-figpopup"></a>
</div>
</div>
<div class="html-fig_description">
<strong>Figure 3.</strong> (<strong>a</strong>) Most significant words given by <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> and (<strong>b</strong>) accuracy on the test set for the different number of features. We show results for the term frequency vector reduced by the term frequency (solid line) and the <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> (dashed line). <!--     <p><a class="html-figpopup" href="#fig_body_display_informatics-11-00024-f003">
      Click here to enlarge figure
    </a></p> -->
</div>
</div>
<div id="fig_body_display_informatics-11-00024-f003" class="html-fig_show mfp-hide">
<div class="html-caption">
<strong>Figure 3.</strong> (<strong>a</strong>) Most significant words given by <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> and (<strong>b</strong>) accuracy on the test set for the different number of features. We show results for the term frequency vector reduced by the term frequency (solid line) and the <math display="inline"><semantics> <msup> <mi>χ</mi> <mn>2</mn> </msup> </semantics></math> (dashed line).
</div>
<div class="html-img">
<img src="media/file7.png" alt="Informatics 11 00024 g003" />
</div>
</div>
<div id="informatics-11-00024-f004" class="html-fig-wrap" data-position="float">
<div class="html-fig_img">
<div class="html-figpopup html-figpopup-link" href="#fig_body_display_informatics-11-00024-f004">
<img src="media/file8.jpg" alt="Informatics 11 00024 g004" /> <a href="ch001.xhtml#fig_body_display_informatics-11-00024-f004" class="html-expand html-figpopup"></a>
</div>
</div>
<div class="html-fig_description">
<strong>Figure 4.</strong> Explained variance for <span class="html-italic">n</span> components. <!--     <p><a class="html-figpopup" href="#fig_body_display_informatics-11-00024-f004">
      Click here to enlarge figure
    </a></p> -->
</div>
</div>
<div id="fig_body_display_informatics-11-00024-f004" class="html-fig_show mfp-hide">
<div class="html-caption">
<strong>Figure 4.</strong> Explained variance for <span class="html-italic">n</span> components.
</div>
<div class="html-img">
<img src="media/file9.png" alt="Informatics 11 00024 g004" />
</div>
</div>
<div id="informatics-11-00024-t001" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t001">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t001" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 1.</strong> Lexicon used to filter the COVID-19 related tweets for the corpus creation.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t001" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 1.</strong> Lexicon used to filter the COVID-19 related tweets for the corpus creation.
</div>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">VARIANTS COVID</th>
<th style="text-align: left;">SYMPTOMS</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">COVID-19</td>
<td style="text-align: left;">me dio diarrea</td>
</tr>
<tr class="even">
<td style="text-align: left;">coronavirus</td>
<td style="text-align: left;">dolor de cabeza agudo</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Covid-19</td>
<td style="text-align: left;">cuerpo cortado</td>
</tr>
<tr class="even">
<td style="text-align: left;">Coronavirus</td>
<td style="text-align: left;">fiebre (leve)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Covid19</td>
<td style="text-align: left;">tos (seca)</td>
</tr>
<tr class="even">
<td style="text-align: left;">Covid</td>
<td style="text-align: left;">dolor de garganta</td>
</tr>
<tr class="odd">
<td style="text-align: left;">lo del contagio</td>
<td style="text-align: left;">altas tamperaturas</td>
</tr>
<tr class="even">
<td style="text-align: left;">esta pandemia</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Corona Virus</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">el virus</td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>HASHTAGS</strong></td>
<td style="text-align: left;"><strong>HASHTAGS</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">#AbrahamSealaverga</td>
<td style="text-align: left;">#Covid19</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#AburridoEnCasa</td>
<td style="text-align: left;">#covidmexico</td>
</tr>
<tr class="even">
<td style="text-align: left;">#AislamientoSocial</td>
<td style="text-align: left;">#CuarentenaCoronavirus</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#BastaDeFakeNews</td>
<td style="text-align: left;">#CuidaALosTuyos</td>
</tr>
<tr class="even">
<td style="text-align: left;">#carroñavirus</td>
<td style="text-align: left;">#CuidemosALosMayoresYPequeños</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#CODVID19</td>
<td style="text-align: left;">#EnCuarentena</td>
</tr>
<tr class="even">
<td style="text-align: left;">#ConferenciaCovid19</td>
<td style="text-align: left;">#MeQuedoEnHome</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#ConLaFuerzaDeLosProtocolosSI</td>
<td style="text-align: left;">#NoSonVacaciones</td>
</tr>
<tr class="even">
<td style="text-align: left;">#Coronavirus</td>
<td style="text-align: left;">#QuedateEnCasa</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#CoronavirusMx</td>
<td style="text-align: left;">#QuédateEnTuCasa</td>
</tr>
<tr class="even">
<td style="text-align: left;">#coronaviruspeleishon</td>
<td style="text-align: left;">#COVID19mexico</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#COVID19mx</td>
<td style="text-align: left;">#CuandoEstoSeAcabe</td>
</tr>
<tr class="even">
<td style="text-align: left;">#Cuarentena</td>
<td style="text-align: left;">#cuarentenamexico</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#CuidaALosDemas</td>
<td style="text-align: left;">#CuidarnosEsTareaDeTodos</td>
</tr>
<tr class="even">
<td style="text-align: left;">#Cuidate</td>
<td style="text-align: left;">#CulturaEnCasa</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#encasa</td>
<td style="text-align: left;">#Enfermera</td>
</tr>
<tr class="even">
<td style="text-align: left;">#MeQuedoEnCasa</td>
<td style="text-align: left;">#México</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#NeumoniaAtipica</td>
<td style="text-align: left;">#QuedarseEnCasa</td>
</tr>
<tr class="even">
<td style="text-align: left;">#quédate</td>
<td style="text-align: left;">#QuédateEnCasaUnMesMas</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#QuedateEnLaCasa</td>
<td style="text-align: left;">#QuedateEnTuCasaCarajo</td>
</tr>
<tr class="even">
<td style="text-align: left;">#quedateentuputacasaalaverga</td>
<td style="text-align: left;">#QuedenseEnCasa</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#QuePorMiNoQuede</td>
<td style="text-align: left;">#sabadodecuarentena</td>
</tr>
<tr class="even">
<td style="text-align: left;">#SaltilloQuédateEnCasa</td>
<td style="text-align: left;">#SeFuerteMexico</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#SiTeSalesTeMueres</td>
<td style="text-align: left;">#StayAtHome</td>
</tr>
<tr class="even">
<td style="text-align: left;">#StayAtHomeAndStaySafe</td>
<td style="text-align: left;">#Super</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#SusanaDistancia</td>
<td style="text-align: left;">#teamwork</td>
</tr>
<tr class="even">
<td style="text-align: left;">#TecuidasTúNosCuidamosTodos</td>
<td style="text-align: left;">#TipsDeCuarentena</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#ÚltimaHora</td>
<td style="text-align: left;">#UltimaOportunidad</td>
</tr>
<tr class="even">
<td style="text-align: left;">#YaBastaDeFakeNews</td>
<td style="text-align: left;">#yolecreoagattel</td>
</tr>
<tr class="odd">
<td style="text-align: left;">#YoMeQuedoEnCASA</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t002" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t002">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t002" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 2.</strong> Agreement score by the annotators of the classification of sentiments without a guide.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t002" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 2.</strong> Agreement score by the annotators of the classification of sentiments without a guide.
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Annotator Pair</th>
<th style="text-align: left;">A&amp;B</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Percent of agreement</td>
<td style="text-align: left;">41%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Cohen’s <math display="inline"><semantics> <mi>κ</mi> </semantics></math> score</td>
<td style="text-align: left;">0.1785</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t003" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t003">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t003" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 3.</strong> Agreement scores by each pair of annotators of the classification with the guide.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t003" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 3.</strong> Agreement scores by each pair of annotators of the classification with the guide.
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Annotator Pair</th>
<th style="text-align: left;">1&amp;2</th>
<th style="text-align: left;">2&amp;3</th>
<th style="text-align: left;">1&amp;3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Percent of agreement</td>
<td style="text-align: left;">61%</td>
<td style="text-align: left;">70%</td>
<td style="text-align: left;">62%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Cohen’s <math display="inline"><semantics> <mi>κ</mi> </semantics></math> score</td>
<td style="text-align: left;">0.3945</td>
<td style="text-align: left;">0.5547</td>
<td style="text-align: left;">0.3716</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t004" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t004">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t004" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 4.</strong> General statistics computed from word counts on each tweet.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t004" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 4.</strong> General statistics computed from word counts on each tweet.
</div>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Positive Tag</th>
<th style="text-align: left;">Negative Tag</th>
<th style="text-align: left;">Neutral Tag</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Average number of words per tweet</td>
<td style="text-align: left;">22.85</td>
<td style="text-align: left;">26.39</td>
<td style="text-align: left;">20.97</td>
</tr>
<tr class="even">
<td style="text-align: left;">Standard Deviation</td>
<td style="text-align: left;">12.69</td>
<td style="text-align: left;">15.46</td>
<td style="text-align: left;">13.59</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Variance</td>
<td style="text-align: left;">161.14</td>
<td style="text-align: left;">239.02</td>
<td style="text-align: left;">184.65</td>
</tr>
<tr class="even">
<td style="text-align: left;">Minimum number of words in a tweet</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">3</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Maximum number of words in a tweet</td>
<td style="text-align: left;">59</td>
<td style="text-align: left;">339</td>
<td style="text-align: left;">88</td>
</tr>
<tr class="even">
<td style="text-align: left;">Total number of words</td>
<td style="text-align: left;">25,729</td>
<td style="text-align: left;">48,398</td>
<td style="text-align: left;">38,580</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Tweets count</td>
<td style="text-align: left;">1126</td>
<td style="text-align: left;">1834</td>
<td style="text-align: left;">1840</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t005" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t005">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t005" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 5.</strong> Original (raw) and processed version of a sample of tweets.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t005" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 5.</strong> Original (raw) and processed version of a sample of tweets.
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Raw Tweet</th>
<th style="text-align: left;">Processed Tweet</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">#CuarentenaNacional #CDMX consulta:https://t.co/TcjustEg</td>
<td style="text-align: left;">cuarentena nacional cdmx consulta url</td>
</tr>
<tr class="even">
<td style="text-align: left;">Buen díaa!!!#ConCaféEnMano para alegrar la mañana #EnCasa</td>
<td style="text-align: left;">buen dia con cafe en mano para alegrar la mañana en casa</td>
</tr>
<tr class="odd">
<td style="text-align: left;">@CONANPmx@GobiernoMX lleno de gente en Av. Tenorio</td>
<td style="text-align: left;">usuario lleno de gente en av tenorio</td>
</tr>
<tr class="even">
<td style="text-align: left;">Marcarle a mi preciosita en momento de crisis. 🥺 🥺 🥺</td>
<td style="text-align: left;">marcarle preciosa momento crisis cara por favor</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Uff 😢 <span id="informatics-11-00024-i001" class="html-fig-inline"><img src="media/file10.png" alt="Informatics 11 00024 i001" /></span> <span id="informatics-11-00024-i001" class="html-fig-inline"><img src="media/file10.png" alt="Informatics 11 00024 i001" /></span> <span id="informatics-11-00024-i001" class="html-fig-inline"><img src="media/file10.png" alt="Informatics 11 00024 i001" /></span> #QuedateEnCasa <span id="informatics-11-00024-i002" class="html-fig-inline"><img src="media/file11.png" alt="Informatics 11 00024 i002" /></span>#Coahuila #Mexico 😥</td>
<td style="text-align: left;">uf cara triste alivio microbio quedar casa jardin coahuila mexico</td>
</tr>
<tr class="even">
<td style="text-align: left;">#SNTEsalud <span id="informatics-11-00024-i003" class="html-fig-inline"><img src="media/file12.png" alt="Informatics 11 00024 i003" /></span> <span id="informatics-11-00024-i004" class="html-fig-inline"><img src="media/file13.png" alt="Informatics 11 00024 i004" /></span>ALERTA alto contagio en los mochis</td>
<td style="text-align: left;">sntesalud simbolo medicina advertencia alerta alto contagiar mochis</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t007" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t007">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t007" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 7.</strong> Sorted features with smallest and largest Tf-Idf values.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t007" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 7.</strong> Sorted features with smallest and largest Tf-Idf values.
</div>
<table>
<thead>
<tr>
<th align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-center">
Tf-Idf Values
</th>
<th align="left" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-left">
Features
</th>
</tr>
</thead>
<tbody>
<tr>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Smallest
</td>
<td align="left" valign="middle" style="border-bottom:solid thin" class="html-align-left">
‘buen lunes’, ‘app’, ‘inicio semana’, ‘inmediato’, ‘oms’, ‘periodico hoy’
</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" class="html-align-left">
‘alto contagio’, ‘ganar seguidor’, ‘calidad’, ‘amlolujo’
</td>
</tr>
<tr>
<td rowspan="2" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Largest
</td>
<td align="left" valign="middle" style="border-bottom:solid thin" class="html-align-left">
‘financiero’, ‘muerte covid’, ‘lugar’, ‘movil’, ‘movilidad’, ‘dar positivo’
</td>
</tr>
<tr>
<td align="left" valign="middle" style="border-bottom:solid thin" class="html-align-left">
‘lopez’, ‘muerte’, ‘cuidarte profesional’, ‘gracia’
</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t008" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t008">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t008" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 8.</strong> Phrase detection tokens yield by each model.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t008" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 8.</strong> Phrase detection tokens yield by each model.
</div>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Phrase Detection</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Unigram</td>
<td style="text-align: left;">[‘@usuario’, ‘por’, ‘su’, ‘trabajo’, ‘no’, ‘es’, ‘justo’,</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">‘para’, ‘los’, ‘demas’, ‘quedate’, ‘en’, ‘casa’]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Bigram</td>
<td style="text-align: left;">[’@usuario’, ‘por’, ‘su trabajo’, ‘no es’,</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">‘justo’, ‘para’, ‘los’, ‘demas’, ‘quedate’, ‘en casa’]</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Trigram</td>
<td style="text-align: left;">[’@usuario’, ‘por’, ‘su’, ‘trabajo’, ‘no es</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;">justo’, ‘para’, ‘los’, ‘demas’, ‘quedate en casa’]</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t009" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t009">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t009" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 9.</strong> TextBlob outputs for different statements in Spanish.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t009" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 9.</strong> TextBlob outputs for different statements in Spanish.
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Input</th>
<th style="text-align: left;">‘polarity’</th>
<th style="text-align: left;">‘subjectivity’</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Este teléfono tiene una pantalla de excelente resolución, además es muy rápido</td>
<td style="text-align: left;">0.63</td>
<td style="text-align: left;">0.89</td>
</tr>
<tr class="even">
<td style="text-align: left;">Este teléfono tiene una pantalla de alta resolución, además es rápido</td>
<td style="text-align: left;">0.18</td>
<td style="text-align: left;">0.57</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Este telefono es lo máximo, lo adoro &lt;3 :D</td>
<td style="text-align: left;">1.0</td>
<td style="text-align: left;">1.0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Este telefono no me gusta :(</td>
<td style="text-align: left;">−0.75</td>
<td style="text-align: left;">1.0</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t010" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t010">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t010" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 10.</strong> Vader outputs for different statements (in Spanish).
</div>
</div>
<div id="table_body_display_informatics-11-00024-t010" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 10.</strong> Vader outputs for different statements (in Spanish).
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Input</th>
<th style="text-align: left;">‘neg’</th>
<th style="text-align: left;">‘neu’</th>
<th style="text-align: left;">‘pos’</th>
<th style="text-align: left;">‘compound’</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">hoy es un pésimo día</td>
<td style="text-align: left;">0.779</td>
<td style="text-align: left;">0.221</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">−0.5461</td>
</tr>
<tr class="even">
<td style="text-align: left;">hoy es un mal día</td>
<td style="text-align: left;">0.646</td>
<td style="text-align: left;">0.354</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">−0.7424</td>
</tr>
<tr class="odd">
<td style="text-align: left;">hoy es un día cualquiera</td>
<td style="text-align: left;">0.123</td>
<td style="text-align: left;">0.637</td>
<td style="text-align: left;">0.24</td>
<td style="text-align: left;">0.231</td>
</tr>
<tr class="even">
<td style="text-align: left;">hoy es un gran día</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.408</td>
<td style="text-align: left;">0.592</td>
<td style="text-align: left;">0.5404</td>
</tr>
<tr class="odd">
<td style="text-align: left;">hoy es un excelente día</td>
<td style="text-align: left;">0.0</td>
<td style="text-align: left;">0.294</td>
<td style="text-align: left;">0.706</td>
<td style="text-align: left;">0.8633</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t011" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t011">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t011" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 11.</strong> Distribution of labels in the train and test partitions.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t011" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 11.</strong> Distribution of labels in the train and test partitions.
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">(Seed = 37)</th>
<th style="text-align: left;">Negative</th>
<th style="text-align: left;">Neutral</th>
<th style="text-align: left;">Positive</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Train</td>
<td style="text-align: left;">33.642%</td>
<td style="text-align: left;">44.934%</td>
<td style="text-align: left;">21.422%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Test</td>
<td style="text-align: left;">34.899%</td>
<td style="text-align: left;">44.380%</td>
<td style="text-align: left;">20.713%</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t012" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t012">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t012" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 12.</strong> Accuracy for <span class="html-italic">n</span> components.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t012" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 12.</strong> Accuracy for <span class="html-italic">n</span> components.
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">n_components</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1000</td>
<td style="text-align: left;">63.12%</td>
</tr>
<tr class="even">
<td style="text-align: left;">1500</td>
<td style="text-align: left;">64.79%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">2000</td>
<td style="text-align: left;">65.76%</td>
</tr>
<tr class="even">
<td style="text-align: left;">2500</td>
<td style="text-align: left;">65.51%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3000</td>
<td style="text-align: left;">64.83%</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t013" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t013">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t013" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 13.</strong> Test accuracy for Doc2Vec models. The best result is highlighted in bold.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t013" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 13.</strong> Test accuracy for Doc2Vec models. The best result is highlighted in bold.
</div>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">1-Gram</th>
<th style="text-align: left;">2-Gram</th>
<th style="text-align: left;">3-Gram</th>
<th style="text-align: left;">Best</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">DBOW</td>
<td style="text-align: left;">60.642%</td>
<td style="text-align: left;">59.934%</td>
<td style="text-align: left;">60.422%</td>
<td style="text-align: left;">60.642%</td>
</tr>
<tr class="even">
<td style="text-align: left;">DMC</td>
<td style="text-align: left;">56.893%</td>
<td style="text-align: left;">54.387%</td>
<td style="text-align: left;">55.713%</td>
<td style="text-align: left;">56.893%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DMM</td>
<td style="text-align: left;">59.641%</td>
<td style="text-align: left;">58.935%</td>
<td style="text-align: left;">57.253%</td>
<td style="text-align: left;">59.641%</td>
</tr>
<tr class="even">
<td style="text-align: left;">DBOW+DMC</td>
<td style="text-align: left;">61.927%</td>
<td style="text-align: left;">61.234%</td>
<td style="text-align: left;">62.422%</td>
<td style="text-align: left;">62.422%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">DBOW+DMM</td>
<td style="text-align: left;">63.185%</td>
<td style="text-align: left;">62.617%</td>
<td style="text-align: left;">63.373%</td>
<td style="text-align: left;"><strong>63.373</strong>%</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t014" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t014">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t014" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 14.</strong> Optimal hyperparameters settings selected for each model based on th optimization of accuracy through grid-search and cross-validation.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t014" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 14.</strong> Optimal hyperparameters settings selected for each model based on th optimization of accuracy through grid-search and cross-validation.
</div>
<table>
<thead>
<tr>
<th colspan="3" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-center">
GridSearchCV (CV = 10)
</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<strong>Model</strong>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<strong>Hyperparameters Tested</strong>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<strong>Optimal Value</strong>
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Logistic<br />
Regression
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<math display="inline"> <semantics> <mrow> <mspace width="-0.166667em"></mspace> <mtable displaystyle="true"> <mtr> <mtd columnalign="right"> <mrow> <mi mathvariant="normal">C</mi> </mrow> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>:</mo> <mo>[</mo> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>5</mn> </mrow> </msup> <mo>,</mo> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>4</mn> </mrow> </msup> <mo>,</mo> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>3</mn> </mrow> </msup> <mo>,</mo> <mn>0.01</mn> <mo>,</mo> <mn>0.1</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo> <mn>10</mn> <mo>,</mo> <msup> <mn>10</mn> <mn>2</mn> </msup> <mo>]</mo> </mrow> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd></mtd> <mtd columnalign="left"> <mrow> <mo> </mo> <mo> </mo> <mo>[</mo> <mn>0.1</mn> <mo>,</mo> <mn>0.2</mn> <mo>,</mo> <mn>0.3</mn> <mo>,</mo> <mn>0.4</mn> <mo>,</mo> <mn>0.5</mn> <mo>,</mo> <mn>0.6</mn> <mo>,</mo> <mn>0.7</mn> <mo>,</mo> <mn>0.8</mn> <mo>,</mo> <mn>0.9</mn> <mo>]</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mrow> <mi>solver</mi> </mrow> </mtd> <mtd columnalign="left"> <mrow> <mo>:</mo> <mo>[</mo> <mo>‘</mo> <mi>newton</mi> <mo>−</mo> <mi>cg</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>lbfgs</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>liblinear</mi> <mo>’</mo> <mo>]</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mrow> <mi>penalty</mi> </mrow> </mtd> <mtd columnalign="left"> <mrow> <mo>:</mo> <mo>[</mo> <mo>‘</mo> <mi>none</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi mathvariant="normal">l</mi> <mn>1</mn> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi mathvariant="normal">l</mi> <mn>2</mn> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>elasticnet</mi> <mo>’</mo> <mo>]</mo> </mrow> </mtd> </mtr> </mtable> </mrow> </semantics> </math>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<math display="inline"> <semantics> <mrow> <mspace width="-0.166667em"></mspace> <mtable displaystyle="true"> <mtr> <mtd columnalign="right"> <mi mathvariant="normal">C</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mn>0.7</mn> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>solver</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>‘</mo> <mi>lbfgs</mi> <mo>’</mo> </mrow> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>penalty</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>‘</mo> <mi mathvariant="normal">l</mi> <mn>2</mn> <mo>’</mo> </mrow> </mrow> </mtd> </mtr> </mtable> </mrow> </semantics> </math>
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Multinomial<br />
Naive<br />
Bayes
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<math display="inline"> <semantics> <mrow> <mspace width="-0.166667em"></mspace> <mtable displaystyle="true"> <mtr> <mtd columnalign="right"> <mi>alpha</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mn>20</mn> <mi>values</mi> <mi>from</mi> <mn>0</mn> <mi>to</mi> <mn>1</mn> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mrow> <mi>fit</mi> <mo>_</mo> <mi>prior</mi> <mo>:</mo> </mrow> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>[</mo> <mo>‘</mo> <mi>true</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>false</mi> <mo>’</mo> <mo>]</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mrow> <mi>class</mi> <mo>_</mo> <mi>prior</mi> <mo>:</mo> </mrow> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>[</mo> <mi>None</mi> <mo>,</mo> <mo>[</mo> <mn>0.35</mn> <mo>,</mo> <mn>0.4</mn> <mo>,</mo> <mn>0.25</mn> <mo>]</mo> <mo>]</mo> </mrow> </mrow> </mtd> </mtr> </mtable> </mrow> </semantics> </math>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<math display="inline"> <semantics> <mrow> <mspace width="-0.166667em"></mspace> <mtable displaystyle="true"> <mtr> <mtd columnalign="right"> <mi>alpha</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mn>0.85</mn> </mrow> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mrow> <mi>force</mi> <mo>_</mo> <mi>alpha</mi> <mo>:</mo> </mrow> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>‘</mo> <mi>true</mi> <mo>’</mo> </mrow> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mrow> <mi>class</mi> <mo>_</mo> <mi>prior</mi> <mo>:</mo> </mrow> </mtd> <mtd columnalign="left"> <mrow> <mi>None</mi> </mrow> </mtd> </mtr> </mtable> </mrow> </semantics> </math>
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
SVM
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<math display="inline"> <semantics> <mrow> <mspace width="-0.166667em"></mspace> <mtable displaystyle="true"> <mtr> <mtd columnalign="right"> <mi mathvariant="normal">C</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>[</mo> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>5</mn> </mrow> </msup> <mo>,</mo> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>4</mn> </mrow> </msup> <mo>,</mo> <msup> <mn>10</mn> <mrow> <mo>−</mo> <mn>3</mn> </mrow> </msup> <mo>,</mo> <mn>0.01</mn> <mo>,</mo> <mn>0.1</mn> <mo>,</mo> <mn>1</mn> <mo>,</mo> <mn>10</mn> <mo>,</mo> <msup> <mn>10</mn> <mn>2</mn> </msup> <mo>]</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>kernel</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>[</mo> <mo>‘</mo> <mi>linear</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>poly</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>rbf</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>sigmoid</mi> <mo>’</mo> <mo>]</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>gamma</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>[</mo> <mo>‘</mo> <mi>scale</mi> <mo>’</mo> <mo>]</mo> </mrow> </mrow> </mtd> </mtr> </mtable> </mrow> </semantics> </math>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<math display="inline"> <semantics> <mrow> <mspace width="-0.166667em"></mspace> <mtable displaystyle="true"> <mtr> <mtd columnalign="right"> <mi mathvariant="normal">C</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mn>1</mn> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>gamma</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>‘</mo> <mi>scale</mi> <mo>’</mo> </mrow> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>kernel</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>‘</mo> <mi>poly</mi> <mo>’</mo> </mrow> </mrow> </mtd> </mtr> </mtable> </mrow> </semantics> </math>
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Multilayer<br />
Perceptron
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<math display="inline"> <semantics> <mrow> <mspace width="-0.166667em"></mspace> <mtable displaystyle="true"> <mtr> <mtd columnalign="right"> <mi>Layers</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mi>from</mi> <mn>1</mn> <mi>to</mi> <mn>5</mn> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>solver</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>[</mo> <mo>‘</mo> <mi>lbfgs</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>sgd</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>adam</mi> <mo>’</mo> <mo>]</mo> </mrow> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>Activation</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>[</mo> <mo>‘</mo> <mi>relu</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>tanh</mi> <mo>’</mo> <mo>,</mo> <mo>‘</mo> <mi>logistic</mi> <mo>’</mo> <mo>]</mo> </mrow> </mrow> </mtd> </mtr> </mtable> </mrow> </semantics> </math>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<math display="inline"> <semantics> <mrow> <mspace width="-0.166667em"></mspace> <mtable displaystyle="true"> <mtr> <mtd columnalign="right"> <mi>Layers</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mn>2</mn> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>solver</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>‘</mo> <mi>adam</mi> <mo>’</mo> </mrow> <mo>,</mo> </mrow> </mtd> </mtr> <mtr> <mtd columnalign="right"> <mi>Activation</mi> <mo>:</mo> </mtd> <mtd columnalign="left"> <mrow> <mrow> <mo>‘</mo> <mi>relu</mi> <mo>’</mo> </mrow> </mrow> </mtd> </mtr> </mtable> </mrow> </semantics> </math>
</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t015" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t015">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t015" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 15.</strong> Results of the obtained by training the classification algorithms on the three feature sets. The best result is highlighted in bold.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t015" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 15.</strong> Results of the obtained by training the classification algorithms on the three feature sets. The best result is highlighted in bold.
</div>
<table>
<thead>
<tr>
<th colspan="6" align="center" valign="middle" style="border-bottom:solid thin;border-top:solid thin" class="html-align-center">
Unconstrained BoW
</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<strong>KNN</strong>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<strong>SVM</strong>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<strong>MNB</strong>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<strong>Logistic</strong>
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
<strong>MLP</strong>
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Accuracy
</td>
<td align="center" valign="middle" class="html-align-center">
59.75%
</td>
<td align="center" valign="middle" class="html-align-center">
62.31%
</td>
<td align="center" valign="middle" class="html-align-center">
62.03%
</td>
<td align="center" valign="middle" class="html-align-center">
64.26%
</td>
<td align="center" valign="middle" class="html-align-center">
63.51%
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Precision
</td>
<td align="center" valign="middle" class="html-align-center">
60.08%
</td>
<td align="center" valign="middle" class="html-align-center">
62.25%
</td>
<td align="center" valign="middle" class="html-align-center">
63.24%
</td>
<td align="center" valign="middle" class="html-align-center">
64.39%
</td>
<td align="center" valign="middle" class="html-align-center">
63.92%
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Recall
</td>
<td align="center" valign="middle" class="html-align-center">
59.76%
</td>
<td align="center" valign="middle" class="html-align-center">
62.31%
</td>
<td align="center" valign="middle" class="html-align-center">
62.03%
</td>
<td align="center" valign="middle" class="html-align-center">
64.25%
</td>
<td align="center" valign="middle" class="html-align-center">
63.51%
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
F1-score
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
58.78%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
61.96%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
60.78%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
63.61%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
62.59%
</td>
</tr>
<tr>
<td colspan="6" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
BoW reduced with SVD
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
KNN
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
SVM
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
MNB
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Logistic
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
MLP
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Accuracy
</td>
<td align="center" valign="middle" class="html-align-center">
60.88%
</td>
<td align="center" valign="middle" class="html-align-center">
64.98%
</td>
<td align="center" valign="middle" class="html-align-center">
64.19%
</td>
<td align="center" valign="middle" class="html-align-center">
<strong>67.12</strong>%
</td>
<td align="center" valign="middle" class="html-align-center">
68.84%
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Precision
</td>
<td align="center" valign="middle" class="html-align-center">
61.40%
</td>
<td align="center" valign="middle" class="html-align-center">
64.91%
</td>
<td align="center" valign="middle" class="html-align-center">
62.27%
</td>
<td align="center" valign="middle" class="html-align-center">
66.91%
</td>
<td align="center" valign="middle" class="html-align-center">
67.25%
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Recall
</td>
<td align="center" valign="middle" class="html-align-center">
60.88%
</td>
<td align="center" valign="middle" class="html-align-center">
64.98%
</td>
<td align="center" valign="middle" class="html-align-center">
64.19%
</td>
<td align="center" valign="middle" class="html-align-center">
67.12%
</td>
<td align="center" valign="middle" class="html-align-center">
68.84%
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
F1-score
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
59.92%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
64.02%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
62.98%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
66.24%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
67.90%
</td>
</tr>
<tr>
<td colspan="6" align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Doc2Vec with DBOW+DMM
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
KNN
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
SVM
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
MNB
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
Logistic
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
MLP
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Accuracy
</td>
<td align="center" valign="middle" class="html-align-center">
56.47%
</td>
<td align="center" valign="middle" class="html-align-center">
62.56%
</td>
<td align="center" valign="middle" class="html-align-center">
62.96%
</td>
<td align="center" valign="middle" class="html-align-center">
63.68%
</td>
<td align="center" valign="middle" class="html-align-center">
64.31%
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Precision
</td>
<td align="center" valign="middle" class="html-align-center">
54.76%
</td>
<td align="center" valign="middle" class="html-align-center">
63.81%
</td>
<td align="center" valign="middle" class="html-align-center">
63.54%
</td>
<td align="center" valign="middle" class="html-align-center">
61.94%
</td>
<td align="center" valign="middle" class="html-align-center">
60.81%
</td>
</tr>
<tr>
<td align="center" valign="middle" class="html-align-center">
Recall
</td>
<td align="center" valign="middle" class="html-align-center">
56.47%
</td>
<td align="center" valign="middle" class="html-align-center">
62.56%
</td>
<td align="center" valign="middle" class="html-align-center">
62.96%
</td>
<td align="center" valign="middle" class="html-align-center">
63.68%
</td>
<td align="center" valign="middle" class="html-align-center">
62.31%
</td>
</tr>
<tr>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
F1-score
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
42.93%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
57.59%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
63.88%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
61.55%
</td>
<td align="center" valign="middle" style="border-bottom:solid thin" class="html-align-center">
62.11%
</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t016" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t016">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t016" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 16.</strong> Results obtained by the sentiment analysis libraries. The best result is highlighted in bold.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t016" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 16.</strong> Results obtained by the sentiment analysis libraries. The best result is highlighted in bold.
</div>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">TextBlob</th>
<th style="text-align: left;">Nltk Vader</th>
<th style="text-align: left;">Pysentimiento</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">accuracy</td>
<td style="text-align: left;">51.23%</td>
<td style="text-align: left;">58.07%</td>
<td style="text-align: left;"><strong>68.89</strong>%</td>
</tr>
<tr class="even">
<td style="text-align: left;">precision</td>
<td style="text-align: left;">55.45%</td>
<td style="text-align: left;">58.60%</td>
<td style="text-align: left;">72.20%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">recall</td>
<td style="text-align: left;">51.23%</td>
<td style="text-align: left;">57.19%</td>
<td style="text-align: left;">52.81%</td>
</tr>
<tr class="even">
<td style="text-align: left;">F1-score</td>
<td style="text-align: left;">52.92%</td>
<td style="text-align: left;">56.42%</td>
<td style="text-align: left;">60.38%</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t017" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t017">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t017" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 17.</strong> Classification results of the Spanish BERT models. The best result is highlighted in bold.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t017" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 17.</strong> Classification results of the Spanish BERT models. The best result is highlighted in bold.
</div>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">BETO-Uncased</th>
<th style="text-align: left;">roBERTa-Sentiment</th>
<th style="text-align: left;">BerTin-Base</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">training set accuracy</td>
<td style="text-align: left;">96.20%</td>
<td style="text-align: left;">97.54%</td>
<td style="text-align: left;">96.91%</td>
</tr>
<tr class="even">
<td style="text-align: left;">validation set accuracy</td>
<td style="text-align: left;"><strong>73.26</strong>%</td>
<td style="text-align: left;">71.88%</td>
<td style="text-align: left;">72.14%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">validation loss</td>
<td style="text-align: left;">0.3945</td>
<td style="text-align: left;">0.2847</td>
<td style="text-align: left;">0.2141</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t018" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t018">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t018" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 18.</strong> Results of an increasing number of epochs using BETO. The best result is highlighted in bold.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t018" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 18.</strong> Results of an increasing number of epochs using BETO. The best result is highlighted in bold.
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Epoch</th>
<th style="text-align: left;">Train Set Accuracy</th>
<th style="text-align: left;">Test Set Accuracy</th>
<th style="text-align: left;">Validation Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">3</td>
<td style="text-align: left;">92.89%</td>
<td style="text-align: left;">70.33%</td>
<td style="text-align: left;">0.4554</td>
</tr>
<tr class="even">
<td style="text-align: left;">5</td>
<td style="text-align: left;">96.20%</td>
<td style="text-align: left;"><strong>73.26</strong>%</td>
<td style="text-align: left;">0.3945</td>
</tr>
<tr class="odd">
<td style="text-align: left;">10</td>
<td style="text-align: left;">97.12%</td>
<td style="text-align: left;">72.76%</td>
<td style="text-align: left;">0.3161</td>
</tr>
</tbody>
</table>
</div>
<div id="informatics-11-00024-t019" class="html-table-wrap" data-position="float">
<div class="html-table_wrap_td">
<div class="html-tablepopup html-tablepopup-link" href="#table_body_display_informatics-11-00024-t019">
<img src="media/file1.png" /> <a href="ch001.xhtml#table_body_display_informatics-11-00024-t019" class="html-expand html-tablepopup"></a>
</div>
</div>
<div class="html-table_wrap_discription">
<strong>Table 19.</strong> Summary of the performance evaluated based on the accuracy of different sentiment analysis models on the SENT-COVID corpus. The best result is highlighted in bold.
</div>
</div>
<div id="table_body_display_informatics-11-00024-t019" class="html-table_show mfp-hide">
<div class="html-caption">
<strong>Table 19.</strong> Summary of the performance evaluated based on the accuracy of different sentiment analysis models on the SENT-COVID corpus. The best result is highlighted in bold.
</div>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">TextBlob</td>
<td style="text-align: left;">51.23%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Nltk Vader</td>
<td style="text-align: left;">58.07%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Pysentimiento</td>
<td style="text-align: left;">68.89%</td>
</tr>
<tr class="even">
<td style="text-align: left;">SVM</td>
<td style="text-align: left;">64.89%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Naive Bayes</td>
<td style="text-align: left;">62.22%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Logistic Regression</td>
<td style="text-align: left;">67.12%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">MLP</td>
<td style="text-align: left;">68.84%</td>
</tr>
<tr class="even">
<td style="text-align: left;">BETO-uncased</td>
<td style="text-align: left;"><strong>73.26</strong>%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">roBERTa-sentiment</td>
<td style="text-align: left;">71.88%</td>
</tr>
<tr class="even">
<td style="text-align: left;">BerTin-base</td>
<td style="text-align: left;">72.14 %</td>
</tr>
</tbody>
</table>
</div>
</section>
<section class="html-fn_group">
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td style="text-align: left;"><div class="html-p">
<strong>Disclaimer/Publisher’s Note:</strong> The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.
</div></td>
</tr>
</tbody>
</table>
</section>
<section id="html-copyright">
<br />
© 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (<a href="https://creativecommons.org/licenses/by/4.0/" class="uri">https://creativecommons.org/licenses/by/4.0/</a>).
</section>
</div>
</article>
<div id="preview-toolbar" class="tw-fixed tw-top-12 tw-right-1 tw-z-50 tw-flex tw-flex-col tw-gap-y-1 tw-text-sm tw-items-end" data-controller="ref">
<span id="stream-warpper" style="display: none;"></span>
<div class="tw-w-fit">
<button class="tw-cursor-pointer tw-inline-flex tw-self-center tw-items-center tw-px-3 tw-py-2.5 tw-w-full tw-rounded-md tw-border tw-bg-white/60 tw-text-center" data-action="ref#checkACSOrder" id="check-acs-order-btn">
Check ACS Ref Order
</button>
</div>
<div class="tw-w-fit">
<button class="tw-cursor-pointer tw-inline-flex tw-self-center tw-items-center tw-px-3 tw-py-2.5 tw-w-full tw-rounded-md tw-border tw-bg-white/60 tw-text-center" data-action="ref#checkFootNoteOrder" id="check-fn-order-btn">
Check Foot Note Order
</button>
</div>
<div class="tw-w-fit">
<button class="tw-cursor-pointer tw-inline-flex tw-self-center tw-items-center tw-px-3 tw-py-2.5 tw-w-full tw-rounded-md tw-border tw-bg-white/60 tw-text-center" data-action="ref#checkCrossref" id="check-crossref-btn">
Check CrossRef
</button>
</div>
<div id="invalid-dois">

</div>
<div class="tw-mt-16 tw-mb-2 tw-h-32 tw-inline-flex" data-controller="text-search" style="display: none;">
<textarea type="text" class="tw-px-2 tw-py-1 tw-bg-white tw-rounded-md" style="resize: none" placeholder="search multiple line on page" id="article-search-text-input tw-border tw-border-solid" autocomplete="off" data-action="text-search#perform">
</textarea>
</div>
<div class="tw-w-fit">
<button class="tw-cursor-pointer tw-inline-flex tw-self-center tw-items-center tw-px-3 tw-py-2.5 tw-w-full tw-rounded-md tw-border tw-bg-white/60 tw-text-center" data-action data-reflex="click-&gt;PmcCheck#create" id="check-pmc-btn" data-article_id="gid://xml-to-html/Article/685c51e4823d9b0a3778bd31">
Check DTD &amp; PMC Style
</button>
</div>
<div id="check-pmc-result" class="tw-flex tw-flex-col tw-gap-y-1 tw-items-end tw-pl-2 tw-max-w-[14rem] tw-bg-white/90">

</div>
<div class="tw-w-fit" data-controller="link-validator">
<button class="tw-cursor-pointer tw-inline-flex tw-self-center tw-items-center tw-px-3 tw-py-2.5 tw-w-full tw-rounded-md tw-border tw-bg-white/60 tw-text-center" data-action="click-&gt;link-validator#perform" id="check-link-btn">
Check Links
</button>
</div>
<div id="validate-link-result">

</div>
</div>
<style type="text/css">
      body {
        font-size: 11pt;
      }
    </style>
</section>
</body>
</html>

