{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5f85cd",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e185828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from utils import d2l\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661e8c4",
   "metadata": {},
   "source": [
    "## 9.5 Recurrent Neural Networks from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aeb661bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNScratch(nn.Module):\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, device='cpu'):\n",
    "        super().__init__()\n",
    "        # Set device\n",
    "        self.device = device\n",
    "\n",
    "        # Set hyperparameters\n",
    "        self.n_inputs = n_inputs # Length of input sequence\n",
    "        self.n_hidden = n_hidden # Number of hidden dimensions\n",
    "        self.n_outputs = n_outputs # Length of output dimensions\n",
    "\n",
    "        # Initialize latent layer parameters\n",
    "        self.W_xh = self._init_parameter((n_inputs, n_hidden))\n",
    "        self.W_hh = self._init_parameter((n_hidden, n_hidden))\n",
    "        self.b_h = self._init_zeros((n_hidden,))\n",
    "\n",
    "        # Initialize output layer parameters\n",
    "        self.W_hq = self._init_parameter((n_hidden, n_outputs))\n",
    "        self.b_q = self._init_zeros((n_outputs,))\n",
    "    \n",
    "    def _init_parameter(self, shape: tuple):\n",
    "        # Initialize parameters with Xavier distribution\n",
    "        return nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(shape, device=self.device)))\n",
    "    \n",
    "    def _init_zeros(self, shape: tuple):\n",
    "        # Initialize bias with zeros\n",
    "        return nn.Parameter(torch.zeros(shape, device=self.device))\n",
    "\n",
    "    def forward(self, X: torch.Tensor, hidden_state: torch.Tensor = None, training: bool = True):\n",
    "        if training and (hidden_state is None):\n",
    "            # Hidden state does not require gradient\n",
    "            self.hidden_state = torch.zeros((X.shape[0], self.n_hidden), device=self.device, requires_grad=False)\n",
    "        \n",
    "        # Compute the new hidden state\n",
    "        L_cat = torch.cat((X, self.hidden_state), 1)\n",
    "        R_cat = torch.cat((self.W_xh, self.W_hh), 0)\n",
    "        self.hidden_state = torch.tanh(torch.mm(L_cat, R_cat) + self.b_h)\n",
    "        \n",
    "        # Compute output\n",
    "        output = torch.mm(self.hidden_state, self.W_hq) + self.b_q\n",
    "        \n",
    "        return output, self.hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed6f3fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn_epoch(net, train_iterator, loss, optimizer):\n",
    "    \"\"\"Train a model for one epoch.\"\"\"\n",
    "    # Set the model to training mode\n",
    "    net.train()\n",
    "    # Initialize the total loss and number of samples\n",
    "    total_loss, num_samples = 0, 0\n",
    "\n",
    "    hidden_state = None\n",
    "    for X, y in train_iterator:\n",
    "        # Move data to the appropriate device\n",
    "        X, y = X.to(net.device), y.to(net.device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_hat, hidden_state = net(X, hidden_state)\n",
    "\n",
    "        # Compute the loss\n",
    "        l = loss(y_hat, y[:, -1].long())\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        l.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the total loss and number of samples\n",
    "        total_loss += l.item() * y.shape[0]\n",
    "        num_samples += y.shape[0]\n",
    "    return total_loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ca5e0810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "282c4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15574453",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_machine = d2l.TimeMachine(64, seq_len, 10112, 5056)\n",
    "train_data = time_machine.get_dataloader(False)\n",
    "test_data = time_machine.get_dataloader(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39bd2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNScratch(n_inputs=seq_len, n_hidden=512, n_outputs=len(time_machine.vocab), device=device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb862e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.9843\n",
      "Epoch 2, Loss: 2.7560\n",
      "Epoch 3, Loss: 2.6404\n",
      "Epoch 4, Loss: 2.5428\n",
      "Epoch 5, Loss: 2.4350\n",
      "Epoch 6, Loss: 2.3342\n",
      "Epoch 7, Loss: 2.2483\n",
      "Epoch 8, Loss: 2.1761\n",
      "Epoch 9, Loss: 2.1292\n",
      "Epoch 10, Loss: 2.1013\n",
      "Epoch 11, Loss: 2.0783\n",
      "Epoch 12, Loss: 2.0217\n",
      "Epoch 13, Loss: 1.9936\n",
      "Epoch 14, Loss: 1.9313\n",
      "Epoch 15, Loss: 1.8846\n",
      "Epoch 16, Loss: 1.8441\n",
      "Epoch 17, Loss: 1.8392\n",
      "Epoch 18, Loss: 1.7996\n",
      "Epoch 19, Loss: 1.7684\n",
      "Epoch 20, Loss: 1.7034\n",
      "Epoch 21, Loss: 1.6301\n",
      "Epoch 22, Loss: 1.5975\n",
      "Epoch 23, Loss: 1.5893\n",
      "Epoch 24, Loss: 1.5297\n",
      "Epoch 25, Loss: 1.4526\n",
      "Epoch 26, Loss: 1.4264\n",
      "Epoch 27, Loss: 1.3930\n",
      "Epoch 28, Loss: 1.3600\n",
      "Epoch 29, Loss: 1.3353\n",
      "Epoch 30, Loss: 1.3095\n",
      "Epoch 31, Loss: 1.2898\n",
      "Epoch 32, Loss: 1.2726\n",
      "Epoch 33, Loss: 1.2813\n",
      "Epoch 34, Loss: 1.1605\n",
      "Epoch 35, Loss: 1.0478\n",
      "Epoch 36, Loss: 1.0398\n",
      "Epoch 37, Loss: 1.0277\n",
      "Epoch 38, Loss: 0.9523\n",
      "Epoch 39, Loss: 0.9336\n",
      "Epoch 40, Loss: 0.9034\n",
      "Epoch 41, Loss: 0.8376\n",
      "Epoch 42, Loss: 0.7923\n",
      "Epoch 43, Loss: 0.7955\n",
      "Epoch 44, Loss: 0.7965\n",
      "Epoch 45, Loss: 0.7755\n",
      "Epoch 46, Loss: 0.7524\n",
      "Epoch 47, Loss: 0.7363\n",
      "Epoch 48, Loss: 0.7193\n",
      "Epoch 49, Loss: 0.6864\n",
      "Epoch 50, Loss: 0.6620\n",
      "Epoch 51, Loss: 0.5956\n",
      "Epoch 52, Loss: 0.5688\n",
      "Epoch 53, Loss: 0.5502\n",
      "Epoch 54, Loss: 0.5166\n",
      "Epoch 55, Loss: 0.4521\n",
      "Epoch 56, Loss: 0.4423\n",
      "Epoch 57, Loss: 0.4393\n",
      "Epoch 58, Loss: 0.4149\n",
      "Epoch 59, Loss: 0.3772\n",
      "Epoch 60, Loss: 0.3491\n",
      "Epoch 61, Loss: 0.3165\n",
      "Epoch 62, Loss: 0.2855\n",
      "Epoch 63, Loss: 0.2563\n",
      "Epoch 64, Loss: 0.2214\n",
      "Epoch 65, Loss: 0.1851\n",
      "Epoch 66, Loss: 0.1650\n",
      "Epoch 67, Loss: 0.1482\n",
      "Epoch 68, Loss: 0.1208\n",
      "Epoch 69, Loss: 0.1113\n",
      "Epoch 70, Loss: 0.0978\n",
      "Epoch 71, Loss: 0.0819\n",
      "Epoch 72, Loss: 0.0713\n",
      "Epoch 73, Loss: 0.0597\n",
      "Epoch 74, Loss: 0.0523\n",
      "Epoch 75, Loss: 0.0457\n",
      "Epoch 76, Loss: 0.0415\n",
      "Epoch 77, Loss: 0.0364\n",
      "Epoch 78, Loss: 0.0321\n",
      "Epoch 79, Loss: 0.0285\n",
      "Epoch 80, Loss: 0.0252\n",
      "Epoch 81, Loss: 0.0220\n",
      "Epoch 82, Loss: 0.0194\n",
      "Epoch 83, Loss: 0.0173\n",
      "Epoch 84, Loss: 0.0156\n",
      "Epoch 85, Loss: 0.0142\n",
      "Epoch 86, Loss: 0.0130\n",
      "Epoch 87, Loss: 0.0120\n",
      "Epoch 88, Loss: 0.0111\n",
      "Epoch 89, Loss: 0.0104\n",
      "Epoch 90, Loss: 0.0097\n",
      "Epoch 91, Loss: 0.0091\n",
      "Epoch 92, Loss: 0.0085\n",
      "Epoch 93, Loss: 0.0080\n",
      "Epoch 94, Loss: 0.0076\n",
      "Epoch 95, Loss: 0.0072\n",
      "Epoch 96, Loss: 0.0068\n",
      "Epoch 97, Loss: 0.0064\n",
      "Epoch 98, Loss: 0.0061\n",
      "Epoch 99, Loss: 0.0058\n",
      "Epoch 100, Loss: 0.0055\n",
      "Epoch 101, Loss: 0.0053\n",
      "Epoch 102, Loss: 0.0050\n",
      "Epoch 103, Loss: 0.0048\n",
      "Epoch 104, Loss: 0.0046\n",
      "Epoch 105, Loss: 0.0044\n",
      "Epoch 106, Loss: 0.0042\n",
      "Epoch 107, Loss: 0.0040\n",
      "Epoch 108, Loss: 0.0039\n",
      "Epoch 109, Loss: 0.0037\n",
      "Epoch 110, Loss: 0.0036\n",
      "Epoch 111, Loss: 0.0034\n",
      "Epoch 112, Loss: 0.0033\n",
      "Epoch 113, Loss: 0.0031\n",
      "Epoch 114, Loss: 0.0030\n",
      "Epoch 115, Loss: 0.0029\n",
      "Epoch 116, Loss: 0.0028\n",
      "Epoch 117, Loss: 0.0027\n",
      "Epoch 118, Loss: 0.0026\n",
      "Epoch 119, Loss: 0.0025\n",
      "Epoch 120, Loss: 0.0024\n",
      "Epoch 121, Loss: 0.0023\n",
      "Epoch 122, Loss: 0.0022\n",
      "Epoch 123, Loss: 0.0021\n",
      "Epoch 124, Loss: 0.0020\n",
      "Epoch 125, Loss: 0.0020\n",
      "Epoch 126, Loss: 0.0019\n",
      "Epoch 127, Loss: 0.0018\n",
      "Epoch 128, Loss: 0.0017\n",
      "Epoch 129, Loss: 0.0017\n",
      "Epoch 130, Loss: 0.0016\n",
      "Epoch 131, Loss: 0.0015\n",
      "Epoch 132, Loss: 0.0015\n",
      "Epoch 133, Loss: 0.0014\n",
      "Epoch 134, Loss: 0.0014\n",
      "Epoch 135, Loss: 0.0013\n",
      "Epoch 136, Loss: 0.0013\n",
      "Epoch 137, Loss: 0.0012\n",
      "Epoch 138, Loss: 0.0012\n",
      "Epoch 139, Loss: 0.0011\n",
      "Epoch 140, Loss: 0.0011\n",
      "Epoch 141, Loss: 0.0010\n",
      "Epoch 142, Loss: 0.0010\n",
      "Epoch 143, Loss: 0.0010\n",
      "Epoch 144, Loss: 0.0009\n",
      "Epoch 145, Loss: 0.0009\n",
      "Epoch 146, Loss: 0.0009\n",
      "Epoch 147, Loss: 0.0008\n",
      "Epoch 148, Loss: 0.0008\n",
      "Epoch 149, Loss: 0.0008\n",
      "Epoch 150, Loss: 0.0007\n",
      "Epoch 151, Loss: 0.0007\n",
      "Epoch 152, Loss: 0.0007\n",
      "Epoch 153, Loss: 0.0007\n",
      "Epoch 154, Loss: 0.0006\n",
      "Epoch 155, Loss: 0.0006\n",
      "Epoch 156, Loss: 0.0006\n",
      "Epoch 157, Loss: 0.0006\n",
      "Epoch 158, Loss: 0.0005\n",
      "Epoch 159, Loss: 0.0005\n",
      "Epoch 160, Loss: 0.0005\n",
      "Epoch 161, Loss: 0.0005\n",
      "Epoch 162, Loss: 0.0005\n",
      "Epoch 163, Loss: 0.0004\n",
      "Epoch 164, Loss: 0.0004\n",
      "Epoch 165, Loss: 0.0004\n",
      "Epoch 166, Loss: 0.0004\n",
      "Epoch 167, Loss: 0.0004\n",
      "Epoch 168, Loss: 0.0004\n",
      "Epoch 169, Loss: 0.0003\n",
      "Epoch 170, Loss: 0.0003\n",
      "Epoch 171, Loss: 0.0003\n",
      "Epoch 172, Loss: 0.0003\n",
      "Epoch 173, Loss: 0.0003\n",
      "Epoch 174, Loss: 0.0003\n",
      "Epoch 175, Loss: 0.0003\n",
      "Epoch 176, Loss: 0.0003\n",
      "Epoch 177, Loss: 0.0003\n",
      "Epoch 178, Loss: 0.0002\n",
      "Epoch 179, Loss: 0.0002\n",
      "Epoch 180, Loss: 0.0002\n",
      "Epoch 181, Loss: 0.0002\n",
      "Epoch 182, Loss: 0.0002\n",
      "Epoch 183, Loss: 0.0002\n",
      "Epoch 184, Loss: 0.0002\n",
      "Epoch 185, Loss: 0.0002\n",
      "Epoch 186, Loss: 0.0002\n",
      "Epoch 187, Loss: 0.0002\n",
      "Epoch 188, Loss: 0.0002\n",
      "Epoch 189, Loss: 0.0002\n",
      "Epoch 190, Loss: 0.0001\n",
      "Epoch 191, Loss: 0.0001\n",
      "Epoch 192, Loss: 0.0001\n",
      "Epoch 193, Loss: 0.0001\n",
      "Epoch 194, Loss: 0.0001\n",
      "Epoch 195, Loss: 0.0001\n",
      "Epoch 196, Loss: 0.0001\n",
      "Epoch 197, Loss: 0.0001\n",
      "Epoch 198, Loss: 0.0001\n",
      "Epoch 199, Loss: 0.0001\n",
      "Epoch 200, Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_rnn_epoch(net, train_data, loss, optimizer)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6bd6e7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98110efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eller with a slight accession of cheerfu \n",
      " ller with a slight accession of cheerful\n",
      " two dimensions but how about up and dow \n",
      " two dimensions but how about up and down\n",
      "raced and caressed us rather than submit \n",
      " aced and caressed us rather than submitt\n",
      "ciety said i erected on a strictly commu \n",
      " iety said i erected on a strictly commun\n",
      "al existence there i object said filby oi\n",
      " l existence there i object said filby of\n",
      "ong it but some foolish people have got c\n",
      " ng it but some foolish people have got h\n",
      "rimental verification said the time trave\n",
      " imental verification said the time trave\n",
      "tter because it happens that our conscio \n",
      " ter because it happens that our consciou\n",
      " time as we move about in the other dimes\n",
      " time as we move about in the other dimen\n",
      " the table was a small shaded lamp the bi\n",
      " the table was a small shaded lamp the br\n",
      "it is spoken of as having three dimensio \n",
      " t is spoken of as having three dimension\n",
      " he walked slowly out of the room and wei\n",
      " he walked slowly out of the room and we \n",
      " one or two ideas that are almost univera\n",
      " one or two ideas that are almost univers\n",
      "ne to travel through time exclaimed the r\n",
      " e to travel through time exclaimed the v\n",
      "ovincial mayor it is simply this that sp \n",
      " vincial mayor it is simply this that spa\n",
      "re going to verify that the experiment ct\n",
      " e going to verify that the experiment cr\n",
      "t this fourth dimension i have not said v\n",
      "  this fourth dimension i have not said t\n",
      "veller proceeded any real body must havea\n",
      " eller proceeded any real body must have \n",
      "sterday it was so high yesterday night io\n",
      " terday it was so high yesterday night it\n",
      " red hair i do not mean to ask you to ac \n",
      " red hair i do not mean to ask you to acc\n",
      "arkably convenient for the historian then\n",
      " rkably convenient for the historian the \n",
      "ns it must have length breadth thickness \n",
      " s it must have length breadth thickness \n",
      "no means of staying back for any length s\n",
      " o means of staying back for any length o\n",
      "t before the balloons save for spasmodicn\n",
      "  before the balloons save for spasmodic \n",
      "d is an absolutely unaccountable thing hi\n",
      "  is an absolutely unaccountable thing he\n",
      "een another at seventeen another at twen \n",
      " en another at seventeen another at twent\n",
      " is the future said the very young man ja\n",
      " is the future said the very young man ju\n",
      "u in a moment we incline to overlook thi \n",
      "  in a moment we incline to overlook this\n",
      "ory in it and some transparent crystallid\n",
      " ry in it and some transparent crystallin\n",
      "ait a moment can an instantaneous cube es\n",
      " it a moment can an instantaneous cube ex\n",
      "need from you you know of course that a c\n",
      " eed from you you know of course that a m\n",
      "t the experiment cried filby who was geto\n",
      "  the experiment cried filby who was gett\n",
      "mall octagonal tables that were scattereh\n",
      " all octagonal tables that were scattered\n",
      "very young man just think one might invet\n",
      " ery young man just think one might inves\n",
      " being which is a fixed and unalterable t\n",
      " being which is a fixed and unalterable t\n",
      "ground for it you will soon admit as muct\n",
      " round for it you will soon admit as much\n",
      "o most people think but wait a moment cah\n",
      "  most people think but wait a moment can\n",
      "ght it and his fecundity you must follow \n",
      " ht it and his fecundity you must follow \n",
      "looked continued the time traveller withl\n",
      " ooked continued the time traveller with \n",
      "ch are immaterial and have no dimensions \n",
      " h are immaterial and have no dimensions \n",
      "alked slowly out of the room and we heare\n",
      " lked slowly out of the room and we heard\n",
      "way of looking at time there is no diffe \n",
      " ay of looking at time there is no differ\n",
      "ally accepted the geometry for instance w\n",
      " lly accepted the geometry for instance t\n",
      "heard his slippers shuffling down the lon\n",
      " eard his slippers shuffling down the lon\n",
      "e three planes of space and a fourth timk\n",
      "  three planes of space and a fourth time\n",
      "ere was that luxurious after dinner atmo \n",
      " re was that luxurious after dinner atmos\n",
      "space and time as the driver determines g\n",
      " pace and time as the driver determines f\n",
      " so it seemed to me and so i never talke \n",
      " so it seemed to me and so i never talked\n",
      "nger as we sat and lazily admired his eai\n",
      " ger as we sat and lazily admired his ear\n",
      "ngth breadth and thickness and is always \n",
      " gth breadth and thickness and is always \n",
      "than a small clock and very delicately ml\n",
      " han a small clock and very delicately ma\n",
      " in a balloon and why should he not hopea\n",
      " in a balloon and why should he not hope \n",
      "e exclaimed the very young man that shalh\n",
      "  exclaimed the very young man that shall\n",
      "n along the latter from the beginning tot\n",
      "  along the latter from the beginning to \n",
      " about in time for instance if i am recat\n",
      " about in time for instance if i am recal\n",
      "ent we incline to overlook this fact thet\n",
      " nt we incline to overlook this fact ther\n",
      "beginning to the end of our lives that sh\n",
      " eginning to the end of our lives that sa\n",
      "t of which fell upon the model there werh\n",
      "  of which fell upon the model there were\n",
      "urface man had no freedom of vertical moe\n",
      " rface man had no freedom of vertical mov\n",
      " i need from you you know of course thate\n",
      " i need from you you know of course that \n",
      "rgumentative person with red hair i do nn\n",
      " gumentative person with red hair i do no\n",
      "e planes each at right angles to the othh\n",
      "  planes each at right angles to the othe\n",
      "n newcomb was expounding this to the newe\n",
      "  newcomb was expounding this to the new \n",
      "imed the very young man that shall travea\n",
      " med the very young man that shall travel\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    x, y = next(iter(test_data))\n",
    "    # Move data to device\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    pred, _ = net(x, None, training=False)\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    pred = torch.cat((x, pred.unsqueeze(1)), dim=1)\n",
    "    \n",
    "    # Move tensors back to CPU for printing\n",
    "    pred_cpu = pred.cpu()\n",
    "    y_cpu = y.cpu()\n",
    "    \n",
    "    for hat, actual in zip(pred_cpu, y_cpu):\n",
    "        print(\"\".join(time_machine.vocab.to_tokens(hat.tolist())))\n",
    "        print(\"\".join([' '] + time_machine.vocab.to_tokens(actual.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.RNNLMScratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
